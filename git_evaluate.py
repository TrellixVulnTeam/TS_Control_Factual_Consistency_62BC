# -*- coding: utf-8 -*-
"""git_evaluate

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1BkHs5siYkzhcCN2fKb7DqKuXVIa9MMa0

# setup
"""

from google.colab import drive
drive.mount('/content/drive')

import os
import numpy as np
import ast
import matplotlib.pyplot as plt
import scipy
import pickle
import pandas as pd
import seaborn as sns

from IPython.display import HTML, display

def set_css():
  display(HTML('''
  <style>
    pre {
        white-space: pre-wrap;
    }
  </style>
  '''))
get_ipython().events.register('pre_run_cell', set_css)

gpu_info = !nvidia-smi
gpu_info = '\n'.join(gpu_info)
if gpu_info.find('failed') >= 0:
  print('Select the Runtime > "Change runtime type" menu to enable a GPU accelerator, ')
  print('and then re-execute this cell.')
else:
  print(gpu_info)

muss_path = '/content/drive/MyDrive/muss'

!echo $muss_path

# Commented out IPython magic to ensure Python compatibility.
# %cd $muss_path

!pwd

pip install -e . &> /dev/null

# Commented out IPython magic to ensure Python compatibility.
# %%capture
# !pip install fairseq==0.10.2

from muss.simplifiers import get_fairseq_simplifier, get_preprocessed_simplifier
from muss.preprocessors import get_preprocessors, get_preprocessor_by_name
from easse.utils.helpers import read_lines



def get_mean_confidence_interval(data, confidence=0.95):
    '''
    from https://github.com/facebookresearch/muss/blob/8b016942d9d0984866f6832594c049a189c1b46e/scripts/train_paper_models.py#L29
    '''

    data = np.array(data)
    a = 1.0 * data
    a = a[~np.isnan(a)]
    n = len(a)
    se = scipy.stats.sem(a)
    h = se * scipy.stats.t.ppf((1 + confidence) / 2.0, n - 1)
    return h

"""## load named entities"""

import pickle
import pandas as pd

test_complex_df = pd.read_pickle("/content/drive/MyDrive/muss/test_complex_df_0825.pkl")

with open ('/content/drive/MyDrive/muss/qualitative/asset_test_df_0810.pkl', 'rb') as fp:
    test_simple_df = pickle.load(fp)

def find_num_appered_entity(NER_list,sentence):

  num_appered_entity = 0

  for NER in NER_list:
    if NER in sentence:
      num_appered_entity += 1

  return num_appered_entity


def get_NER_retain_portion(simple_or_complex,output_path):
  sentences = read_lines(output_path)

  if simple_or_complex=='simple':
    df = test_simple_df
  elif simple_or_complex=='complex':
    df = test_complex_df
  else:
    assert False, "please specify simple or complex"

  # store the index that has len(NER) != 0
  index_list = []
  NER_percentage_list = []

  for index, row in df.iterrows():
    if len(row['NER']) == 0:
      continue
    
    index_list.append(index)
    NER_num = find_num_appered_entity(row['NER'],sentences[index])

    NER_percentage = NER_num/len(row['NER'])

    NER_percentage_list.append(NER_percentage)

  df_output = pd.DataFrame(NER_percentage_list,columns=['NER_percent'])
  print(df_output['NER_percent'].describe())
  df_output['NER_percent'].value_counts().plot.bar()
  
  return np.mean(NER_percentage_list),np.std(NER_percentage_list)

with open ('/content/drive/MyDrive/muss/qualitative/asset_ABCD_valid_0911', 'rb') as fp:
    valid_complex_ABCD_df = pickle.load(fp)

with open ('/content/drive/MyDrive/muss/qualitative/asset_ABCD_test_0911', 'rb') as fp:
    test_complex_ABCD_df = pickle.load(fp)

def get_hard_word_retain_portion(phase,output_path):

  sentences = read_lines(output_path)

  if phase=='valid':
    df = valid_complex_ABCD_df
  elif phase=='test':
    df = test_complex_ABCD_df

  df = test_complex_ABCD_df
  
  NER_percentage_list = []

  for index, row in df.iterrows():
    if isinstance(row['C2C1B2_in_complex'],float):
      continue
    if len(row['C2C1B2_in_complex']) == 0:
      continue
    
    NER_num = find_num_appered_entity(row['C2C1B2_in_complex'],sentences[index])

    NER_percentage = NER_num/len(row['C2C1B2_in_complex'])

    NER_percentage_list.append(NER_percentage)

  df_output = pd.DataFrame(NER_percentage_list,columns=['hard_word_remain_percent'])
  print(df_output['hard_word_remain_percent'].describe())
  df_output['hard_word_remain_percent'].value_counts().plot.bar()
  
  return np.mean(NER_percentage_list),np.std(NER_percentage_list)

"""## define functions"""

!pip install sacrebleu==1.5.1

def easse_my_model_eval(output_id,generate_id,dataset,phase,output_file_name):

  pred_path = '/content/drive/MyDrive/muss/output/'+output_id+'/'+generate_id+'/'+output_file_name

  print('pred_path',pred_path)
  if phase=='valid':
    return os.popen("easse evaluate -t asset_valid -m 'bleu,sari,fkgl' -q < %s" %pred_path).read()
  elif phase=='test':
    return os.popen("easse evaluate -t asset_test -m 'bleu,sari,fkgl' -q < %s" %pred_path).read()

def evaluate_my_model(output_id,generate_id,dataset,phase,output_file_name,ABCD=False,muss=False,simple_or_complex=None):#,use_token=True,use_simple=False):
  
  if muss:
    result = ast.literal_eval(os.popen("easse evaluate -t asset_test -m 'bleu,sari,fkgl' -q < %s" %output_id).read())
    print(result)

    if ABCD:
      NE_retain_mean,NE_retain_std = get_hard_word_retain_portion(phase,output_id)
    else:
      NE_retain_mean,NE_retain_std = get_NER_retain_portion(simple_or_complex,output_id)

    return result['bleu'], result['sari'], result['fkgl'],NE_retain_mean,NE_retain_std
  else:
    result = ast.literal_eval(easse_my_model_eval(output_id,generate_id,dataset,phase,output_file_name))

    print(result)
    print('model id:',output_id)
    print('dataset:',dataset)

    pred_path = '/content/drive/MyDrive/muss/output/'+output_id+'/'+generate_id+'/'+output_file_name

    if ABCD:
      NE_retain_mean,NE_retain_std = get_hard_word_retain_portion(phase,pred_path)
    else:
      NE_retain_mean,NE_retain_std = get_NER_retain_portion(simple_or_complex,pred_path)

    return result['bleu'], result['sari'], result['fkgl'],NE_retain_mean,NE_retain_std

def get_std(sari_list_list):
  calculated_std_list = []
  for i in range(np.array(sari_list_list).T.shape[0]):
      calculated_std_list.append(get_mean_confidence_interval(np.array(sari_list_list).T[i]))
  return calculated_std_list

def easse_my_model_eval_operation(output_id,generate_id,dataset,phase,output_file_name):

  pred_path = '/content/drive/MyDrive/muss/output/'+output_id+'/'+generate_id+'/'+output_file_name

  print('pred_path',pred_path)
  if phase=='valid':
    return os.popen("easse evaluate -t asset_valid -m 'sari_by_operation' -q < %s" %pred_path).read()
  elif phase=='test':
    return os.popen("easse evaluate -t asset_test -m 'sari_by_operation' -q < %s" %pred_path).read()

def evaluate_my_model_operation(output_id,generate_id,dataset,phase,output_file_name,ABCD=False,muss=False):#,use_token=True,use_simple=False):
  
  if muss:
    result = ast.literal_eval(os.popen("easse evaluate -t asset_test -m 'sari_by_operation' -q < %s" %output_id).read())
    print(result)

    return result['sari_add'], result['sari_keep'], result['sari_del']
  else:
    result = ast.literal_eval(easse_my_model_eval_operation(output_id,generate_id,dataset,phase,output_file_name))

    print(result)
    
    print('model id:',output_id)
    print('dataset:',dataset)

    pred_path = '/content/drive/MyDrive/muss/output/'+output_id+'/'+generate_id+'/'+output_file_name

    return result['sari_add'], result['sari_keep'], result['sari_del']



"""# define data path for experiments of differnt Train and Parameter setting

## define test set path (simple)
"""

DATA_DIR = '/content/drive/MyDrive/muss/resources/datasets/'

asset_valid_complex_dir =  DATA_DIR + f'token_asset_0803/valid.complex'
asset_test_complex_dir = DATA_DIR + f'token_asset_0803/test.complex'

asset_valid_simple_dir = DATA_DIR + f'token_asset_simple_NER_0810/valid.complex'
asset_test_simple_dir = DATA_DIR + f'token_asset_simple_NER_0810/test.complex'

asset_valid_all_no_random_dir = DATA_DIR + f'asset_simple_all_words_no_random_0825/valid.complex'
asset_test_all_no_random_dir = DATA_DIR + f'asset_simple_all_words_no_random_0825/test.complex'

asset_valid_all_random_dir = DATA_DIR + f'asset_simple_all_words_random_0825/valid.complex'
asset_test_all_random_dir = DATA_DIR + f'asset_simple_all_words_random_0825/test.complex'

complex_model_data_test_dir_list=[asset_test_complex_dir,asset_test_simple_dir]
complex_model_data_test_list=['asset.complex.test','asset.simple.test']

test_data_dir_list,test_data_list = [],[]

tmp_data_dir_list = []
tmp_data_list = []

for phase in ['test']:
  for i in range(7):
    path_name = 'asset_'+phase+'_'+str(i)+'_simple_dir'
    globals()[path_name] = DATA_DIR + f'asset_simple_'+str(i)+'NE_0825/'+phase+'.complex'

    tmp_data_dir_list.append(globals()[path_name])
    tmp_data_list.append('asset.'+str(i)+'.simple.'+phase)

test_data_dir_list.extend(tmp_data_dir_list)
test_data_list.extend(tmp_data_list)

test_data_dir_list

test_data_list

test_data_dir_list.extend([
            asset_test_simple_dir,
             asset_test_complex_dir,
             asset_test_all_no_random_dir,
             asset_test_all_random_dir
             ]
)

test_data_list.extend([
            'asset.simple.test',
             'asset.complex.test',
             'asset.norandom.test',
             'asset.random.test',
             ]
)

test_data_dir_list

test_data_list

#  x  asix is thing prepend to sentence
# NE from named entities detected in simple sentence

# mean of #NE is 3, so increase lateern atfetr 3
test_data_explain_list = ['0 NE at simple',
                          '1 NE at simple',
                          '2 NE at simple',
                          '3 NE at simple',
                          '4 NE at simple',
                          '5 NE at simple',
                          '6 NE at simple',
                          'all NE at simple',
                          'all NE at complex',
                          'all at simple',
                          'all at simple shuffled',
                          ]

"""## define test set path (complex)"""

DATA_DIR = '/content/drive/MyDrive/muss/resources/datasets/'


complex_asset_valid_all_no_random_dir = DATA_DIR + f'asset_complex_all_words_0828/valid.complex'
complex_asset_test_all_no_random_dir = DATA_DIR + f'asset_complex_all_words_0828/test.complex'

complex_asset_valid_all_random_dir = DATA_DIR + f'asset_complex_all_words_random_0828/valid.complex'
complex_asset_test_all_random_dir = DATA_DIR + f'asset_complex_all_words_random_0828/test.complex'

test_data_dir_list,test_data_list = [],[]

tmp_data_dir_list = []
tmp_data_list = []

for phase in ['test']:
  for i in range(7):
    path_name = 'asset_'+phase+'_'+str(i)+'_complex_dir'
    globals()[path_name] = DATA_DIR + f'asset_complex_'+str(i)+'NE_0828/'+phase+'.complex'

    tmp_data_dir_list.append(globals()[path_name])
    tmp_data_list.append('asset.'+str(i)+'.complex.'+phase)

test_data_dir_list.extend(tmp_data_dir_list)
test_data_list.extend(tmp_data_list)

test_data_dir_list

test_data_list

test_data_dir_list.extend([

            complex_asset_test_all_no_random_dir,
             complex_asset_test_all_random_dir
             ]
)

test_data_list.extend([

            'asset.complex.all.test',
             'asset.complex.allrandom.test',
             ]
)

test_data_dir_list

test_data_list

test_data_explain_list=['0 NE at complex',
 '1 NE at complex',
 '2 NE at complex',
 '3 NE at complex',
 '4 NE at complex',
 '5 NE at complex',
 '6 NE at complex',
 'all words at complex',
 'all words at complex shuffled']

"""## define test set path complex & simple"""

DATA_DIR = '/content/drive/MyDrive/muss/resources/datasets/'

# complex 
asset_valid_complex_dir =  DATA_DIR + f'token_asset_0803/valid.complex'
asset_test_complex_dir = DATA_DIR + f'token_asset_0803/test.complex'

complex_asset_valid_all_no_random_dir = DATA_DIR + f'asset_complex_all_words_0828/valid.complex'
complex_asset_test_all_no_random_dir = DATA_DIR + f'asset_complex_all_words_0828/test.complex'

complex_asset_valid_all_random_dir = DATA_DIR + f'asset_complex_all_words_random_0828/valid.complex'
complex_asset_test_all_random_dir = DATA_DIR + f'asset_complex_all_words_random_0828/test.complex'

asset_valid_simple_dir = DATA_DIR + f'token_asset_simple_NER_0810/valid.complex'
asset_test_simple_dir = DATA_DIR + f'token_asset_simple_NER_0810/test.complex'

asset_valid_all_no_random_dir = DATA_DIR + f'asset_simple_all_words_no_random_0825/valid.complex'
asset_test_all_no_random_dir = DATA_DIR + f'asset_simple_all_words_no_random_0825/test.complex'

asset_valid_all_random_dir = DATA_DIR + f'asset_simple_all_words_random_0825/valid.complex'
asset_test_all_random_dir = DATA_DIR + f'asset_simple_all_words_random_0825/test.complex'

test_data_dir_list,test_data_list = [],[]

tmp_data_dir_list = []
tmp_data_list = []

for phase in ['test']:
  for i in range(7):
    path_name = 'asset_'+phase+'_'+str(i)+'_complex_dir'
    globals()[path_name] = DATA_DIR + f'asset_complex_'+str(i)+'NE_0828/'+phase+'.complex'

    tmp_data_dir_list.append(globals()[path_name])
    tmp_data_list.append('asset.'+str(i)+'.complex.'+phase)

test_data_dir_list.extend(tmp_data_dir_list)
test_data_list.extend(tmp_data_list)

test_data_dir_list

test_data_list

test_data_dir_list.extend([
                           asset_test_complex_dir,

             ]
)

test_data_list.extend([
                       'asset.allNE.complex.test'

             ]
)

for phase in ['test']:
  for i in range(7):
    path_name = 'asset_'+phase+'_'+str(i)+'_simple_dir'
    globals()[path_name] = DATA_DIR + f'asset_simple_'+str(i)+'NE_0825/'+phase+'.complex'

    test_data_dir_list.append(globals()[path_name])
    test_data_list.append('asset.'+str(i)+'.simple.'+phase)

test_data_dir_list.extend([
            asset_test_simple_dir,
            complex_asset_test_all_no_random_dir,
             complex_asset_test_all_random_dir,
             asset_test_all_no_random_dir,
             asset_test_all_random_dir
             ]
)

test_data_list.extend([
            'asset.simple.test',
            'asset.complex.all.test',
             'asset.complex.allrandom.test',
             'asset.norandom.test',
             'asset.random.test',
             ]
)

test_data_dir_list

test_data_list

test_data_explain_list=['0 NE at complex',
 '1 NE at complex',
 '2 NE at complex',
 '3 NE at complex',
 '4 NE at complex',
 '5 NE at complex',
 '6 NE at complex',
 'all NE at complex',

 '0 NE at simple',
 '1 NE at simple',
 '2 NE at simple',
 '3 NE at simple',
 '4 NE at simple',
 '5 NE at simple',
 '6 NE at simple',
 'all NE at simple',

 'all words at complex',
 'all words at complex shuffled',

 'all words at simple',
 'all words at simple shuffled',
 
 ]

"""## define test set path CERF"""

DATA_DIR = '/content/drive/MyDrive/muss/resources/datasets/'

asset_valid_all_ABCDword_dir = DATA_DIR + f'asset_ABCD_C1C2B2/valid.complex'
asset_test_all_ABCDword_dir = DATA_DIR + f'asset_ABCD_C1C2B2/test.complex'

asset_valid_all_no_random_dir = DATA_DIR + f'asset_ABCD_all_words_0911/valid.complex'
asset_test_all_no_random_dir = DATA_DIR + f'asset_ABCD_all_words_0911/test.complex'

asset_valid_all_random_dir = DATA_DIR + f'asset_ABCD_all_words_random_0911/valid.complex'
asset_test_all_random_dir = DATA_DIR + f'asset_ABCD_all_words_random_0911/test.complex'

test_data_dir_list,test_data_list = [],[]

tmp_data_dir_list = []
tmp_data_list = []

for phase in ['test']:
  for i in range(7):
    path_name = 'asset_'+phase+'_'+str(i)+'_complex_dir'
    globals()[path_name] = DATA_DIR + f'asset_ABCD_'+str(i)+'words_0911/'+phase+'.complex'

    tmp_data_dir_list.append(globals()[path_name])
    tmp_data_list.append('asset.'+str(i)+'.ABCD.'+phase)

test_data_dir_list.extend(tmp_data_dir_list)
test_data_list.extend(tmp_data_list)

test_data_dir_list

test_data_list

test_data_dir_list.extend([
                           asset_test_all_ABCDword_dir,
                           asset_test_all_no_random_dir,
                           asset_test_all_random_dir
            
             ]
)

test_data_list.extend([
            'asset.allABCD.ABCD.test',
             'asset.allwords.ABCD.test',
             'asset.allwordsrandom.ABCD.test',
             ]
)

test_data_dir_list

test_data_list

test_data_explain_list=[
                        '0 hard word',
                        '1 hard word',
                        '2 hard word',
                        '3 hard word',
                        '4 hard word',
                        '5 hard word',
                        '6 hard word',
                        'all hard word',
                        'all word',
                        'all word shuffled',

]

"""## define test set path Named Entities + CERF"""

DATA_DIR = '/content/drive/MyDrive/muss/resources/datasets/'

asset_valid_all_all_dir = DATA_DIR + f'asset_ABCD_NER/valid.complex'
asset_test_all_all_dir = DATA_DIR + f'asset_ABCD_NER/test.complex'

test_data_dir_list,test_data_list = [],[]

tmp_data_dir_list = []
tmp_data_list = []

for i in range(7):
  path_name = 'asset_ABCD_NER_fix_CERF_'+str(i)+'NE_dir'
  globals()[path_name] = DATA_DIR+'asset_ABCD_NER_fix_CERF_'+str(i)+'NE/test.complex'

  tmp_data_dir_list.append(globals()[path_name])
  tmp_data_list.append('asset.allCERF.'+str(i)+'NE')

test_data_dir_list.extend(tmp_data_dir_list)
test_data_list.extend(tmp_data_list)

test_data_dir_list

test_data_list

tmp_data_dir_list = []
tmp_data_list = []

for i in range(7):
  path_name = 'asset_ABCD_NER_fix_NE_'+str(i)+'CERF_dir'
  globals()[path_name] = DATA_DIR+'asset_ABCD_NER_fix_NE_'+str(i)+'CERF/test.complex'

  tmp_data_dir_list.append(globals()[path_name])
  tmp_data_list.append('asset.allNE.'+str(i)+'CERF')

test_data_dir_list.extend(tmp_data_dir_list)
test_data_list.extend(tmp_data_list)

test_data_dir_list

test_data_list

test_data_dir_list.extend([
                           asset_test_all_all_dir
            
             ]
)

test_data_list.extend([
            'asset.allNE.allCERF',

             ]
)

test_data_dir_list

test_data_list

test_data_explain_list=[
'0 NE, all hard words',
'1 NE, all hard words',
'2 NE, all hard words',
'3 NE, all hard words',
'4 NE, all hard words',
'5 NE, all hard words',
'6 NE, all hard words',
'all NE, 0 hard words',
'all NE, 1 hard words',
'all NE, 2 hard words',
'all NE, 3 hard words',
'all NE, 4 hard words',
'all NE, 5 hard words',
'all NE, 6 hard words',
'all NE, all hard words',
]

"""# model info dictionary"""

MODEL_DIR = '/content/drive/MyDrive/muss/experiments/fairseq/'

model_dir_dict = {}

def add_item_to_dict(**kwargs):

  id = len(model_dir_dict)
  model_dir_dict[id]={}

  for key, value in kwargs.items():
    model_dir_dict[id][key]=value
  print('added:',model_dir_dict[id])

add_item_to_dict(model_id=len(model_dir_dict),exp_dir='/content/drive/MyDrive/muss/resources/models/bart_mined',model_name='bart_mined')

add_item_to_dict(model_id=len(model_dir_dict),exp_dir='/content/drive/MyDrive/muss/resources/models/bart_mined_wikilarge',model_name='bart_mined_wikilarge',preprocessors_kwargs = {
        'LengthRatioPreprocessor': {'target_ratio': 0.9, 'use_short_name': False},
        'ReplaceOnlyLevenshteinPreprocessor': {'target_ratio': 0.65, 'use_short_name': False},
        'WordRankRatioPreprocessor': {'target_ratio': 0.75, 'language': 'en', 'use_short_name': False},
        'DependencyTreeDepthRatioPreprocessor': {'target_ratio': 0.4, 'language': 'en', 'use_short_name': False},
    })

add_item_to_dict(model_id=len(model_dir_dict),
                #  exp_dir=MODEL_DIR+'local_1626908793158/',
                #  model_name='bart_wikilarge_wo_token',
                #  info=
                #  '''
                #  training not completed. 
                #  use original code.
                #  use detoken wikilarge data. i.e. no NER token added.
                #  use_asset=True.
                #  load bart.large.
                #  '''
                 )

add_item_to_dict(model_id=len(model_dir_dict),
                #  exp_dir=MODEL_DIR+'local_1627676101775/',
                #  model_name='bart_change_all_data',
                #  info=
                #  '''
                #  training not completed. 
                #  train, test, valid data: NER added wikilarge data.
                #  use_asset=True.
                #  load bart.large.
                #  '''
                 )

add_item_to_dict(model_id=len(model_dir_dict),
                #  exp_dir=MODEL_DIR+'local_1627757030330/', 
                #  model_name='bart_change_all_data',
                #  info=
                #  '''
                #  training completed.
                #  scores={'bleu': 76.51763175185911, 'sari': 44.9886054760627, 'fkgl': 6.2702108605400255}!!(should rerun using predict_file=asset_token)
                #  train, test, valid data: NER added wikilarge data.
                #  use_asset=True.
                #  load bart.large.
                #  continue train on local_1627676101775/checkpoints/checkpoint_6_3200.pt
                #  ''',
                #  recommended_preprocessors_kwargs={'LengthRatioPreprocessor': {'target_ratio': 0.8183618791572886, 'use_short_name': False}, 'ReplaceOnlyLevenshteinPreprocessor': {'target_ratio': 0.7897938410469331, 'use_short_name': False}, 'WordRankRatioPreprocessor': {'target_ratio': 0.8047148154836818, 'language': 'en', 'use_short_name': False}, 'DependencyTreeDepthRatioPreprocessor': {'target_ratio': 0.7532679172898086, 'language': 'en', 'use_short_name': False}, 'GPT2BPEPreprocessor': {}}
                 )

add_item_to_dict(model_id=len(model_dir_dict),
                #  exp_dir=MODEL_DIR+'local_1627767953241/',
                #  model_name='muss_change_all_data',
                #  info=
                #  '''
                #  training not completed. 
                #  train, test, valid data: NER added wikilarge data.
                #  use_asset=True.
                #  load muss_mined.
                #  ''',
                #  train_args=
                #  '''
                #  fairseq-train /content/drive/MyDrive/muss/resources/datasets/_59df15dce93dacacb2a9ada082637f1e/fairseq_preprocessed_complex-simple --task translation --source-lang complex --target-lang simple --save-dir /content/drive/MyDrive/muss/experiments/fairseq/local_1627767953241/checkpoints --optimizer adam --adam-betas '(0.9, 0.98)' --clip-norm 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --lr-scheduler polynomial_decay --lr 3e-05 --warmup-updates 500 --update-freq 128 --arch bart_large --dropout 0.1 --weight-decay 0.0 --clip-norm 0.1 --share-all-embeddings --no-epoch-checkpoints --save-interval 999999 --validate-interval 999999 --max-update 20000 --save-interval-updates 100 --keep-interval-updates 1 --patience 10 --batch-size 64 --seed 555 --distributed-world-size 1 --distributed-port 16491 --fp16 --restore-file '/content/drive/MyDrive/muss/resources/models/bart_mined/model.pt' --max-tokens 512 --truncate-source --layernorm-embedding --share-all-embeddings --share-decoder-input-output-embed --reset-optimizer --reset-dataloader --reset-meters --required-batch-size-multiple 1 --label-smoothing 0.1 --attention-dropout 0.1 --weight-decay 0.01 --optimizer 'adam' --adam-betas '(0.9, 0.999)' --adam-eps 1e-08 --clip-norm 0.1 --skip-invalid-size-inputs-valid-test --find-unused-parameters
                #  '''
                 )

add_item_to_dict(model_id=len(model_dir_dict),
#                  exp_dir=MODEL_DIR+'local_1627838987960/', THIS AND BEFORE DELETED
#                  model_name='muss_change_all_data',
#                  info=
#                  '''
#                  train complete.
#                  scores={'bleu': 80.62918711490221, 'sari': 44.16175529588565, 'fkgl': 7.079753954439823}(predict file used asset_token.)
#                  train, test, valid data: NER added wikilarge data.
#                  use_asset=True.
#                  load muss_mined.
#                  ''',
#                  recommended_preprocessors_kwargs={'DependencyTreeDepthRatioPreprocessor': {'language': 'en',
#   'target_ratio': 0.8937919408369409,
#   'use_short_name': False},
#  'GPT2BPEPreprocessor': {},
#  'LengthRatioPreprocessor': {'target_ratio': 0.8625830047202065,
#   'use_short_name': False},
#  'ReplaceOnlyLevenshteinPreprocessor': {'target_ratio': 0.8706196758216501,
#   'use_short_name': False},
#  'WordRankRatioPreprocessor': {'language': 'en',
#   'target_ratio': 0.9829037295830106,
#   'use_short_name': False}}
  )

add_item_to_dict(model_id=len(model_dir_dict),
                #  exp_dir=MODEL_DIR+'local_1628033181968/',
                #  model_name='muss_1',
                #  info=
                #  '''
                #  train complete
                #  # complex NE
                #  train, test, valid data: NER added wikilarge data.
                #  1. changed dictionaru
                # 2. changed extract special token
                # 3. changed train kwargs
                # 4.muss/evaluation\general 2 function changed 
                # 5.muss/fairseq/base train arg
                #  use_asset=True.
                #  load muss_mined.
                #  ''',
                #  recommended_preprocessors_kwargs=
                #  {'LengthRatioPreprocessor': {'target_ratio': 0.9013845768472064, 'use_short_name': False}, 
                #   'ReplaceOnlyLevenshteinPreprocessor': {'target_ratio': 0.8412439617313068, 'use_short_name': False}, 
                #   'WordRankRatioPreprocessor': {'target_ratio': 0.945465089807053, 'language': 'en', 'use_short_name': False}, 
                #   'DependencyTreeDepthRatioPreprocessor': {'target_ratio': 0.3179728920946812, 'language': 'en', 'use_short_name': False}, 
                #   'GPT2BPEPreprocessor': {}}
                 )

add_item_to_dict(model_id=len(model_dir_dict),
                 exp_dir=MODEL_DIR+'local_1628299139742/',
                 model_name='muss_train_use_target_NER',
                 info=
                 '''
                 train complete
                 train,test, valid: insert target NER into source.
                 1. changed dictionary
                2. changed extract special token
                3. changed train kwargs
                4.muss/evaluation\general 2 function changed 
                5.muss/fairseq/base train arg
                 use_asset=True.
                 load muss_mined.
                 token_data_wikilarge_0807
                 ''',
                 # use source NE
                 recommended_preprocessors_kwargs=
                 {'LengthRatioPreprocessor': {'target_ratio': 0.9232791040422811, 'use_short_name': False}, 
 'ReplaceOnlyLevenshteinPreprocessor': {'target_ratio': 0.7547104722847939, 'use_short_name': False}, 
 'WordRankRatioPreprocessor': {'target_ratio': 0.8384149385346878, 'language': 'en', 'use_short_name': False}, 
 'DependencyTreeDepthRatioPreprocessor': {'target_ratio': 0.45741492410366896, 'language': 'en', 'use_short_name': False}, 
 'GPT2BPEPreprocessor': {}}
                 )

# use clean target NE
# # recommended_preprocessors_kwargs=
# {'LengthRatioPreprocessor': {'target_ratio': 0.909607027613668, 'use_short_name': False}, 
#  'ReplaceOnlyLevenshteinPreprocessor': {'target_ratio': 0.8162749783306084, 'use_short_name': False}, 
#  'WordRankRatioPreprocessor': {'target_ratio': 0.5462451568701605, 'language': 'en', 'use_short_name': False}, 
#  'DependencyTreeDepthRatioPreprocessor': {'target_ratio': 0.3694423178221648, 'language': 'en', 'use_short_name': False}, 
#  'GPT2BPEPreprocessor': {}}

add_item_to_dict(model_id=len(model_dir_dict),
                 exp_dir=MODEL_DIR+'local_1628356825356/',
                 model_name='muss_train_use_co_occur_NER',
                 info=
                 '''
                 train complete
                 train: insert NER that is both in target and source.
                test, valid data: unchanged wikilarge. NER is from source.
                 use_asset=True.
                 load muss_mined.
                 token_data_wikilarge_0807_cooccur
                 ''',
                 
                 )

add_item_to_dict(model_id=len(model_dir_dict),
                 exp_dir=MODEL_DIR+'local_1628357063986/',
                 model_name='muss_no_NER',
                 info=
                 '''
                 train complete
                 train: insert NER that is both in target and source.
                test, valid data: unchanged wikilarge. NER is from source.
                 use_asset=True.
                 load muss_mined.
                 token_data_wikilarge_0807_no_NER
                 ''',
                 recommended_preprocessors_kwargs=
                 {'LengthRatioPreprocessor': {'target_ratio': 0.671471662548423, 'use_short_name': False}, 
                  'ReplaceOnlyLevenshteinPreprocessor': {'target_ratio': 0.7899616811007996, 'use_short_name': False}, 
                  'WordRankRatioPreprocessor': {'target_ratio': 0.40813050670938067, 'language': 'en', 'use_short_name': False}, 
                  'DependencyTreeDepthRatioPreprocessor': {'target_ratio': 0.791613961476961, 'language': 'en', 'use_short_name': False}, 
                  'GPT2BPEPreprocessor': {}}

                 )

add_item_to_dict(model_id=len(model_dir_dict),
                 exp_dir=MODEL_DIR+'local_1628299139742/',
                 model_name='muss_train_use_target_NER',
                 info=
                 '''
                 train complete
                 train: insert target sentence NER into source.
                test, valid data: unchanged wikilarge. NER is from source.
                 1. changed dictionary
                2. changed extract special token
                3. changed train kwargs
                4.muss/evaluation\general 2 function changed 
                5.muss/fairseq/base train arg
                 use_asset=True.
                 load muss_mined.
                 token_data_wikilarge_0807
                 ''',
                 # use clean target NE
                 recommended_preprocessors_kwargs=
                 {'LengthRatioPreprocessor': {'target_ratio': 0.909607027613668, 'use_short_name': False}, 
 'ReplaceOnlyLevenshteinPreprocessor': {'target_ratio': 0.8162749783306084, 'use_short_name': False}, 
 'WordRankRatioPreprocessor': {'target_ratio': 0.5462451568701605, 'language': 'en', 'use_short_name': False}, 
 'DependencyTreeDepthRatioPreprocessor': {'target_ratio': 0.3694423178221648, 'language': 'en', 'use_short_name': False}, 
 'GPT2BPEPreprocessor': {}}
                 )

# use clean target NE
# # recommended_preprocessors_kwargs=
# {'LengthRatioPreprocessor': {'target_ratio': 0.909607027613668, 'use_short_name': False}, 
#  'ReplaceOnlyLevenshteinPreprocessor': {'target_ratio': 0.8162749783306084, 'use_short_name': False}, 
#  'WordRankRatioPreprocessor': {'target_ratio': 0.5462451568701605, 'language': 'en', 'use_short_name': False}, 
#  'DependencyTreeDepthRatioPreprocessor': {'target_ratio': 0.3694423178221648, 'language': 'en', 'use_short_name': False}, 
#  'GPT2BPEPreprocessor': {}}

add_item_to_dict(model_id=len(model_dir_dict),
                 exp_dir=MODEL_DIR+'local_1628632701978/',
                 model_name='muss_complex_0810',
                 dataset='token_complex_wikilarge_0810',
                 info=
                 '''
                 train complete
                 train,test,valid
                 use_asset=True.
                 load muss_mined.
                 ''',
                 # source NE

                 )

add_item_to_dict(model_id=len(model_dir_dict),
                 exp_dir=MODEL_DIR+'local_1628632701978/',
                 model_name='muss_complex_0810',
                 dataset='token_complex_wikilarge_0810',
                 info=
                 '''
                 train complete
                 train,test,valid
                 use_asset=True.
                 load muss_mined.
                 ''',
                 # target NE
                 
                 )

add_item_to_dict(model_id=len(model_dir_dict),
                 exp_dir=MODEL_DIR+'local_1628635148170/',
                 model_name='muss_co_occur_0810', # clean
                 dataset='token_data_wikilarge_0810_cooccur',
                 info=
                 '''
                 train complete
                 train,test,valid: insert NER that is both in target and source.
                 use_asset=True.
                 load muss_mined.
                 ''',
                #  soruce NE
                 # cluster
                 recommended_preprocessors_kwargs=
                 {'LengthRatioPreprocessor': 
                  {'target_ratio': 0.7106468975998875, 'use_short_name': False}, 
                  'ReplaceOnlyLevenshteinPreprocessor': {'target_ratio': 0.7209310103866642, 'use_short_name': False}, 
                  'WordRankRatioPreprocessor': {'target_ratio': 0.6883820678495148, 'language': 'en', 'use_short_name': False},
                  'DependencyTreeDepthRatioPreprocessor': {'target_ratio': 0.22377179007596412, 'language': 'en', 'use_short_name': False}, 
                  'GPT2BPEPreprocessor': {}}
                 )

add_item_to_dict(model_id=len(model_dir_dict),
                 exp_dir=MODEL_DIR+'local_1628635148170/',
                 model_name='muss_co_occur_0810', # clean
                 dataset='token_data_wikilarge_0810_cooccur',
                 info=
                 '''
                 train complete
                 train,test,valid: insert NER that is both in target and source.
                 use_asset=True.
                 load muss_mined.
                 ''',
                #  target NE
                recommended_preprocessors_kwargs=
                 {'LengthRatioPreprocessor': {'target_ratio': 1.0420012310744888, 'use_short_name': False}, 
                  'ReplaceOnlyLevenshteinPreprocessor': {'target_ratio': 0.7158657422607126, 'use_short_name': False}, 
                  'WordRankRatioPreprocessor': {'target_ratio': 0.5760708375407306, 'language': 'en', 'use_short_name': False}, 
                  'DependencyTreeDepthRatioPreprocessor': {'target_ratio': 0.21329612191393035, 'language': 'en', 'use_short_name': False}, 
                  'GPT2BPEPreprocessor': {}}
                 )

add_item_to_dict(model_id=len(model_dir_dict),
                 exp_dir=MODEL_DIR+'local_1628718821939/',
                 model_name='muss_simple_0810', # clean
                 dataset='token_simple_wikilarge_0810',
                 info=
                 '''
                 train complete
                 train,test,valid: insert NER that is both in target and source.
                 use_asset=True.
                 load muss_mined.
                 ''',
                #  source NE
                recommended_preprocessors_kwargs=
                 {'LengthRatioPreprocessor': {'target_ratio': 0.8745775160578683, 'use_short_name': False}, 
                  'ReplaceOnlyLevenshteinPreprocessor': {'target_ratio': 0.8184146111849491, 'use_short_name': False}, 
                  'WordRankRatioPreprocessor': {'target_ratio': 0.8301636377664322, 'language': 'en', 'use_short_name': False}, 
                  'DependencyTreeDepthRatioPreprocessor': {'target_ratio': 0.574831422440179, 'language': 'en', 'use_short_name': False}, 
                  'GPT2BPEPreprocessor': {}}
                 )

add_item_to_dict(model_id=len(model_dir_dict),
                 exp_dir=MODEL_DIR+'local_1628718821939/',
                 model_name='muss_simple_0810', # clean
                 dataset='token_simple_wikilarge_0810',
                 info=
                 '''
                 train complete
                 train,test,valid: insert NER that is both in target and source.
                 use_asset=True.
                 load muss_mined.
                 ''',
                #  target NE

                 )

#############################
# 0825

add_item_to_dict(model_id=len(model_dir_dict),
                 exp_dir=MODEL_DIR+'local_1629593348299/',
                 model_name='muss_complex_0822', # delete simple sentence not contain '.', and any of word in simple not in complex
                 dataset='wikilarge_final_complex_0821',
                 info=
                 '''
                 train complete
                 use_asset=True.
                 load muss_mined.
                 ''',
                # complex NE
                 recommended_preprocessors_kwargs={
                     'LengthRatioPreprocessor': {'target_ratio': 0.956836653713281, 'use_short_name': False}, 
                     'ReplaceOnlyLevenshteinPreprocessor': {'target_ratio': 0.7285203296261804, 'use_short_name': False}, 
                     'WordRankRatioPreprocessor': {'target_ratio': 0.77610910374061, 'language': 'en', 'use_short_name': False}, 
                     'DependencyTreeDepthRatioPreprocessor': {'target_ratio': 0.33930538813771477, 'language': 'en', 'use_short_name': False}, 
                     'GPT2BPEPreprocessor': {}}

                 )

add_item_to_dict(model_id=len(model_dir_dict),
                 exp_dir=MODEL_DIR+'local_1629593348299/',
                 model_name='muss_complex_0822', # delete simple sentence not contain '.', and any of word in simple not in complex
                 dataset='wikilarge_final_complex_0821',
                 info=
                 '''
                 train complete
                 use_asset=True.
                 load muss_mined.
                 ''',
                # simple NE
                 
recommended_preprocessors_kwargs=
{'LengthRatioPreprocessor': {'target_ratio': 0.9700595349762818, 'use_short_name': False}, 
 'ReplaceOnlyLevenshteinPreprocessor': {'target_ratio': 0.81504274123193, 'use_short_name': False}, 
 'WordRankRatioPreprocessor': {'target_ratio': 0.8269654939477459, 'language': 'en', 'use_short_name': False},
 'DependencyTreeDepthRatioPreprocessor': {'target_ratio': 0.29353456265546596, 'language': 'en', 'use_short_name': False},
 'GPT2BPEPreprocessor': {}}
                 )

# 0825

add_item_to_dict(model_id=len(model_dir_dict),
                 exp_dir=MODEL_DIR+'local_1629593322552/',
                 model_name='muss_simple_0822', # delete simple sentence not contain '.', and any of word in simple not in complex
                 dataset='wikilarge_final_simple_0821',
                 info=
                 '''
                 train complete
                 use_asset=True.
                 load muss_mined.
                 ''',
                # complex NE
                 recommended_preprocessors_kwargs=
                 {'LengthRatioPreprocessor': {'target_ratio': 0.9489030771756881, 'use_short_name': False}, 
                  'ReplaceOnlyLevenshteinPreprocessor': {'target_ratio': 0.8512708177758973, 'use_short_name': False}, 
                  'WordRankRatioPreprocessor': {'target_ratio': 0.4332282006330827, 'language': 'en', 'use_short_name': False}, 
                  'DependencyTreeDepthRatioPreprocessor': {'target_ratio': 0.48180181825919544, 'language': 'en', 'use_short_name': False}, 
                  'GPT2BPEPreprocessor': {}}
                 )

# 0825

add_item_to_dict(model_id=len(model_dir_dict),
                 exp_dir=MODEL_DIR+'local_1629593322552/',
                 model_name='muss_simple_0822', # delete simple sentence not contain '.', and any of word in simple not in complex
                 dataset='wikilarge_final_simple_0821',
                 info=
                 '''
                 train complete
                 use_asset=True.
                 load muss_mined.
                 ''',
                # simple NE
                 recommended_preprocessors_kwargs=
                 {'LengthRatioPreprocessor': {'target_ratio': 0.9027911048335139, 'use_short_name': False}, 
                  'ReplaceOnlyLevenshteinPreprocessor': {'target_ratio': 0.8490285416814356, 'use_short_name': False}, 
                  'WordRankRatioPreprocessor': {'target_ratio': 0.823359682206033, 'language': 'en', 'use_short_name': False},
                  'DependencyTreeDepthRatioPreprocessor': {'target_ratio': 0.41809516782054756, 'language': 'en', 'use_short_name': False}, 
                  'GPT2BPEPreprocessor': {}}
                 )

# 0825

add_item_to_dict(model_id=len(model_dir_dict),
                 exp_dir=MODEL_DIR+'local_1629750798219/',
                 model_name='muss_cooc_0822', # delete simple sentence not contain '.', and any of word in simple not in complex
                 dataset='wikilarge_cooc_0823',
                 info=
                 '''
                 train complete
                 use_asset=True.
                 load muss_mined.
                 ''',
                # complex NE
                 recommended_preprocessors_kwargs=
                 {'LengthRatioPreprocessor': {'target_ratio': 0.835076952260348, 'use_short_name': False}, 
                  'ReplaceOnlyLevenshteinPreprocessor': {'target_ratio': 0.8358750604197434, 'use_short_name': False}, 
                  'WordRankRatioPreprocessor': {'target_ratio': 0.9409659060288595, 'language': 'en', 'use_short_name': False},
                  'DependencyTreeDepthRatioPreprocessor': {'target_ratio': 0.621889800579547, 'language': 'en', 'use_short_name': False}, 
                  'GPT2BPEPreprocessor': {}}


                 )

# 0825

add_item_to_dict(model_id=len(model_dir_dict),
                 exp_dir=MODEL_DIR+'local_1629750798219/',
                 model_name='muss_cooc_0822', # delete simple sentence not contain '.', and any of word in simple not in complex
                 dataset='wikilarge_cooc_0823',
                 info=
                 '''
                 train complete
                 use_asset=True.
                 load muss_mined.
                 ''',
                # simple NE
recommended_preprocessors_kwargs=
{'LengthRatioPreprocessor': {'target_ratio': 0.9449211615293195, 'use_short_name': False}, 
 'ReplaceOnlyLevenshteinPreprocessor': {'target_ratio': 0.8248051135114383, 'use_short_name': False}, 
 'WordRankRatioPreprocessor': {'target_ratio': 0.9249977217558367, 'language': 'en', 'use_short_name': False}, 
 'DependencyTreeDepthRatioPreprocessor': {'target_ratio': 0.3694293245030373, 'language': 'en', 'use_short_name': False}, 
 'GPT2BPEPreprocessor': {}}



                 )

# 0826
# early stop
add_item_to_dict(model_id=len(model_dir_dict),
                 exp_dir=MODEL_DIR+'local_1629763944392/',
                 model_name='0823_train_simple_2BART_2RNN_together', 
                 dataset='wikilarge_final_simple_0821',
                 info=
                 '''
                 train complete
                 use_asset=True.
                 load muss_mined.
                 ''',
                # simple NE
recommended_preprocessors_kwargs=
{'LengthRatioPreprocessor': {'target_ratio': 1.2158014432652016, 'use_short_name': False}, 
 'ReplaceOnlyLevenshteinPreprocessor': {'target_ratio': 0.7184432274104291, 'use_short_name': False}, 
 'WordRankRatioPreprocessor': {'target_ratio': 1.444762809665763, 'language': 'en', 'use_short_name': False},
 'DependencyTreeDepthRatioPreprocessor': {'target_ratio': 1.1892137216162368, 'language': 'en', 'use_short_name': False}, 
 'GPT2BPEPreprocessor': {}}


                 )

# 0826
# choose randomly one to to evalution since loss is pretty low compared to the begining 
add_item_to_dict(model_id=len(model_dir_dict),
                 exp_dir=MODEL_DIR+'local_1629841731976/',
                 model_name='0823_train_together_for2_then_fix_RNN', 
                 dataset='wikilarge_final_simple_0821',
                 info=
                 '''
                 train complete
                 use_asset=True.
                 load muss_mined.
                 ''',
                 generate_use_checkpoint_dir='/content/drive/MyDrive/muss/experiments/fairseq/local_1629841731976/checkpoints/checkpoint12.pt',
                # simple NE
recommended_preprocessors_kwargs=
{'LengthRatioPreprocessor': {'target_ratio': 1.487917569735402, 'use_short_name': False}, 
 'ReplaceOnlyLevenshteinPreprocessor': {'target_ratio': 0.8763562448202163, 'use_short_name': False}, 
 'WordRankRatioPreprocessor': {'target_ratio': 0.9380293764647682, 'language': 'en', 'use_short_name': False}, 
 'DependencyTreeDepthRatioPreprocessor': {'target_ratio': 1.3320036079739315, 'language': 'en', 'use_short_name': False}, 
 'GPT2BPEPreprocessor': {}}



                 )

# 0827
# train for 3 epochs

add_item_to_dict(model_id=len(model_dir_dict),
                 exp_dir=MODEL_DIR+'local_1630019050042/',
                 model_name='0826_train_together_model_simple_0_0_0_para_1', 
                 dataset='wikilarge_final_simple_0821',
                 info=
                 '''
                 train complete
                 use_asset=True.
                 load muss_mined.
                 ''',
                # simple NE @ 3 epochs
                  
recommended_preprocessors_kwargs=
{'LengthRatioPreprocessor': {'target_ratio': 1.124169888991001, 'use_short_name': False}, 
 'ReplaceOnlyLevenshteinPreprocessor': {'target_ratio': 0.4667328463927494, 'use_short_name': False}, 
 'WordRankRatioPreprocessor': {'target_ratio': 1.3922614814378658, 'language': 'en', 'use_short_name': False}, 
 'DependencyTreeDepthRatioPreprocessor': {'target_ratio': 0.21508139243375501, 'language': 'en', 'use_short_name': False}, 
 'GPT2BPEPreprocessor': {}}

                 )

# 0827
# train for 2 epochs

add_item_to_dict(model_id=len(model_dir_dict),
                 exp_dir=MODEL_DIR+'local_1630019050042/',
                 model_name='0826_train_together_model_simple_0_0_0_para_1', 
                 dataset='wikilarge_final_simple_0821',
                 info=
                 '''
                 train complete
                 use_asset=True.
                 load muss_mined.
                 ''',
                 generate_use_checkpoint_dir='/content/drive/MyDrive/muss/experiments/fairseq/local_1630019050042/checkpoints/checkpoint2.pt',
                # simple NE @ 2 epochs
                  
recommended_preprocessors_kwargs=
{'LengthRatioPreprocessor': {'target_ratio': 1.4183927088210322, 'use_short_name': False}, 
 'ReplaceOnlyLevenshteinPreprocessor': {'target_ratio': 0.46991609764759845, 'use_short_name': False}, 
 'WordRankRatioPreprocessor': {'target_ratio': 1.1101461716597665, 'language': 'en', 'use_short_name': False}, 
 'DependencyTreeDepthRatioPreprocessor': {'target_ratio': 1.0879423219775408, 'language': 'en', 'use_short_name': False}, 
 'GPT2BPEPreprocessor': {}}
                 )

# 0827
# train for 3 epochs

add_item_to_dict(model_id=len(model_dir_dict),
                 exp_dir=MODEL_DIR+'local_1630019555772/',
                 model_name='0826_train_muss_together_0_0_0_para_1', 
                 dataset='wikilarge_final_simple_0821',
                 info=
                 '''
                 train complete
                 use_asset=True.
                 load muss_mined.
                 ''',
                # simple NE
recommended_preprocessors_kwargs=
{'LengthRatioPreprocessor': {'target_ratio': 1.0926432098556644, 'use_short_name': False}, 
 'ReplaceOnlyLevenshteinPreprocessor': {'target_ratio': 0.6555919488220876, 'use_short_name': False}, 
 'WordRankRatioPreprocessor': {'target_ratio': 1.1239251594138633, 'language': 'en', 'use_short_name': False}, 
 'DependencyTreeDepthRatioPreprocessor': {'target_ratio': 1.474779058586391, 'language': 'en', 'use_short_name': False}, 
 'GPT2BPEPreprocessor': {}}

                 )

# 0827
# train for 2 epochs

add_item_to_dict(model_id=len(model_dir_dict),
                 exp_dir=MODEL_DIR+'local_1630019555772/',
                 model_name='0826_train_muss_together_0_0_0_para_1', 
                 dataset='wikilarge_final_simple_0821',
                 info=
                 '''
                 train complete
                 use_asset=True.
                 load muss_mined.
                 ''',
                 generate_use_checkpoint_dir='/content/drive/MyDrive/muss/experiments/fairseq/local_1630019555772/checkpoints/checkpoint2.pt',
                # simple NE
recommended_preprocessors_kwargs=
{'LengthRatioPreprocessor': {'target_ratio': 1.4183927088210322, 'use_short_name': False}, 
 'ReplaceOnlyLevenshteinPreprocessor': {'target_ratio': 0.46991609764759845, 'use_short_name': False}, 
 'WordRankRatioPreprocessor': {'target_ratio': 1.1101461716597665, 'language': 'en', 'use_short_name': False}, 
 'DependencyTreeDepthRatioPreprocessor': {'target_ratio': 1.0879423219775408, 'language': 'en', 'use_short_name': False}, 
 'GPT2BPEPreprocessor': {}}

                 )

# 0827
# train for 3 epochs

add_item_to_dict(model_id=len(model_dir_dict),
                 exp_dir=MODEL_DIR+'local_1630021031879/',
                 model_name='0826_train_together_model_simple_0_0_0_para_0_1', 
                 dataset='wikilarge_final_simple_0821',
                 info=
                 '''
                 train complete
                 use_asset=True.
                 load muss_mined.
                 ''',
                # simple NE
recommended_preprocessors_kwargs=
{'LengthRatioPreprocessor': {'target_ratio': 1.0798647080199697, 'use_short_name': False}, 
 'ReplaceOnlyLevenshteinPreprocessor': {'target_ratio': 0.3381431454564642, 'use_short_name': False}, 
 'WordRankRatioPreprocessor': {'target_ratio': 1.1412509349407423, 'language': 'en', 'use_short_name': False},
 'DependencyTreeDepthRatioPreprocessor': {'target_ratio': 1.1007668301973916, 'language': 'en', 'use_short_name': False},
 'GPT2BPEPreprocessor': {}}


                 )

# 0827
# train for 2 epochs

add_item_to_dict(model_id=len(model_dir_dict),
                 exp_dir=MODEL_DIR+'local_1630021031879/',
                 model_name='0826_train_together_model_simple_0_0_0_para_0_1', 
                 dataset='wikilarge_final_simple_0821',
                 info=
                 '''
                 train complete
                 use_asset=True.
                 load muss_mined.
                 ''',
                 generate_use_checkpoint_dir='/content/drive/MyDrive/muss/experiments/fairseq/local_1630021031879/checkpoints/checkpoint2.pt',
                # simple NE

                 )

# 0829

add_item_to_dict(model_id=len(model_dir_dict),
                 exp_dir=MODEL_DIR+'local_1630093363664/',
                 model_name='muss_train_0827_ABCD', 
                 dataset='C2C1B2_wikilarge',
                 info=
                 '''
                 train complete
                 use_asset=True.
                 load muss_mined.
                 ''',
                #  generate_use_checkpoint_dir='/content/drive/MyDrive/muss/experiments/fairseq/local_1630021031879/checkpoints/checkpoint2.pt',
                # find para using ABCD asset
recommended_preprocessors_kwargs=
{'LengthRatioPreprocessor': {'target_ratio': 0.9266386739130521, 'use_short_name': False}, 
 'ReplaceOnlyLevenshteinPreprocessor': {'target_ratio': 0.8221555944889615, 'use_short_name': False}, 
 'WordRankRatioPreprocessor': {'target_ratio': 0.8824017602688162, 'language': 'en', 'use_short_name': False}, 
 'DependencyTreeDepthRatioPreprocessor': {'target_ratio': 0.27765722850669167, 'language': 'en', 'use_short_name': False},
 'GPT2BPEPreprocessor': {}}

                 )

# 33 


add_item_to_dict(model_id=len(model_dir_dict),
                 exp_dir=MODEL_DIR+'local_1630195482716/',
                 model_name='0828_train_model_together_0_0_0_para_0_1_drop_2', 
                 dataset='wikilarge_final_simple_0821',
                 info=
                 '''
                 train complete
                 use_asset=True.
                 load muss_mined.
                 ''',
                #  generate_use_checkpoint_dir='/content/drive/MyDrive/muss/experiments/fairseq/local_1630021031879/checkpoints/checkpoint2.pt',
                # simple NE
recommended_preprocessors_kwargs=
{'LengthRatioPreprocessor': {'target_ratio': 1.5, 'use_short_name': False}, 
 'ReplaceOnlyLevenshteinPreprocessor': {'target_ratio': 1.0, 'use_short_name': False}, 
 'WordRankRatioPreprocessor': {'target_ratio': 1.5, 'language': 'en', 'use_short_name': False}, 
 'DependencyTreeDepthRatioPreprocessor': {'target_ratio': 1.0844414998933403, 'language': 'en', 'use_short_name': False},
 'GPT2BPEPreprocessor': {}}
                 )

# 0830 

add_item_to_dict(model_id=len(model_dir_dict),
                 exp_dir=MODEL_DIR+'local_1630152576723/',
                 model_name='muss_train_0828_na_include_ABCD', 
                 dataset='C2C1B2_wikilarge_all',
                 info=
                 '''
                 train complete
                 use_asset=True.
                 load muss_mined.
                 ''',
                #  generate_use_checkpoint_dir='/content/drive/MyDrive/muss/experiments/fairseq/local_1630021031879/checkpoints/checkpoint2.pt',
                # find para using ABCD asset
recommended_preprocessors_kwargs=
{'LengthRatioPreprocessor': {'target_ratio': 1.2749009548425654, 'use_short_name': False}, 
 'ReplaceOnlyLevenshteinPreprocessor': {'target_ratio': 0.7693734277221083, 'use_short_name': False}, 
 'WordRankRatioPreprocessor': {'target_ratio': 0.8541376616472772, 'language': 'en', 'use_short_name': False}, 
 'DependencyTreeDepthRatioPreprocessor': {'target_ratio': 0.42718717259268596, 'language': 'en', 'use_short_name': False}, 
 'GPT2BPEPreprocessor': {}}


                 )

add_item_to_dict(model_id=len(model_dir_dict),
                 exp_dir=MODEL_DIR+'local_1630338618428/',
                 model_name='BART_RNN(GRU)_0830_train_model_together_para0_5_epoch2_noBCEwhenvalid', 
                 dataset='wikilarge_final_simple_0821',
                 info=
                 '''
                 train complete
                 use_asset=True.
                 load muss_mined.
                 ''',
                #  generate_use_checkpoint_dir='/content/drive/MyDrive/muss/experiments/fairseq/local_1630021031879/checkpoints/checkpoint2.pt',
                # simple NE
recommended_preprocessors_kwargs=
{'LengthRatioPreprocessor': {'target_ratio': 1.234376999334771, 'use_short_name': False}, 
 'ReplaceOnlyLevenshteinPreprocessor': {'target_ratio': 0.676282864018335, 'use_short_name': False}, 
 'WordRankRatioPreprocessor': {'target_ratio': 0.9242397792347032, 'language': 'en', 'use_short_name': False}, 
 'DependencyTreeDepthRatioPreprocessor': {'target_ratio': 1.2062136513260375, 'language': 'en', 'use_short_name': False}, 
 'GPT2BPEPreprocessor': {}}
                 )

# 33 


add_item_to_dict(model_id=len(model_dir_dict),
                 exp_dir=MODEL_DIR+'local_1630345977698/',
                 model_name='BART_RNN(GRU)_0830_train_model_together_para0_5_fix1_1_epoch2_noBCEwhenvalid', 
                 dataset='wikilarge_final_simple_0821',
                 info=
                 '''
                 train complete
                 use_asset=True.
                 load muss_mined.
                 ''',
                #  generate_use_checkpoint_dir='/content/drive/MyDrive/muss/experiments/fairseq/local_1630021031879/checkpoints/checkpoint2.pt',
                # simple NE
recommended_preprocessors_kwargs=
{'LengthRatioPreprocessor': {'target_ratio': 0.6613155581718627, 'use_short_name': False}, 
 'ReplaceOnlyLevenshteinPreprocessor': {'target_ratio': 0.4719725671806245, 'use_short_name': False}, 
 'WordRankRatioPreprocessor': {'target_ratio': 1.3598832598362611, 'language': 'en', 'use_short_name': False}, 
 'DependencyTreeDepthRatioPreprocessor': {'target_ratio': 1.3373193777099142, 'language': 'en', 'use_short_name': False}, 'GPT2BPEPreprocessor': {}}
                 )

# ABCD
add_item_to_dict(model_id=len(model_dir_dict),
                 exp_dir=MODEL_DIR+'local_1631390882572/',
                 model_name='muss_train_0911_na_include_ABCD', 
                 dataset='0911_ABCD_wikilarge_inc_na',
                 info=
                 '''
                 train complete
                 use_asset=True.
                 load muss_mined.
                 ''',

recommended_preprocessors_kwargs=
{'LengthRatioPreprocessor': {'target_ratio': 1.1836030880472763, 'use_short_name': False},
 'ReplaceOnlyLevenshteinPreprocessor': {'target_ratio': 0.7837758794547234, 'use_short_name': False}, 
 'WordRankRatioPreprocessor': {'target_ratio': 0.44310224347277266, 'language': 'en', 'use_short_name': False},
 'DependencyTreeDepthRatioPreprocessor': {'target_ratio': 0.5450343251841754, 'language': 'en', 'use_short_name': False},
 'GPT2BPEPreprocessor': {}}

                 )

# ABCD
add_item_to_dict(model_id=len(model_dir_dict),
                 exp_dir=MODEL_DIR+'local_1631390826128/',
                 model_name='muss_train_0911_na_no_include_ABCD', 
                 dataset='0911_ABCD_wikilarge_no_inc_na',
                 info=
                 '''
                 train complete
                 use_asset=True.
                 load muss_mined.
                 ''',
recommended_preprocessors_kwargs=
{'LengthRatioPreprocessor': {'target_ratio': 1.1372158070365654, 'use_short_name': False}, 
 'ReplaceOnlyLevenshteinPreprocessor': {'target_ratio': 0.5941372258967513, 'use_short_name': False},
 'WordRankRatioPreprocessor': {'target_ratio': 0.4846390863544413, 'language': 'en', 'use_short_name': False}, 
 'DependencyTreeDepthRatioPreprocessor': {'target_ratio': 0.8201457718035017, 'language': 'en', 'use_short_name': False},
 'GPT2BPEPreprocessor': {}}

                 )

# both new
add_item_to_dict(model_id=len(model_dir_dict),
                 exp_dir=MODEL_DIR+'local_1629750798219/',
                 model_name='muss_cooc_0822', 
                 dataset='wikilarge_cooc_0823',
                 info=
                 '''
                 train complete
                 use_asset=True.
                 load muss_mined.
                 ''',
                #  generate_use_checkpoint_dir='/content/drive/MyDrive/muss/experiments/fairseq/local_1630021031879/checkpoints/checkpoint2.pt',
                # find para using ABCD asset
recommended_preprocessors_kwargs=
{'LengthRatioPreprocessor': {'target_ratio': 1.0504094095374035, 'use_short_name': False}, 
 'ReplaceOnlyLevenshteinPreprocessor': {'target_ratio': 0.7753307276413414, 'use_short_name': False},
 'WordRankRatioPreprocessor': {'target_ratio': 0.596112042588367, 'language': 'en', 'use_short_name': False}, 
 'DependencyTreeDepthRatioPreprocessor': {'target_ratio': 0.20027131014394287, 'language': 'en', 'use_short_name': False},
 'GPT2BPEPreprocessor': {}}

                 )

add_item_to_dict(model_id=len(model_dir_dict),
                 exp_dir=MODEL_DIR+'local_1631567843618/',
                 model_name='muss_train_0913_ABCD_NER', 
                 dataset='0913_ABCD_NER_wikilarge',
                 info=
                 '''
                 train complete
                 use_asset=True.
                 load muss_mined.
                 ''',

recommended_preprocessors_kwargs=
{'LengthRatioPreprocessor': {'target_ratio': 0.8362634626138021, 'use_short_name': False}, 
 'ReplaceOnlyLevenshteinPreprocessor': {'target_ratio': 0.7719594617931818, 'use_short_name': False},
 'WordRankRatioPreprocessor': {'target_ratio': 0.8373903360773232, 'language': 'en', 'use_short_name': False},
 'DependencyTreeDepthRatioPreprocessor': {'target_ratio': 0.7178625295578446, 'language': 'en', 'use_short_name': False}, 
 'GPT2BPEPreprocessor': {}}


                 )

model_dir_dict

"""# baseline

## asset test
"""

# NE(simple) retention rate

bleu_list, sari_list, fkgl_list,NE_retain_mean_list,NE_retain_std_list = [],[],[],[],[]

for i in range(10):
  dir = '/content/drive/MyDrive/muss/resources/datasets/asset/test.simple.'+str(i)
  bleu, sari, fkgl,NE_retain_mean,NE_retain_std = evaluate_my_model(dir,0,0,0,0,muss=True)
  bleu_list.append(bleu)
  sari_list.append(sari)
  fkgl_list.append(fkgl)
  NE_retain_mean_list.append(NE_retain_mean)
  NE_retain_std_list.append(NE_retain_std)

asset_result = [bleu_list, sari_list, fkgl_list,NE_retain_mean_list,NE_retain_std_list]

with open('/content/drive/MyDrive/muss/qualitative/asset_evaluation', 'wb') as fp:
    pickle.dump(asset_result, fp)

# NE(complex) retention rate

bleu_list, sari_list, fkgl_list,NE_retain_mean_list,NE_retain_std_list = [],[],[],[],[]

for i in range(10):
  dir = '/content/drive/MyDrive/muss/resources/datasets/asset/test.simple.'+str(i)
  bleu, sari, fkgl,NE_retain_mean,NE_retain_std = evaluate_my_model(dir,0,0,0,0,muss=True,simple_or_complex='complex')
  bleu_list.append(bleu)
  sari_list.append(sari)
  fkgl_list.append(fkgl)
  NE_retain_mean_list.append(NE_retain_mean)
  NE_retain_std_list.append(NE_retain_std)

asset_result = [bleu_list, sari_list, fkgl_list,NE_retain_mean_list,NE_retain_std_list]

with open('/content/drive/MyDrive/muss/qualitative/asset_evaluation_complex_NE', 'wb') as fp:
    pickle.dump(asset_result, fp)

# CERF hard words retention rate

bleu_list, sari_list, fkgl_list,NE_retain_mean_list,NE_retain_std_list = [],[],[],[],[]

for i in range(10):
  dir = '/content/drive/MyDrive/muss/resources/datasets/asset/test.simple.'+str(i)
  bleu, sari, fkgl,NE_retain_mean,NE_retain_std = evaluate_my_model(dir,0,0,0,0,ABCD=True,muss=True)
  bleu_list.append(bleu)
  sari_list.append(sari)
  fkgl_list.append(fkgl)
  NE_retain_mean_list.append(NE_retain_mean)
  NE_retain_std_list.append(NE_retain_std)

asset_result_ABCD = [bleu_list, sari_list, fkgl_list,NE_retain_mean_list,NE_retain_std_list]

with open('/content/drive/MyDrive/muss/qualitative/asset_evaluation_ABCD', 'wb') as fp:
    pickle.dump(asset_result_ABCD, fp)



"""### SARI Leave one out"""

from easse.cli import report, get_orig_and_refs_sents, evaluate_system_output

from muss.utils.helpers import write_lines, get_temp_filepath

'''A simplifier is a function with signature: simplifier(complex_filepath, output_pred_filepath)'''

complete_ref_path = ''
for i in range(10):
  complete_ref_path += '/content/drive/MyDrive/muss/resources/datasets/asset/test.simple.'+str(i)
  if i != 9:
    complete_ref_path += ','
complete_ref_path_list = complete_ref_path.split(',')
complete_ref_path_list

test_set = 'custom'

bleu_list,fkgl_list,sari_list, sari_add_list,sari_del_list,sari_keep_list= [],[],[],[],[],[]

for i in range(10):

  sys_sents_path = complete_ref_path_list[i]
  orig_sents_path = '/content/drive/MyDrive/muss/resources/datasets/asset/test.complex'
  refs_sents_paths = ','.join(complete_ref_path_list[:i]+complete_ref_path_list[i+1:]+[complete_ref_path_list[i-1]])

  result = evaluate_system_output(
      test_set,
      sys_sents_path=sys_sents_path,
      orig_sents_path=orig_sents_path,
      refs_sents_paths=refs_sents_paths,
      metrics=['bleu','fkgl','sari', 'sari_by_operation'],
      quality_estimation=False,
  )

  bleu_list.append(result['bleu'])
  fkgl_list.append(result['fkgl'])
  sari_list.append(result['sari'])
  sari_add_list.append(result['sari_add'])
  sari_del_list.append(result['sari_del'])
  sari_keep_list.append(result['sari_keep'])

with open('/content/drive/MyDrive/muss/qualitative/SARI_ope_asset', 'wb') as fp:
    pickle.dump([sari_add_list,sari_keep_list,sari_del_list], fp)

np.mean(bleu_list),get_mean_confidence_interval(bleu_list)

np.mean(sari_list),get_mean_confidence_interval(sari_list)

np.mean(fkgl_list),get_mean_confidence_interval(fkgl_list)

np.mean(sari_add_list),get_mean_confidence_interval(sari_add_list)

np.mean(sari_keep_list),get_mean_confidence_interval(sari_keep_list)

np.mean(sari_del_list),get_mean_confidence_interval(sari_del_list)

print('asset test')
print('sari_add',round(np.mean(sari_add_list), 2),'±',round(get_mean_confidence_interval(sari_add_list), 2))
print('sari_keep_list',round(np.mean(sari_keep_list), 2),'±',round(get_mean_confidence_interval(sari_keep_list), 2))
print('sari_del_list',round(np.mean(sari_del_list), 2),'±',round(get_mean_confidence_interval(sari_del_list), 2))



"""## muss"""

MUSS_OUTPUT_DIR = '/content/drive/MyDrive/muss/muss_system_outputs/asset/test/'

cd '/content/drive/MyDrive/muss/muss_system_outputs/asset/test/'

muss_sys_output = os.listdir(os.getcwd())
muss_sys_output

muss_sys_output=['muss_bart_access_wikilarge_mined']

import ast

output_dir =  '/content/drive/MyDrive/muss/muss_system_outputs/asset/test/muss_bart_access_wikilarge_mined'
filenames = next(os.walk(output_dir), (None, None, []))[2] 

bleu_list, sari_list, fkgl_list,NE_retain_mean_list,NE_retain_std_list = [],[],[],[],[]
for file_name in filenames:
  
  dir = output_dir+'/'+file_name
  bleu, sari, fkgl,NE_retain_mean,NE_retain_std = evaluate_my_model(dir,0,0,0,0,muss=True)
  bleu_list.append(bleu)
  sari_list.append(sari)
  fkgl_list.append(fkgl)
  NE_retain_mean_list.append(NE_retain_mean)
  NE_retain_std_list.append(NE_retain_std)

muss_result = [bleu_list, sari_list, fkgl_list,NE_retain_mean_list,NE_retain_std_list]

with open('/content/drive/MyDrive/muss/qualitative/muss_evaluation', 'wb') as fp:
    pickle.dump(muss_result, fp)

NE_retain_mean_list

with open('/content/drive/MyDrive/muss/qualitative/muss_evaluation', 'rb') as fp:
    muss_output = pickle.load(fp)

np.mean(muss_output[0]),get_mean_confidence_interval(muss_output[0])

# CERF

output_dir =  '/content/drive/MyDrive/muss/muss_system_outputs/asset/test/muss_bart_access_wikilarge_mined'
filenames = next(os.walk(output_dir), (None, None, []))[2]  # [] if no file

bleu_list, sari_list, fkgl_list,NE_retain_mean_list,NE_retain_std_list = [],[],[],[],[]
for file_name in filenames:
  
  dir = output_dir+'/'+file_name
  bleu, sari, fkgl,NE_retain_mean,NE_retain_std = evaluate_my_model(dir,0,0,0,0,ABCD=True,muss=True)
  bleu_list.append(bleu)
  sari_list.append(sari)
  fkgl_list.append(fkgl)
  NE_retain_mean_list.append(NE_retain_mean)
  NE_retain_std_list.append(NE_retain_std)

muss_result_ABCD = [bleu_list, sari_list, fkgl_list,NE_retain_mean_list,NE_retain_std_list]

with open('/content/drive/MyDrive/muss/qualitative/muss_evaluation_ABCD', 'wb') as fp:
    pickle.dump(muss_result_ABCD, fp)

np.mean(muss_result_ABCD[-1]),get_mean_confidence_interval(muss_result_ABCD[-1])





# operation

output_dir =  '/content/drive/MyDrive/muss/muss_system_outputs/asset/test/muss_bart_access_wikilarge_mined'
filenames = next(os.walk(output_dir), (None, None, []))[2]  

sari_add_list, sari_keep_list, sari_del_list = [],[],[]
for file_name in filenames:
  
  dir = output_dir+'/'+file_name
  sari_add, sari_keep, sari_del = evaluate_my_model_operation(dir,0,0,'test',0,muss=True)
  sari_add_list.append(sari_add)
  sari_keep_list.append(sari_keep)
  sari_del_list.append(sari_del)

with open('/content/drive/MyDrive/muss/qualitative/SARI_ope_muss', 'wb') as fp:
    pickle.dump([sari_add_list,sari_keep_list,sari_del_list], fp)

np.mean(sari_add_list),get_mean_confidence_interval(sari_add_list)

np.mean(sari_keep_list),get_mean_confidence_interval(sari_keep_list)

np.mean(sari_del_list),get_mean_confidence_interval(sari_del_list)

# NE(complex) retention rate

output_dir =  '/content/drive/MyDrive/muss/muss_system_outputs/asset/test/muss_bart_access_wikilarge_mined'
filenames = next(os.walk(output_dir), (None, None, []))[2]  # [] if no file

# bleu_list_list, sari_list_list, fkgl_list_list,NE_retain_mean_list_list,NE_retain_std_list_list = [],[],[],[],[]
bleu_list, sari_list, fkgl_list,NE_retain_mean_list,NE_retain_std_list = [],[],[],[],[]
for file_name in filenames:
  
  dir = output_dir+'/'+file_name
  # for output_file_name in test_data_list:
  bleu, sari, fkgl,NE_retain_mean,NE_retain_std = evaluate_my_model(dir,0,0,0,0,muss=True,simple_or_complex='complex')
  bleu_list.append(bleu)
  sari_list.append(sari)
  fkgl_list.append(fkgl)
  NE_retain_mean_list.append(NE_retain_mean)
  NE_retain_std_list.append(NE_retain_std)

  # bleu_list_list += [bleu_list]
  # sari_list_list += [sari_list]
  # fkgl_list_list += [fkgl_list]
  # NE_retain_mean_list_list += [NE_retain_mean_list]
  # NE_retain_std_list_list += [NE_retain_std_list]

muss_result = [bleu_list, sari_list, fkgl_list,NE_retain_mean_list,NE_retain_std_list]

with open('/content/drive/MyDrive/muss/qualitative/muss_evaluation_complex_NE', 'wb') as fp:
    pickle.dump(muss_result, fp)

"""# operation perserving

## model 18
"""

# run the section define test set path (simple)
test_data_dir_list,test_data_list

# NE(simple) retention rate

i = 18
output_id = 'model_'+str(i)+'_'+model_dir_dict[i]['exp_dir'].split('/')[-2]

bleu_list_list, sari_list_list, fkgl_list_list,NE_retain_mean_list_list,NE_retain_std_list_list = [],[],[],[],[]
for generate_id_list in ['02','03','04','05','06']:
    
  dataset = 'asset'
  phase = 'test'

  bleu_list, sari_list, fkgl_list,NE_retain_mean_list,NE_retain_std_list = [],[],[],[],[]
  for output_file_name in test_data_list:
    bleu, sari, fkgl,NE_retain_mean,NE_retain_std = evaluate_my_model(output_id,generate_id_list,dataset,phase,output_file_name)
    bleu_list.append(bleu)
    sari_list.append(sari)
    fkgl_list.append(fkgl)
    NE_retain_mean_list.append(NE_retain_mean)
    NE_retain_std_list.append(NE_retain_std)
  
  bleu_list_list += [bleu_list]
  sari_list_list += [sari_list]
  fkgl_list_list += [fkgl_list]
  NE_retain_mean_list_list += [NE_retain_mean_list]
  NE_retain_std_list_list += [NE_retain_std_list]

model_18_result = [bleu_list_list, sari_list_list, fkgl_list_list,NE_retain_mean_list_list,NE_retain_std_list_list]

with open('/content/drive/MyDrive/muss/qualitative/model18_evaluation', 'wb') as fp:
    pickle.dump(model_18_result, fp)

# operation sari

i = 18
generate_id_list = ['02','03','04','05','06']
test_data_list = ['asset.simple.test']

output_id = 'model_'+str(i)+'_'+model_dir_dict[i]['exp_dir'].split('/')[-2] # model_18_local_1629593348299

sari_add_list, sari_keep_list, sari_del_list = [],[],[]

dataset = 'asset'
phase = 'test'

for generate_id in generate_id_list:
  
  
  for output_file_name in test_data_list:
    sari_add, sari_keep, sari_del = evaluate_my_model_operation(output_id,generate_id,dataset,phase,output_file_name)
    sari_add_list.append(sari_add)
    sari_keep_list.append(sari_keep)
    sari_del_list.append(sari_del)

np.mean(sari_add_list),get_mean_confidence_interval(sari_add_list)

np.mean(sari_keep_list),get_mean_confidence_interval(sari_keep_list)

np.mean(sari_del_list),get_mean_confidence_interval(sari_del_list)



# NE(complex) retention rate

i = 18
output_id = 'model_'+str(i)+'_'+model_dir_dict[i]['exp_dir'].split('/')[-2]

bleu_list_list, sari_list_list, fkgl_list_list,NE_retain_mean_list_list,NE_retain_std_list_list = [],[],[],[],[]
for generate_id_list in ['02','03','04','05','06']:
    
  dataset = 'asset'
  phase = 'test'

  bleu_list, sari_list, fkgl_list,NE_retain_mean_list,NE_retain_std_list = [],[],[],[],[]
  for output_file_name in test_data_list:
    bleu, sari, fkgl,NE_retain_mean,NE_retain_std = evaluate_my_model(output_id,generate_id_list,dataset,phase,output_file_name,simple_or_complex='complex')
    bleu_list.append(bleu)
    sari_list.append(sari)
    fkgl_list.append(fkgl)
    NE_retain_mean_list.append(NE_retain_mean)
    NE_retain_std_list.append(NE_retain_std)
  
  bleu_list_list += [bleu_list]
  sari_list_list += [sari_list]
  fkgl_list_list += [fkgl_list]
  NE_retain_mean_list_list += [NE_retain_mean_list]
  NE_retain_std_list_list += [NE_retain_std_list]

with open('/content/drive/MyDrive/muss/qualitative/model18_evaluation_all_complex_NE_retain_rate', 'wb') as fp:
    pickle.dump([bleu_list_list, sari_list_list, fkgl_list_list,NE_retain_mean_list_list,NE_retain_std_list_list], fp)

"""## model 18 complex"""

# run define test set path (complex)
test_data_dir_list,test_data_list

i = 18
output_id = 'model_'+str(i)+'_'+model_dir_dict[i]['exp_dir'].split('/')[-2]

bleu_list_list, sari_list_list, fkgl_list_list,NE_retain_mean_list_list,NE_retain_std_list_list = [],[],[],[],[]


for generate_id_list in ['07','08','09','10','11']:
    
  dataset = 'asset'
  phase = 'test'

  bleu_list, sari_list, fkgl_list,NE_retain_mean_list,NE_retain_std_list = [],[],[],[],[]
  for output_file_name in test_data_list:
    bleu, sari, fkgl,NE_retain_mean,NE_retain_std = evaluate_my_model(output_id,generate_id_list,dataset,phase,output_file_name)
    bleu_list.append(bleu)
    sari_list.append(sari)
    fkgl_list.append(fkgl)
    NE_retain_mean_list.append(NE_retain_mean)
    NE_retain_std_list.append(NE_retain_std)
  
  bleu_list_list += [bleu_list]
  sari_list_list += [sari_list]
  fkgl_list_list += [fkgl_list]
  NE_retain_mean_list_list += [NE_retain_mean_list]
  NE_retain_std_list_list += [NE_retain_std_list]

model_18_result = [bleu_list_list, sari_list_list, fkgl_list_list,NE_retain_mean_list_list,NE_retain_std_list_list]

with open('/content/drive/MyDrive/muss/qualitative/model18_evaluation_complex', 'wb') as fp:
    pickle.dump(model_18_result, fp)

with open('/content/drive/MyDrive/muss/qualitative/model18_evaluation_complex', 'rb') as fp:
    model_18_result = pickle.load(fp)

# NE(complex) retention rate

i = 18
output_id = 'model_'+str(i)+'_'+model_dir_dict[i]['exp_dir'].split('/')[-2]

bleu_list_list, sari_list_list, fkgl_list_list,NE_retain_mean_list_list,NE_retain_std_list_list = [],[],[],[],[]


for generate_id_list in ['07','08','09','10','11']:
    
  dataset = 'asset'
  phase = 'test'

  bleu_list, sari_list, fkgl_list,NE_retain_mean_list,NE_retain_std_list = [],[],[],[],[]
  for output_file_name in test_data_list:
    bleu, sari, fkgl,NE_retain_mean,NE_retain_std = evaluate_my_model(output_id,generate_id_list,dataset,phase,output_file_name,simple_or_complex='complex')
    bleu_list.append(bleu)
    sari_list.append(sari)
    fkgl_list.append(fkgl)
    NE_retain_mean_list.append(NE_retain_mean)
    NE_retain_std_list.append(NE_retain_std)
  
  bleu_list_list += [bleu_list]
  sari_list_list += [sari_list]
  fkgl_list_list += [fkgl_list]
  NE_retain_mean_list_list += [NE_retain_mean_list]
  NE_retain_std_list_list += [NE_retain_std_list]

model_18_result = [bleu_list_list, sari_list_list, fkgl_list_list,NE_retain_mean_list_list,NE_retain_std_list_list]

with open('/content/drive/MyDrive/muss/qualitative/model18_evaluation_complex_NE', 'wb') as fp:
    pickle.dump(model_18_result, fp)

"""## model 19"""

test_data_dir_list,test_data_list

i = 19
output_id = 'model_'+str(i)+'_'+model_dir_dict[i]['exp_dir'].split('/')[-2]

bleu_list_list, sari_list_list, fkgl_list_list,NE_retain_mean_list_list,NE_retain_std_list_list = [],[],[],[],[]
for generate_id_list in ['00','01','02','03','04']:#,'05','06']:
    
  dataset = 'asset'
  phase = 'test'

  bleu_list, sari_list, fkgl_list,NE_retain_mean_list,NE_retain_std_list = [],[],[],[],[]
  for output_file_name in test_data_list:
    bleu, sari, fkgl,NE_retain_mean,NE_retain_std = evaluate_my_model(output_id,generate_id_list,dataset,phase,output_file_name)
    bleu_list.append(bleu)
    sari_list.append(sari)
    fkgl_list.append(fkgl)
    NE_retain_mean_list.append(NE_retain_mean)
    NE_retain_std_list.append(NE_retain_std)
  
  bleu_list_list += [bleu_list]
  sari_list_list += [sari_list]
  fkgl_list_list += [fkgl_list]
  NE_retain_mean_list_list += [NE_retain_mean_list]
  NE_retain_std_list_list += [NE_retain_std_list]

model_19_result = [bleu_list_list, sari_list_list, fkgl_list_list,NE_retain_mean_list_list,NE_retain_std_list_list]

with open('/content/drive/MyDrive/muss/qualitative/model19_evaluation', 'wb') as fp:
    pickle.dump(model_19_result, fp)

i = 19
output_id = 'model_'+str(i)+'_'+model_dir_dict[i]['exp_dir'].split('/')[-2]

bleu_list_list, sari_list_list, fkgl_list_list,NE_retain_mean_list_list,NE_retain_std_list_list = [],[],[],[],[]
for generate_id_list in ['00','01','02','03','04']:#,'05','06']:
    
  dataset = 'asset'
  phase = 'test'

  bleu_list, sari_list, fkgl_list,NE_retain_mean_list,NE_retain_std_list = [],[],[],[],[]
  for output_file_name in test_data_list:
    bleu, sari, fkgl,NE_retain_mean,NE_retain_std = evaluate_my_model(output_id,generate_id_list,dataset,phase,output_file_name,simple_or_complex='complex')
    bleu_list.append(bleu)
    sari_list.append(sari)
    fkgl_list.append(fkgl)
    NE_retain_mean_list.append(NE_retain_mean)
    NE_retain_std_list.append(NE_retain_std)
  
  bleu_list_list += [bleu_list]
  sari_list_list += [sari_list]
  fkgl_list_list += [fkgl_list]
  NE_retain_mean_list_list += [NE_retain_mean_list]
  NE_retain_std_list_list += [NE_retain_std_list]

with open('/content/drive/MyDrive/muss/qualitative/model19_evaluation_all_complex_NE_retain_rate', 'wb') as fp:
    pickle.dump([bleu_list_list, sari_list_list, fkgl_list_list,NE_retain_mean_list_list,NE_retain_std_list_list], fp)

"""## model 19 complex"""

test_data_dir_list,test_data_list

i = 19
output_id = 'model_'+str(i)+'_'+model_dir_dict[i]['exp_dir'].split('/')[-2]

bleu_list_list, sari_list_list, fkgl_list_list,NE_retain_mean_list_list,NE_retain_std_list_list = [],[],[],[],[]
for generate_id_list in ['05','06','07','08','09']:#,'05','06']:
    
  dataset = 'asset'
  phase = 'test'

  bleu_list, sari_list, fkgl_list,NE_retain_mean_list,NE_retain_std_list = [],[],[],[],[]
  for output_file_name in test_data_list:
    bleu, sari, fkgl,NE_retain_mean,NE_retain_std = evaluate_my_model(output_id,generate_id_list,dataset,phase,output_file_name)
    bleu_list.append(bleu)
    sari_list.append(sari)
    fkgl_list.append(fkgl)
    NE_retain_mean_list.append(NE_retain_mean)
    NE_retain_std_list.append(NE_retain_std)
  
  bleu_list_list += [bleu_list]
  sari_list_list += [sari_list]
  fkgl_list_list += [fkgl_list]
  NE_retain_mean_list_list += [NE_retain_mean_list]
  NE_retain_std_list_list += [NE_retain_std_list]

model_19_result = [bleu_list_list, sari_list_list, fkgl_list_list,NE_retain_mean_list_list,NE_retain_std_list_list]

with open('/content/drive/MyDrive/muss/qualitative/model19_evaluation_complex', 'wb') as fp:
    pickle.dump(model_19_result, fp)



i = 19
output_id = 'model_'+str(i)+'_'+model_dir_dict[i]['exp_dir'].split('/')[-2]

bleu_list_list, sari_list_list, fkgl_list_list,NE_retain_mean_list_list,NE_retain_std_list_list = [],[],[],[],[]
for generate_id_list in ['05','06','07','08','09']:#,'05','06']:
    
  dataset = 'asset'
  phase = 'test'

  bleu_list, sari_list, fkgl_list,NE_retain_mean_list,NE_retain_std_list = [],[],[],[],[]
  for output_file_name in test_data_list:
    bleu, sari, fkgl,NE_retain_mean,NE_retain_std = evaluate_my_model(output_id,generate_id_list,dataset,phase,output_file_name,simple_or_complex='complex')
    bleu_list.append(bleu)
    sari_list.append(sari)
    fkgl_list.append(fkgl)
    NE_retain_mean_list.append(NE_retain_mean)
    NE_retain_std_list.append(NE_retain_std)
  
  bleu_list_list += [bleu_list]
  sari_list_list += [sari_list]
  fkgl_list_list += [fkgl_list]
  NE_retain_mean_list_list += [NE_retain_mean_list]
  NE_retain_std_list_list += [NE_retain_std_list]

model_19_result = [bleu_list_list, sari_list_list, fkgl_list_list,NE_retain_mean_list_list,NE_retain_std_list_list]

with open('/content/drive/MyDrive/muss/qualitative/model19_evaluation_complex_NE', 'wb') as fp:
    pickle.dump(model_19_result, fp)

"""## model 20"""

test_data_dir_list,test_data_list

i = 20
output_id = 'model_'+str(i)+'_'+model_dir_dict[i]['exp_dir'].split('/')[-2]

bleu_list_list, sari_list_list, fkgl_list_list,NE_retain_mean_list_list,NE_retain_std_list_list = [],[],[],[],[]
for generate_id_list in ['00','01','02','03','05']:#,'05','06']:
    
  dataset = 'asset'
  phase = 'test'

  bleu_list, sari_list, fkgl_list,NE_retain_mean_list,NE_retain_std_list = [],[],[],[],[]
  for output_file_name in test_data_list:
    bleu, sari, fkgl,NE_retain_mean,NE_retain_std = evaluate_my_model(output_id,generate_id_list,dataset,phase,output_file_name)
    bleu_list.append(bleu)
    sari_list.append(sari)
    fkgl_list.append(fkgl)
    NE_retain_mean_list.append(NE_retain_mean)
    NE_retain_std_list.append(NE_retain_std)
  
  bleu_list_list += [bleu_list]
  sari_list_list += [sari_list]
  fkgl_list_list += [fkgl_list]
  NE_retain_mean_list_list += [NE_retain_mean_list]
  NE_retain_std_list_list += [NE_retain_std_list]

model_20_result = [bleu_list_list, sari_list_list, fkgl_list_list,NE_retain_mean_list_list,NE_retain_std_list_list]

with open('/content/drive/MyDrive/muss/qualitative/model20_evaluation', 'wb') as fp:
    pickle.dump(model_20_result, fp)





i = 20
output_id = 'model_'+str(i)+'_'+model_dir_dict[i]['exp_dir'].split('/')[-2]

bleu_list_list, sari_list_list, fkgl_list_list,NE_retain_mean_list_list,NE_retain_std_list_list = [],[],[],[],[]
for generate_id_list in ['00','01','02','03','05']:#,'05','06']:
    
  dataset = 'asset'
  phase = 'test'

  bleu_list, sari_list, fkgl_list,NE_retain_mean_list,NE_retain_std_list = [],[],[],[],[]
  for output_file_name in test_data_list:
    bleu, sari, fkgl,NE_retain_mean,NE_retain_std = evaluate_my_model(output_id,generate_id_list,dataset,phase,output_file_name,simple_or_complex='complex')
    bleu_list.append(bleu)
    sari_list.append(sari)
    fkgl_list.append(fkgl)
    NE_retain_mean_list.append(NE_retain_mean)
    NE_retain_std_list.append(NE_retain_std)
  
  bleu_list_list += [bleu_list]
  sari_list_list += [sari_list]
  fkgl_list_list += [fkgl_list]
  NE_retain_mean_list_list += [NE_retain_mean_list]
  NE_retain_std_list_list += [NE_retain_std_list]

with open('/content/drive/MyDrive/muss/qualitative/model20_evaluation_all_complex_NE_retain_rate', 'wb') as fp:
    pickle.dump([bleu_list_list, sari_list_list, fkgl_list_list,NE_retain_mean_list_list,NE_retain_std_list_list], fp)

"""## model 20 complex"""

i = 20
output_id = 'model_'+str(i)+'_'+model_dir_dict[i]['exp_dir'].split('/')[-2]

bleu_list_list, sari_list_list, fkgl_list_list,NE_retain_mean_list_list,NE_retain_std_list_list = [],[],[],[],[]
for generate_id_list in ['06','07','08','09','10']:#,'05','06']:
    
  dataset = 'asset'
  phase = 'test'

  bleu_list, sari_list, fkgl_list,NE_retain_mean_list,NE_retain_std_list = [],[],[],[],[]
  for output_file_name in test_data_list:
    bleu, sari, fkgl,NE_retain_mean,NE_retain_std = evaluate_my_model(output_id,generate_id_list,dataset,phase,output_file_name)
    bleu_list.append(bleu)
    sari_list.append(sari)
    fkgl_list.append(fkgl)
    NE_retain_mean_list.append(NE_retain_mean)
    NE_retain_std_list.append(NE_retain_std)
  
  bleu_list_list += [bleu_list]
  sari_list_list += [sari_list]
  fkgl_list_list += [fkgl_list]
  NE_retain_mean_list_list += [NE_retain_mean_list]
  NE_retain_std_list_list += [NE_retain_std_list]

model_20_result = [bleu_list_list, sari_list_list, fkgl_list_list,NE_retain_mean_list_list,NE_retain_std_list_list]

with open('/content/drive/MyDrive/muss/qualitative/model20_evaluation_complex', 'wb') as fp:
    pickle.dump(model_20_result, fp)





i = 20
output_id = 'model_'+str(i)+'_'+model_dir_dict[i]['exp_dir'].split('/')[-2]

bleu_list_list, sari_list_list, fkgl_list_list,NE_retain_mean_list_list,NE_retain_std_list_list = [],[],[],[],[]
for generate_id_list in ['06','07','08','09','10']:#,'05','06']:
    
  dataset = 'asset'
  phase = 'test'

  bleu_list, sari_list, fkgl_list,NE_retain_mean_list,NE_retain_std_list = [],[],[],[],[]
  for output_file_name in test_data_list:
    bleu, sari, fkgl,NE_retain_mean,NE_retain_std = evaluate_my_model(output_id,generate_id_list,dataset,phase,output_file_name,simple_or_complex='complex')
    bleu_list.append(bleu)
    sari_list.append(sari)
    fkgl_list.append(fkgl)
    NE_retain_mean_list.append(NE_retain_mean)
    NE_retain_std_list.append(NE_retain_std)
  
  bleu_list_list += [bleu_list]
  sari_list_list += [sari_list]
  fkgl_list_list += [fkgl_list]
  NE_retain_mean_list_list += [NE_retain_mean_list]
  NE_retain_std_list_list += [NE_retain_std_list]

model_20_result = [bleu_list_list, sari_list_list, fkgl_list_list,NE_retain_mean_list_list,NE_retain_std_list_list]

with open('/content/drive/MyDrive/muss/qualitative/model20_evaluation_complex_NE', 'wb') as fp:
    pickle.dump(model_20_result, fp)

"""## model 21"""

test_data_dir_list,test_data_list

i = 21
output_id = 'model_'+str(i)+'_'+model_dir_dict[i]['exp_dir'].split('/')[-2]

bleu_list_list, sari_list_list, fkgl_list_list,NE_retain_mean_list_list,NE_retain_std_list_list = [],[],[],[],[]
for generate_id_list in ['00','01','02','03','04']:
    
  dataset = 'asset'
  phase = 'test'

  bleu_list, sari_list, fkgl_list,NE_retain_mean_list,NE_retain_std_list = [],[],[],[],[]
  for output_file_name in test_data_list:
    bleu, sari, fkgl,NE_retain_mean,NE_retain_std = evaluate_my_model(output_id,generate_id_list,dataset,phase,output_file_name)
    bleu_list.append(bleu)
    sari_list.append(sari)
    fkgl_list.append(fkgl)
    NE_retain_mean_list.append(NE_retain_mean)
    NE_retain_std_list.append(NE_retain_std)
  
  bleu_list_list += [bleu_list]
  sari_list_list += [sari_list]
  fkgl_list_list += [fkgl_list]
  NE_retain_mean_list_list += [NE_retain_mean_list]
  NE_retain_std_list_list += [NE_retain_std_list]

model_21_result = [bleu_list_list, sari_list_list, fkgl_list_list,NE_retain_mean_list_list,NE_retain_std_list_list]

with open('/content/drive/MyDrive/muss/qualitative/model21_evaluation', 'wb') as fp:
    pickle.dump(model_21_result, fp)

i = 21
output_id = 'model_'+str(i)+'_'+model_dir_dict[i]['exp_dir'].split('/')[-2]

bleu_list_list, sari_list_list, fkgl_list_list,NE_retain_mean_list_list,NE_retain_std_list_list = [],[],[],[],[]
for generate_id_list in ['00','01','02','03','04']:
    
  dataset = 'asset'
  phase = 'test'

  bleu_list, sari_list, fkgl_list,NE_retain_mean_list,NE_retain_std_list = [],[],[],[],[]
  for output_file_name in test_data_list:
    bleu, sari, fkgl,NE_retain_mean,NE_retain_std = evaluate_my_model(output_id,generate_id_list,dataset,phase,output_file_name,simple_or_complex='complex')
    bleu_list.append(bleu)
    sari_list.append(sari)
    fkgl_list.append(fkgl)
    NE_retain_mean_list.append(NE_retain_mean)
    NE_retain_std_list.append(NE_retain_std)
  
  bleu_list_list += [bleu_list]
  sari_list_list += [sari_list]
  fkgl_list_list += [fkgl_list]
  NE_retain_mean_list_list += [NE_retain_mean_list]
  NE_retain_std_list_list += [NE_retain_std_list]

with open('/content/drive/MyDrive/muss/qualitative/model21_evaluation_all_complex_NE_retain_rate', 'wb') as fp:
    pickle.dump([bleu_list_list, sari_list_list, fkgl_list_list,NE_retain_mean_list_list,NE_retain_std_list_list], fp)

"""## model 21 complex"""

test_data_dir_list,test_data_list

i = 21
output_id = 'model_'+str(i)+'_'+model_dir_dict[i]['exp_dir'].split('/')[-2]

bleu_list_list, sari_list_list, fkgl_list_list,NE_retain_mean_list_list,NE_retain_std_list_list = [],[],[],[],[]
for generate_id_list in ['05','06','07','08','09']:
    
  dataset = 'asset'
  phase = 'test'

  bleu_list, sari_list, fkgl_list,NE_retain_mean_list,NE_retain_std_list = [],[],[],[],[]
  for output_file_name in test_data_list:
    bleu, sari, fkgl,NE_retain_mean,NE_retain_std = evaluate_my_model(output_id,generate_id_list,dataset,phase,output_file_name)
    bleu_list.append(bleu)
    sari_list.append(sari)
    fkgl_list.append(fkgl)
    NE_retain_mean_list.append(NE_retain_mean)
    NE_retain_std_list.append(NE_retain_std)
  
  bleu_list_list += [bleu_list]
  sari_list_list += [sari_list]
  fkgl_list_list += [fkgl_list]
  NE_retain_mean_list_list += [NE_retain_mean_list]
  NE_retain_std_list_list += [NE_retain_std_list]

model_21_result = [bleu_list_list, sari_list_list, fkgl_list_list,NE_retain_mean_list_list,NE_retain_std_list_list]

with open('/content/drive/MyDrive/muss/qualitative/model21_evaluation_complex', 'wb') as fp:
    pickle.dump(model_21_result, fp)





i = 21
output_id = 'model_'+str(i)+'_'+model_dir_dict[i]['exp_dir'].split('/')[-2]

bleu_list_list, sari_list_list, fkgl_list_list,NE_retain_mean_list_list,NE_retain_std_list_list = [],[],[],[],[]
for generate_id_list in ['05','06','07','08','09']:
    
  dataset = 'asset'
  phase = 'test'

  bleu_list, sari_list, fkgl_list,NE_retain_mean_list,NE_retain_std_list = [],[],[],[],[]
  for output_file_name in test_data_list:
    bleu, sari, fkgl,NE_retain_mean,NE_retain_std = evaluate_my_model(output_id,generate_id_list,dataset,phase,output_file_name,simple_or_complex='complex')
    bleu_list.append(bleu)
    sari_list.append(sari)
    fkgl_list.append(fkgl)
    NE_retain_mean_list.append(NE_retain_mean)
    NE_retain_std_list.append(NE_retain_std)
  
  bleu_list_list += [bleu_list]
  sari_list_list += [sari_list]
  fkgl_list_list += [fkgl_list]
  NE_retain_mean_list_list += [NE_retain_mean_list]
  NE_retain_std_list_list += [NE_retain_std_list]

model_21_result = [bleu_list_list, sari_list_list, fkgl_list_list,NE_retain_mean_list_list,NE_retain_std_list_list]

with open('/content/drive/MyDrive/muss/qualitative/model21_evaluation_complex_NE', 'wb') as fp:
    pickle.dump(model_21_result, fp)

"""## model 22"""

i = 22
output_id = 'model_'+str(i)+'_'+model_dir_dict[i]['exp_dir'].split('/')[-2]

bleu_list_list, sari_list_list, fkgl_list_list,NE_retain_mean_list_list,NE_retain_std_list_list = [],[],[],[],[]
for generate_id_list in ['00','01','02','03','04']:
    
  dataset = 'asset'
  phase = 'test'

  bleu_list, sari_list, fkgl_list,NE_retain_mean_list,NE_retain_std_list = [],[],[],[],[]
  for output_file_name in test_data_list:
    bleu, sari, fkgl,NE_retain_mean,NE_retain_std = evaluate_my_model(output_id,generate_id_list,dataset,phase,output_file_name)
    bleu_list.append(bleu)
    sari_list.append(sari)
    fkgl_list.append(fkgl)
    NE_retain_mean_list.append(NE_retain_mean)
    NE_retain_std_list.append(NE_retain_std)
  
  bleu_list_list += [bleu_list]
  sari_list_list += [sari_list]
  fkgl_list_list += [fkgl_list]
  NE_retain_mean_list_list += [NE_retain_mean_list]
  NE_retain_std_list_list += [NE_retain_std_list]

model_22_result = [bleu_list_list, sari_list_list, fkgl_list_list,NE_retain_mean_list_list,NE_retain_std_list_list]

with open('/content/drive/MyDrive/muss/qualitative/model22_evaluation', 'wb') as fp:
    pickle.dump(model_22_result, fp)





i = 22
output_id = 'model_'+str(i)+'_'+model_dir_dict[i]['exp_dir'].split('/')[-2]

bleu_list_list, sari_list_list, fkgl_list_list,NE_retain_mean_list_list,NE_retain_std_list_list = [],[],[],[],[]
for generate_id_list in ['00','01','02','03','04']:
    
  dataset = 'asset'
  phase = 'test'

  bleu_list, sari_list, fkgl_list,NE_retain_mean_list,NE_retain_std_list = [],[],[],[],[]
  for output_file_name in test_data_list:
    bleu, sari, fkgl,NE_retain_mean,NE_retain_std = evaluate_my_model(output_id,generate_id_list,dataset,phase,output_file_name,simple_or_complex='complex')
    bleu_list.append(bleu)
    sari_list.append(sari)
    fkgl_list.append(fkgl)
    NE_retain_mean_list.append(NE_retain_mean)
    NE_retain_std_list.append(NE_retain_std)
  
  bleu_list_list += [bleu_list]
  sari_list_list += [sari_list]
  fkgl_list_list += [fkgl_list]
  NE_retain_mean_list_list += [NE_retain_mean_list]
  NE_retain_std_list_list += [NE_retain_std_list]

with open('/content/drive/MyDrive/muss/qualitative/model22_evaluation_all_complex_NE_retain_rate', 'wb') as fp:
    pickle.dump([bleu_list_list, sari_list_list, fkgl_list_list,NE_retain_mean_list_list,NE_retain_std_list_list], fp)

"""## model 22 complex"""

i = 22
output_id = 'model_'+str(i)+'_'+model_dir_dict[i]['exp_dir'].split('/')[-2]

bleu_list_list, sari_list_list, fkgl_list_list,NE_retain_mean_list_list,NE_retain_std_list_list = [],[],[],[],[]
for generate_id_list in ['05','06','07','08','09']:
    
  dataset = 'asset'
  phase = 'test'

  bleu_list, sari_list, fkgl_list,NE_retain_mean_list,NE_retain_std_list = [],[],[],[],[]
  for output_file_name in test_data_list:
    bleu, sari, fkgl,NE_retain_mean,NE_retain_std = evaluate_my_model(output_id,generate_id_list,dataset,phase,output_file_name)
    bleu_list.append(bleu)
    sari_list.append(sari)
    fkgl_list.append(fkgl)
    NE_retain_mean_list.append(NE_retain_mean)
    NE_retain_std_list.append(NE_retain_std)
  
  bleu_list_list += [bleu_list]
  sari_list_list += [sari_list]
  fkgl_list_list += [fkgl_list]
  NE_retain_mean_list_list += [NE_retain_mean_list]
  NE_retain_std_list_list += [NE_retain_std_list]

model_22_result = [bleu_list_list, sari_list_list, fkgl_list_list,NE_retain_mean_list_list,NE_retain_std_list_list]

with open('/content/drive/MyDrive/muss/qualitative/model22_evaluation_complex', 'wb') as fp:
    pickle.dump(model_22_result, fp)





i = 22
output_id = 'model_'+str(i)+'_'+model_dir_dict[i]['exp_dir'].split('/')[-2]

bleu_list_list, sari_list_list, fkgl_list_list,NE_retain_mean_list_list,NE_retain_std_list_list = [],[],[],[],[]
for generate_id_list in ['05','06','07','08','09']:
    
  dataset = 'asset'
  phase = 'test'

  bleu_list, sari_list, fkgl_list,NE_retain_mean_list,NE_retain_std_list = [],[],[],[],[]
  for output_file_name in test_data_list:
    bleu, sari, fkgl,NE_retain_mean,NE_retain_std = evaluate_my_model(output_id,generate_id_list,dataset,phase,output_file_name,simple_or_complex='complex')
    bleu_list.append(bleu)
    sari_list.append(sari)
    fkgl_list.append(fkgl)
    NE_retain_mean_list.append(NE_retain_mean)
    NE_retain_std_list.append(NE_retain_std)
  
  bleu_list_list += [bleu_list]
  sari_list_list += [sari_list]
  fkgl_list_list += [fkgl_list]
  NE_retain_mean_list_list += [NE_retain_mean_list]
  NE_retain_std_list_list += [NE_retain_std_list]

model_22_result = [bleu_list_list, sari_list_list, fkgl_list_list,NE_retain_mean_list_list,NE_retain_std_list_list]

with open('/content/drive/MyDrive/muss/qualitative/model22_evaluation_complex_NE', 'wb') as fp:
    pickle.dump(model_22_result, fp)

"""## model 23"""

i = 23
output_id = 'model_'+str(i)+'_'+model_dir_dict[i]['exp_dir'].split('/')[-2]

bleu_list_list, sari_list_list, fkgl_list_list,NE_retain_mean_list_list,NE_retain_std_list_list = [],[],[],[],[]
for generate_id_list in ['00','01','02','03','04']:
    
  dataset = 'asset'
  phase = 'test'

  bleu_list, sari_list, fkgl_list,NE_retain_mean_list,NE_retain_std_list = [],[],[],[],[]
  for output_file_name in test_data_list:
    bleu, sari, fkgl,NE_retain_mean,NE_retain_std = evaluate_my_model(output_id,generate_id_list,dataset,phase,output_file_name)
    bleu_list.append(bleu)
    sari_list.append(sari)
    fkgl_list.append(fkgl)
    NE_retain_mean_list.append(NE_retain_mean)
    NE_retain_std_list.append(NE_retain_std)
  
  bleu_list_list += [bleu_list]
  sari_list_list += [sari_list]
  fkgl_list_list += [fkgl_list]
  NE_retain_mean_list_list += [NE_retain_mean_list]
  NE_retain_std_list_list += [NE_retain_std_list]

model_23_result = [bleu_list_list, sari_list_list, fkgl_list_list,NE_retain_mean_list_list,NE_retain_std_list_list]

with open('/content/drive/MyDrive/muss/qualitative/model23_evaluation', 'wb') as fp:
    pickle.dump(model_23_result, fp)



i = 23
output_id = 'model_'+str(i)+'_'+model_dir_dict[i]['exp_dir'].split('/')[-2]

bleu_list_list, sari_list_list, fkgl_list_list,NE_retain_mean_list_list,NE_retain_std_list_list = [],[],[],[],[]
for generate_id_list in ['00','01','02','03','04']:
    
  dataset = 'asset'
  phase = 'test'

  bleu_list, sari_list, fkgl_list,NE_retain_mean_list,NE_retain_std_list = [],[],[],[],[]
  for output_file_name in test_data_list:
    bleu, sari, fkgl,NE_retain_mean,NE_retain_std = evaluate_my_model(output_id,generate_id_list,dataset,phase,output_file_name,simple_or_complex='complex')
    bleu_list.append(bleu)
    sari_list.append(sari)
    fkgl_list.append(fkgl)
    NE_retain_mean_list.append(NE_retain_mean)
    NE_retain_std_list.append(NE_retain_std)
  
  bleu_list_list += [bleu_list]
  sari_list_list += [sari_list]
  fkgl_list_list += [fkgl_list]
  NE_retain_mean_list_list += [NE_retain_mean_list]
  NE_retain_std_list_list += [NE_retain_std_list]

with open('/content/drive/MyDrive/muss/qualitative/model23_evaluation_all_complex_NE_retain_rate', 'wb') as fp:
    pickle.dump([bleu_list_list, sari_list_list, fkgl_list_list,NE_retain_mean_list_list,NE_retain_std_list_list], fp)

"""## model 23 complex"""

i = 23
output_id = 'model_'+str(i)+'_'+model_dir_dict[i]['exp_dir'].split('/')[-2]

bleu_list_list, sari_list_list, fkgl_list_list,NE_retain_mean_list_list,NE_retain_std_list_list = [],[],[],[],[]
for generate_id_list in ['05','06','07','08','09']:
    
  dataset = 'asset'
  phase = 'test'

  bleu_list, sari_list, fkgl_list,NE_retain_mean_list,NE_retain_std_list = [],[],[],[],[]
  for output_file_name in test_data_list:
    bleu, sari, fkgl,NE_retain_mean,NE_retain_std = evaluate_my_model(output_id,generate_id_list,dataset,phase,output_file_name)
    bleu_list.append(bleu)
    sari_list.append(sari)
    fkgl_list.append(fkgl)
    NE_retain_mean_list.append(NE_retain_mean)
    NE_retain_std_list.append(NE_retain_std)
  
  bleu_list_list += [bleu_list]
  sari_list_list += [sari_list]
  fkgl_list_list += [fkgl_list]
  NE_retain_mean_list_list += [NE_retain_mean_list]
  NE_retain_std_list_list += [NE_retain_std_list]

model_23_result = [bleu_list_list, sari_list_list, fkgl_list_list,NE_retain_mean_list_list,NE_retain_std_list_list]

with open('/content/drive/MyDrive/muss/qualitative/model23_evaluation_complex', 'wb') as fp:
    pickle.dump(model_23_result, fp)







i = 23
output_id = 'model_'+str(i)+'_'+model_dir_dict[i]['exp_dir'].split('/')[-2]

bleu_list_list, sari_list_list, fkgl_list_list,NE_retain_mean_list_list,NE_retain_std_list_list = [],[],[],[],[]
for generate_id_list in ['05','06','07','08','09']:
    
  dataset = 'asset'
  phase = 'test'

  bleu_list, sari_list, fkgl_list,NE_retain_mean_list,NE_retain_std_list = [],[],[],[],[]
  for output_file_name in test_data_list:
    bleu, sari, fkgl,NE_retain_mean,NE_retain_std = evaluate_my_model(output_id,generate_id_list,dataset,phase,output_file_name,simple_or_complex='complex')
    bleu_list.append(bleu)
    sari_list.append(sari)
    fkgl_list.append(fkgl)
    NE_retain_mean_list.append(NE_retain_mean)
    NE_retain_std_list.append(NE_retain_std)
  
  bleu_list_list += [bleu_list]
  sari_list_list += [sari_list]
  fkgl_list_list += [fkgl_list]
  NE_retain_mean_list_list += [NE_retain_mean_list]
  NE_retain_std_list_list += [NE_retain_std_list]

model_23_result = [bleu_list_list, sari_list_list, fkgl_list_list,NE_retain_mean_list_list,NE_retain_std_list_list]

with open('/content/drive/MyDrive/muss/qualitative/model23_evaluation_complex_NE', 'wb') as fp:
    pickle.dump(model_23_result, fp)

"""## model 39"""

i = 39
output_id = 'model_'+str(i)+'_'+model_dir_dict[i]['exp_dir'].split('/')[-2]

bleu_list_list, sari_list_list, fkgl_list_list,NE_retain_mean_list_list,NE_retain_std_list_list = [],[],[],[],[]
for generate_id_list in ['00','01','02','03','04']:
    
  dataset = 'asset'
  phase = 'test'

  bleu_list, sari_list, fkgl_list,NE_retain_mean_list,NE_retain_std_list = [],[],[],[],[]
  for output_file_name in test_data_list:
    bleu, sari, fkgl,NE_retain_mean,NE_retain_std = evaluate_my_model(output_id,generate_id_list,dataset,phase,output_file_name)
    bleu_list.append(bleu)
    sari_list.append(sari)
    fkgl_list.append(fkgl)
    NE_retain_mean_list.append(NE_retain_mean)
    NE_retain_std_list.append(NE_retain_std)
  
  bleu_list_list += [bleu_list]
  sari_list_list += [sari_list]
  fkgl_list_list += [fkgl_list]
  NE_retain_mean_list_list += [NE_retain_mean_list]
  NE_retain_std_list_list += [NE_retain_std_list]

model_39_result = [bleu_list_list, sari_list_list, fkgl_list_list,NE_retain_mean_list_list,NE_retain_std_list_list]

with open('/content/drive/MyDrive/muss/qualitative/model39_evaluation', 'wb') as fp:
    pickle.dump(model_39_result, fp)



i = 39
output_id = 'model_'+str(i)+'_'+model_dir_dict[i]['exp_dir'].split('/')[-2]

bleu_list_list, sari_list_list, fkgl_list_list,NE_retain_mean_list_list,NE_retain_std_list_list = [],[],[],[],[]
for generate_id_list in ['00','01','02','03','04']:
    
  dataset = 'asset'
  phase = 'test'

  bleu_list, sari_list, fkgl_list,NE_retain_mean_list,NE_retain_std_list = [],[],[],[],[]
  for output_file_name in test_data_list:
    bleu, sari, fkgl,NE_retain_mean,NE_retain_std = evaluate_my_model(output_id,generate_id_list,dataset,phase,output_file_name,simple_or_complex='complex')
    bleu_list.append(bleu)
    sari_list.append(sari)
    fkgl_list.append(fkgl)
    NE_retain_mean_list.append(NE_retain_mean)
    NE_retain_std_list.append(NE_retain_std)
  
  bleu_list_list += [bleu_list]
  sari_list_list += [sari_list]
  fkgl_list_list += [fkgl_list]
  NE_retain_mean_list_list += [NE_retain_mean_list]
  NE_retain_std_list_list += [NE_retain_std_list]

with open('/content/drive/MyDrive/muss/qualitative/model39_evaluation_all_complex_NE_retain_rate', 'wb') as fp:
    pickle.dump([bleu_list_list, sari_list_list, fkgl_list_list,NE_retain_mean_list_list,NE_retain_std_list_list], fp)

"""## model 39 complex"""

i = 39
output_id = 'model_'+str(i)+'_'+model_dir_dict[i]['exp_dir'].split('/')[-2]

bleu_list_list, sari_list_list, fkgl_list_list,NE_retain_mean_list_list,NE_retain_std_list_list = [],[],[],[],[]
for generate_id_list in ['05','06','07','08','09']:
    
  dataset = 'asset'
  phase = 'test'

  bleu_list, sari_list, fkgl_list,NE_retain_mean_list,NE_retain_std_list = [],[],[],[],[]
  for output_file_name in test_data_list:
    bleu, sari, fkgl,NE_retain_mean,NE_retain_std = evaluate_my_model(output_id,generate_id_list,dataset,phase,output_file_name)
    bleu_list.append(bleu)
    sari_list.append(sari)
    fkgl_list.append(fkgl)
    NE_retain_mean_list.append(NE_retain_mean)
    NE_retain_std_list.append(NE_retain_std)
  
  bleu_list_list += [bleu_list]
  sari_list_list += [sari_list]
  fkgl_list_list += [fkgl_list]
  NE_retain_mean_list_list += [NE_retain_mean_list]
  NE_retain_std_list_list += [NE_retain_std_list]

model_39_result = [bleu_list_list, sari_list_list, fkgl_list_list,NE_retain_mean_list_list,NE_retain_std_list_list]

with open('/content/drive/MyDrive/muss/qualitative/model39_evaluation_complex', 'wb') as fp:
    pickle.dump(model_39_result, fp)







i = 39
output_id = 'model_'+str(i)+'_'+model_dir_dict[i]['exp_dir'].split('/')[-2]

bleu_list_list, sari_list_list, fkgl_list_list,NE_retain_mean_list_list,NE_retain_std_list_list = [],[],[],[],[]
for generate_id_list in ['05','06','07','08','09']:
    
  dataset = 'asset'
  phase = 'test'

  bleu_list, sari_list, fkgl_list,NE_retain_mean_list,NE_retain_std_list = [],[],[],[],[]
  for output_file_name in test_data_list:
    bleu, sari, fkgl,NE_retain_mean,NE_retain_std = evaluate_my_model(output_id,generate_id_list,dataset,phase,output_file_name,simple_or_complex='complex')
    bleu_list.append(bleu)
    sari_list.append(sari)
    fkgl_list.append(fkgl)
    NE_retain_mean_list.append(NE_retain_mean)
    NE_retain_std_list.append(NE_retain_std)
  
  bleu_list_list += [bleu_list]
  sari_list_list += [sari_list]
  fkgl_list_list += [fkgl_list]
  NE_retain_mean_list_list += [NE_retain_mean_list]
  NE_retain_std_list_list += [NE_retain_std_list]

model_39_result = [bleu_list_list, sari_list_list, fkgl_list_list,NE_retain_mean_list_list,NE_retain_std_list_list]

with open('/content/drive/MyDrive/muss/qualitative/model39_evaluation_complex_NE', 'wb') as fp:
    pickle.dump(model_39_result, fp)

"""## all combine

"""

id_list = [x for x in range(18,24)]
id_list.append(39)
id_list

for i in id_list:
  

  with open('/content/drive/MyDrive/muss/qualitative/model'+str(i)+'_evaluation', 'rb') as fp:
    var_name = 'model_'+str(i)+'_result'
    globals()[var_name] = pickle.load(fp)


  with open('/content/drive/MyDrive/muss/qualitative/model'+str(i)+'_evaluation_complex_NE', 'rb') as fp:
    var_name_complex = 'model_'+str(i)+'_result_complex'
    globals()[var_name_complex] = pickle.load(fp)

  # with open('/content/drive/MyDrive/muss/qualitative/model'+str(i)+'_evaluation_complex_NE', 'rb') as fp:
  #   var_name_complex_NE = 'model_'+str(i)+'_complex_NE'
  #   globals()[var_name_complex_NE] = pickle.load(fp)

  with open('/content/drive/MyDrive/muss/qualitative/model'+str(i)+'_evaluation_all_complex_NE_retain_rate', 'rb') as fp:
    var_name_complex_NE_single = 'model_'+str(i)+'_complex_NE_single'
    globals()[var_name_complex_NE_single] = pickle.load(fp)
  

  var_name_combined = 'model_'+str(i)+'_result_combine'
  globals()[var_name_combined] = np.array(globals()[var_name_complex])[:,:,:7]
  # globals()[var_name_combined] = np.concatenate((np.array(globals()[var_name_combined]), np.array(globals()[var_name])[:,:,8:9]), axis=2)
  globals()[var_name_combined] = np.concatenate((np.array(globals()[var_name_combined]), np.array(globals()[var_name_complex_NE_single])), axis=2)
  globals()[var_name_combined] = np.concatenate((np.array(globals()[var_name_combined]), np.array(globals()[var_name])[:,:,0:8]), axis=2)
  globals()[var_name_combined] = np.concatenate((np.array(globals()[var_name_combined]), np.array(globals()[var_name_complex])[:,:,7:9]), axis=2)
  globals()[var_name_combined] = np.concatenate((np.array(globals()[var_name_combined]), np.array(globals()[var_name])[:,:,9:]), axis=2)
  
  # break

final_clrs = []
clrs = sns.color_palette('BrBG')#BrBG,,RdBu,RdGy,RdYlBu,Spectral,YlGnBu,,tab20c
final_clrs = clrs[:2]+clrs[-2:]
final_clrs

clrs = sns.color_palette('tab20c')#BrBG,,RdBu,RdGy,RdYlBu,Spectral,YlGnBu,,tab20c
final_clrs.extend(clrs)

sns.reset_orig()  # get default matplotlib styles back

label_list=[
            'complex/complex',
            'complex/simple',
'simple/complex',
'simple/simple',
'both/complex',
'both/simple',
'both/both'

]

color_list=[
            final_clrs[0],
            final_clrs[1],
            final_clrs[2],
            final_clrs[3],
            final_clrs[-4],
            final_clrs[-3],
            final_clrs[-2],

]

marker_list=[
             '8',
             'o',
             's',
             's',
             'P',
             '*',
             'd'

]

ms_list=[8,
         None,
         8,
         None,
         10,
         10,
         10]

fillstyle_list=['none',
                None,
                'none',
                None,
                'none',
                None,
                None,
                None
                
                
                
                ]

linestyle_list=['--',
                None,
                '-.',
                None,
                ':',
                '-',
                None
                
                
                ]

alpha_list=[
            1,
            1,
            1,
            1,
            0.8,
            0.6,
            0.8
]

with open('/content/drive/MyDrive/muss/qualitative/asset_evaluation', 'rb') as fp:
    asset_result = pickle.load(fp)


with open('/content/drive/MyDrive/muss/qualitative/muss_evaluation', 'rb') as fp:
    muss_result = pickle.load(fp)

with open('/content/drive/MyDrive/muss/qualitative/asset_evaluation_complex_NE', 'rb') as fp:
    asset_result_complex_NE = pickle.load(fp)


with open('/content/drive/MyDrive/muss/qualitative/muss_evaluation_complex_NE', 'rb') as fp:
    muss_result_complex_NE = pickle.load(fp)

fig, ((ax1, ax2,ax3),(ax4, ax5,ax6),(ax7, ax8,ax9),(ax10, ax11,ax12)) = plt.subplots(ncols=3,nrows=4, sharey='row',figsize=(13,16))

# BLEU
for index_ax,ax in enumerate([ax1,ax2,ax3]):
  ax.axhspan(68.95+1.33, 68.95-1.33, alpha=1, color=sns.color_palette('BrBG')[2],label='Gold Reference')
  ax.axhspan(72.98+4.28, 72.98-4.28, alpha=1, color=sns.color_palette('BrBG')[3],label='MUSS')
  ax.axhspan(68.95+1.33, 72.98-4.28, alpha=1, color='#cddbbf')

  for index,i in enumerate(id_list):

    if index_ax !=2:
      x_list = [str(x) for x in range(7)]
      x_list.append('all')
    else:
      x_list = ['complex','complex\n(shuffled)','simple','simple\n(shuffled)']
    
    if index_ax == 0:
      ax.errorbar(x_list, np.mean(np.array(globals()['model_'+str(i)+'_result_combine'][0,:,0:8]),axis=0),
                  yerr=get_std(globals()['model_'+str(i)+'_result_combine'][0,:,0:8]),#np.std(np.array(sari_list_list),axis=0),
                  # fmt='-',
                #  elinewidth = 0.8,
                  # capsize = 2, 
                  label=label_list[index],
                  color=color_list[index],
                  marker=marker_list[index],
                  fillstyle=fillstyle_list[index],
                  ms=ms_list[index],
                  linestyle=linestyle_list[index],
                  alpha=alpha_list[index],
                  
                  )

      ax.set_ylabel('BLEU ↑',fontsize=14)
      # ax.set_xlabel('# Prepended Named Entities\n (Entities from Complex Sentence)',fontsize=14)
    elif index_ax ==1:
      ax.errorbar(x_list, np.mean(np.array(globals()['model_'+str(i)+'_result_combine'][0,:,8:16]),axis=0),
                yerr=get_std(globals()['model_'+str(i)+'_result_combine'][0,:,8:16]),#np.std(np.array(sari_list_list),axis=0),
                # fmt='-',
              #  elinewidth = 0.8,
                # capsize = 2, 
                label=label_list[index],
                color=color_list[index],
                marker=marker_list[index],
                fillstyle=fillstyle_list[index],
                ms=ms_list[index],
                linestyle=linestyle_list[index],
                alpha=alpha_list[index]
                )


      # ax.set_xlabel('# Prepended Named Entities\n (Entities from Simple Sentence)',fontsize=14)
    elif index_ax ==2:
    
      ax.errorbar(x_list, np.mean(np.array(globals()['model_'+str(i)+'_result_combine'][0,:,16:]),axis=0),
                  yerr=get_std(globals()['model_'+str(i)+'_result_combine'][0,:,16:]),#np.std(np.array(sari_list_list),axis=0),
                  # fmt='-',
                #  elinewidth = 0.8,
                  # capsize = 2, 
                  label=label_list[index],
                  color=color_list[index],
                  marker=marker_list[index],
                  fillstyle=fillstyle_list[index],
                  ms=ms_list[index],
                  linestyle=linestyle_list[index],
                  alpha=alpha_list[index]
                  )
      # ax.set_xlabel('Prepended All Words in _ Sentnece\n(in order or shuffled)',fontsize=14)

    else:
      assert False,'wrong index ax'


# FKGL
for index_ax,ax in enumerate([ax4,ax5,ax6]):
  # ax.axhspan(6.05+0.51, 6.05-0.51, alpha=0.3, color='lightcoral',label='MUSS')
  # ax.axhspan(6.49+0.15, 6.49-0.15, alpha=0.5, color='gold',label='Gold Reference')
  ax.axhspan(6.49+0.15, 6.49-0.15, alpha=1, color=sns.color_palette('BrBG')[2],label='Gold Reference')
  ax.axhspan(6.05+0.51, 6.05-0.51, alpha=1, color=sns.color_palette('BrBG')[3],label='MUSS')
  ax.axhspan(6.05+0.51, 6.49-0.15, alpha=1, color='#cddbbf')

  for index,i in enumerate(id_list):

    if index_ax !=2:
      x_list = [str(x) for x in range(7)]
      x_list.append('all')
    else:
      x_list = ['complex','complex\n(shuffled)','simple','simple\n(shuffled)']
    
    if index_ax == 0:
      ax.errorbar(x_list, np.mean(np.array(globals()['model_'+str(i)+'_result_combine'][2,:,0:8]),axis=0),
                  yerr=get_std(globals()['model_'+str(i)+'_result_combine'][2,:,0:8]),#np.std(np.array(sari_list_list),axis=0),
                  # fmt='-',
                #  elinewidth = 0.8,
                  # capsize = 2, 
                  label=label_list[index],
                  color=color_list[index],
                  marker=marker_list[index],
                  fillstyle=fillstyle_list[index],
                  ms=ms_list[index],
                  linestyle=linestyle_list[index],
                  alpha=alpha_list[index],
                  
                  )

      ax.set_ylabel('FKGL ↓',fontsize=14)
      # ax.set_xlabel('# Prepended Named Entities\n (Entities from Complex Sentence)',fontsize=14)
    elif index_ax ==1:
      ax.errorbar(x_list, np.mean(np.array(globals()['model_'+str(i)+'_result_combine'][2,:,8:16]),axis=0),
                yerr=get_std(globals()['model_'+str(i)+'_result_combine'][2,:,8:16]),#np.std(np.array(sari_list_list),axis=0),
                # fmt='-',
              #  elinewidth = 0.8,
                # capsize = 2, 
                label=label_list[index],
                color=color_list[index],
                marker=marker_list[index],
                fillstyle=fillstyle_list[index],
                ms=ms_list[index],
                linestyle=linestyle_list[index],
                alpha=alpha_list[index]
                )


      # ax.set_xlabel('# Prepended Named Entities\n (Entities from Simple Sentence)',fontsize=14)
    elif index_ax ==2:
    
      ax.errorbar(x_list, np.mean(np.array(globals()['model_'+str(i)+'_result_combine'][2,:,16:]),axis=0),
                  yerr=get_std(globals()['model_'+str(i)+'_result_combine'][2,:,16:]),#np.std(np.array(sari_list_list),axis=0),
                  # fmt='-',
                #  elinewidth = 0.8,
                  # capsize = 2, 
                  label=label_list[index],
                  color=color_list[index],
                  marker=marker_list[index],
                  fillstyle=fillstyle_list[index],
                  ms=ms_list[index],
                  linestyle=linestyle_list[index],
                  alpha=alpha_list[index]
                  )
      # ax.set_xlabel('Prepended All Words in _ Sentnece\n(in order or shuffled)',fontsize=14)

    else:
      assert False,'wrong index ax'



# SARI
for index_ax,ax in enumerate([ax7,ax8,ax9]):
#   ax.axhspan(44.15-0.56, 44.15+0.56, alpha=0.3, color='lightcoral',label='MUSS')

# ax.axhspan(44.87+0.36, 44.87-0.36, alpha=0.5, color='gold',label='Gold Reference')

  ax.axhspan(44.87+0.36, 44.87-0.36, alpha=1, color=sns.color_palette('BrBG')[2],label='Gold Reference')
  ax.axhspan(44.15-0.56, 44.15+0.56, alpha=1, color=sns.color_palette('BrBG')[3],label='MUSS')
  ax.axhspan(44.15+0.56, 44.87-0.36, alpha=1, color='#cddbbf')

  for index,i in enumerate(id_list):

    if index_ax !=2:
      x_list = [str(x) for x in range(7)]
      x_list.append('all')
    else:
      x_list = ['complex','complex\n(shuffled)','simple','simple\n(shuffled)']
    
    if index_ax == 0:
      ax.errorbar(x_list, np.mean(np.array(globals()['model_'+str(i)+'_result_combine'][1,:,0:8]),axis=0),
                  yerr=get_std(globals()['model_'+str(i)+'_result_combine'][1,:,0:8]),#np.std(np.array(sari_list_list),axis=0),
                  # fmt='-',
                #  elinewidth = 0.8,
                  # capsize = 2, 
                  label=label_list[index],
                  color=color_list[index],
                  marker=marker_list[index],
                  fillstyle=fillstyle_list[index],
                  ms=ms_list[index],
                  linestyle=linestyle_list[index],
                  alpha=alpha_list[index],
                  
                  )

      ax.set_ylabel('SARI ↑',fontsize=14)
      # ax.set_xlabel('# Prepended Named Entities\n (Entities from Complex Sentence)',fontsize=14)
    elif index_ax ==1:
      ax.errorbar(x_list, np.mean(np.array(globals()['model_'+str(i)+'_result_combine'][1,:,8:16]),axis=0),
                yerr=get_std(globals()['model_'+str(i)+'_result_combine'][1,:,8:16]),#np.std(np.array(sari_list_list),axis=0),
                # fmt='-',
              #  elinewidth = 0.8,
                # capsize = 2, 
                label=label_list[index],
                color=color_list[index],
                marker=marker_list[index],
                fillstyle=fillstyle_list[index],
                ms=ms_list[index],
                linestyle=linestyle_list[index],
                alpha=alpha_list[index]
                )


      # ax.set_xlabel('# Prepended Named Entities\n (Entities from Simple Sentence)',fontsize=14)
    elif index_ax ==2:
    
      ax.errorbar(x_list, np.mean(np.array(globals()['model_'+str(i)+'_result_combine'][1,:,16:]),axis=0),
                  yerr=get_std(globals()['model_'+str(i)+'_result_combine'][1,:,16:]),#np.std(np.array(sari_list_list),axis=0),
                  # fmt='-',
                #  elinewidth = 0.8,
                  # capsize = 2, 
                  label=label_list[index],
                  color=color_list[index],
                  marker=marker_list[index],
                  fillstyle=fillstyle_list[index],
                  ms=ms_list[index],
                  linestyle=linestyle_list[index],
                  alpha=alpha_list[index]
                  )
      # ax.set_xlabel('Prepended All Words in _ Sentnece\n(in order or shuffled)',fontsize=14)

    else:
      assert False,'wrong index ax'



# NE retention rate
for index_ax,ax in enumerate([ax10,ax11,ax12]):
  
  if index_ax == 0:
    ax.axhspan(np.mean(asset_result_complex_NE[3])+get_mean_confidence_interval(asset_result_complex_NE[3]), np.mean(asset_result_complex_NE[3])-get_mean_confidence_interval(asset_result_complex_NE[3]), alpha=1, color=sns.color_palette('BrBG')[2],label='Gold Reference')
    ax.axhspan(np.mean(muss_result_complex_NE[3])+get_mean_confidence_interval(muss_result_complex_NE[3]), np.mean(muss_result_complex_NE[3])-get_mean_confidence_interval(muss_result_complex_NE[3]), alpha=1, color=sns.color_palette('BrBG')[3],label='MUSS')
    ax.axhspan(np.mean(muss_result_complex_NE[3])-get_mean_confidence_interval(muss_result_complex_NE[3]), np.mean(asset_result_complex_NE[3])+get_mean_confidence_interval(asset_result_complex_NE[3]), alpha=1, color='#cddbbf')

  if index_ax == 1:
    ax.axhspan(np.mean(asset_result[3])+get_mean_confidence_interval(asset_result[3]), np.mean(asset_result[3])-get_mean_confidence_interval(asset_result[3]), alpha=1, color=sns.color_palette('BrBG')[2],label='Gold Reference')
    ax.axhspan(np.mean(muss_result[3])+get_mean_confidence_interval(muss_result[3]), np.mean(muss_result[3])-get_mean_confidence_interval(muss_result[3]), alpha=1, color=sns.color_palette('BrBG')[3],label='MUSS')
    ax.axhspan(np.mean(muss_result[3])-get_mean_confidence_interval(muss_result[3]), np.mean(asset_result[3])+get_mean_confidence_interval(asset_result[3]), alpha=1, color='#cddbbf')

  for index,i in enumerate(id_list):

    if index_ax !=2:
      x_list = [str(x) for x in range(7)]
      x_list.append('all')
    else:
      x_list = ['complex','complex\n(shuffled)','simple','simple\n(shuffled)']
  

    if index_ax == 0:
      
      ax.errorbar(x_list, np.mean(np.array(globals()['model_'+str(i)+'_result_combine'][3,:,0:8]),axis=0),
                  yerr=get_std(globals()['model_'+str(i)+'_result_combine'][3,:,0:8]),#np.std(np.array(sari_list_list),axis=0),
                  # fmt='-',
                #  elinewidth = 0.8,
                  # capsize = 2, 
                  label=label_list[index],
                  color=color_list[index],
                  marker=marker_list[index],
                  fillstyle=fillstyle_list[index],
                  ms=ms_list[index],
                  linestyle=linestyle_list[index],
                  alpha=alpha_list[index],
                  
                  )
     
      ax.set_ylabel('Named Entities Retention Rate ↑',fontsize=14)
      ax.set_xlabel('# Prefixed Named Entities\n (Entities from Complex Sentence)',fontsize=14)



    elif index_ax ==1:
      ax.errorbar(x_list, np.mean(np.array(globals()['model_'+str(i)+'_result_combine'][3,:,8:16]),axis=0),
                yerr=get_std(globals()['model_'+str(i)+'_result_combine'][3,:,8:16]),#np.std(np.array(sari_list_list),axis=0),
                # fmt='-',
              #  elinewidth = 0.8,
                # capsize = 2, 
                label=label_list[index],
                color=color_list[index],
                marker=marker_list[index],
                fillstyle=fillstyle_list[index],
                ms=ms_list[index],
                linestyle=linestyle_list[index],
                alpha=alpha_list[index]
                )


      ax.set_xlabel('# Prefixed Named Entities\n (Entities from Simple Sentence)',fontsize=14)
    elif index_ax ==2:
    
      ax.errorbar(x_list, np.mean(np.array(globals()['model_'+str(i)+'_result_combine'][3,:,16:]),axis=0),
                  yerr=get_std(globals()['model_'+str(i)+'_result_combine'][3,:,16:]),#np.std(np.array(sari_list_list),axis=0),
                  # fmt='-',
                #  elinewidth = 0.8,
                  # capsize = 2, 
                  label=label_list[index],
                  color=color_list[index],
                  marker=marker_list[index],
                  fillstyle=fillstyle_list[index],
                  ms=ms_list[index],
                  linestyle=linestyle_list[index],
                  alpha=alpha_list[index]
                  )
      ax.set_xlabel('Prefixed All Words in _ Sentnece\n(in Original Order or Shuffled)',fontsize=14)

    else:
      assert False,'wrong index ax'

# ax.legend(loc='lower right',bbox_to_anchor=(1.5,-0.02,0,0),fontsize='large')
ax3.legend(loc='upper right',bbox_to_anchor=(1.01,2,0,0),fontsize='large')
# for label in ax.get_xticklabels():
#     label.set_ha("right")
#     label.set_rotation(45)


plt.tight_layout()

# plt.savefig('all_result.pdf',bbox_inches='tight')
plt.show()

cd /content

"""# operation lexical simplification

## model 37 ABCD
"""

test_data_dir_list,test_data_list

i = 37
output_id = 'model_'+str(i)+'_'+model_dir_dict[i]['exp_dir'].split('/')[-2]

bleu_list_list, sari_list_list, fkgl_list_list,NE_retain_mean_list_list,NE_retain_std_list_list = [],[],[],[],[]
for generate_id_list in ['00','01','02','03','04']:
    
  dataset = 'asset'
  phase = 'test'

  bleu_list, sari_list, fkgl_list,NE_retain_mean_list,NE_retain_std_list = [],[],[],[],[]
  for output_file_name in test_data_list:
    bleu, sari, fkgl,NE_retain_mean,NE_retain_std = evaluate_my_model(output_id,generate_id_list,dataset,phase,output_file_name,ABCD=True)
    bleu_list.append(bleu)
    sari_list.append(sari)
    fkgl_list.append(fkgl)
    NE_retain_mean_list.append(NE_retain_mean)
    NE_retain_std_list.append(NE_retain_std)
  
  bleu_list_list += [bleu_list]
  sari_list_list += [sari_list]
  fkgl_list_list += [fkgl_list]
  NE_retain_mean_list_list += [NE_retain_mean_list]
  NE_retain_std_list_list += [NE_retain_std_list]

model_37_result = [bleu_list_list, sari_list_list, fkgl_list_list,NE_retain_mean_list_list,NE_retain_std_list_list]

with open('/content/drive/MyDrive/muss/qualitative/model37_evaluation_ABCD', 'wb') as fp:
    pickle.dump(model_37_result, fp)

"""## model 38 ABCD"""

test_data_dir_list,test_data_list

i = 38
output_id = 'model_'+str(i)+'_'+model_dir_dict[i]['exp_dir'].split('/')[-2]

bleu_list_list, sari_list_list, fkgl_list_list,NE_retain_mean_list_list,NE_retain_std_list_list = [],[],[],[],[]
for generate_id_list in ['00','01','02','03','04']:
    
  dataset = 'asset'
  phase = 'test'

  bleu_list, sari_list, fkgl_list,NE_retain_mean_list,NE_retain_std_list = [],[],[],[],[]
  for output_file_name in test_data_list:
    bleu, sari, fkgl,NE_retain_mean,NE_retain_std = evaluate_my_model(output_id,generate_id_list,dataset,phase,output_file_name,ABCD=True)
    bleu_list.append(bleu)
    sari_list.append(sari)
    fkgl_list.append(fkgl)
    NE_retain_mean_list.append(NE_retain_mean)
    NE_retain_std_list.append(NE_retain_std)
  
  bleu_list_list += [bleu_list]
  sari_list_list += [sari_list]
  fkgl_list_list += [fkgl_list]
  NE_retain_mean_list_list += [NE_retain_mean_list]
  NE_retain_std_list_list += [NE_retain_std_list]

model_38_result = [bleu_list_list, sari_list_list, fkgl_list_list,NE_retain_mean_list_list,NE_retain_std_list_list]

with open('/content/drive/MyDrive/muss/qualitative/model38_evaluation_ABCD', 'wb') as fp:
    pickle.dump(model_38_result, fp)

"""## model 37 38 combine

"""

id_list=[37,38]

for i in id_list:
  with open('/content/drive/MyDrive/muss/qualitative/model'+str(i)+'_evaluation_ABCD', 'rb') as fp:
    var_name = 'model_'+str(i)+'_result_combine'
    globals()[var_name] = pickle.load(fp)

  # with open('/content/drive/MyDrive/muss/qualitative/model'+str(i)+'_evaluation_complex', 'rb') as fp:
  #   var_name_complex = 'model_'+str(i)+'_result_complex'
  #   globals()[var_name_complex] = pickle.load(fp)

  # var_name_combined = 'model_'+str(i)+'_result_combine'
  # globals()[var_name_combined] = np.array(globals()[var_name_complex])[:,:,:7]
  # globals()[var_name_combined] = np.concatenate((np.array(globals()[var_name_combined]), np.array(globals()[var_name])[:,:,8:9]), axis=2)
  # globals()[var_name_combined] = np.concatenate((np.array(globals()[var_name_combined]), np.array(globals()[var_name])[:,:,0:8]), axis=2)
  # globals()[var_name_combined] = np.concatenate((np.array(globals()[var_name_combined]), np.array(globals()[var_name_complex])[:,:,7:9]), axis=2)
  # globals()[var_name_combined] = np.concatenate((np.array(globals()[var_name_combined]), np.array(globals()[var_name])[:,:,9:]), axis=2)
  
  # break

final_clrs = []
clrs = sns.color_palette('BrBG')#BrBG,,RdBu,RdGy,RdYlBu,Spectral,YlGnBu,,tab20c
final_clrs.extend(clrs)

clrs = sns.color_palette('tab20c')#BrBG,,RdBu,RdGy,RdYlBu,Spectral,YlGnBu,,tab20c
clrs
final_clrs.extend(clrs)

sns.reset_orig()  # get default matplotlib styles back

label_list=[
            'all pairs',
'filtered pairs',

]

color_list=[
            final_clrs[0],
            final_clrs[5],


]

marker_list=[
             '8',
             's',

]

ms_list=[None,
         None,
]

fillstyle_list=[None,
                None,

                
                
                
                ]

linestyle_list=['--',
                None,
                
                
                ]

alpha_list=[
            1,
            1,

]

with open('/content/drive/MyDrive/muss/qualitative/asset_evaluation_ABCD', 'rb') as fp:
    asset_result = pickle.load(fp)


with open('/content/drive/MyDrive/muss/qualitative/muss_evaluation_ABCD', 'rb') as fp:
    muss_result = pickle.load(fp)

fig, ((ax1, ax2),(ax3, ax4),(ax5, ax6),(ax7, ax8)) = plt.subplots(ncols=2,nrows=4, sharey='row',figsize=(10,14))

# BLEU
for index_ax,ax in enumerate([ax1,ax2]):
  ax.axhspan(68.95+1.33, 68.95-1.33, alpha=1, color=sns.color_palette('BrBG')[2],label='Gold Reference')
  ax.axhspan(72.98+4.28, 72.98-4.28, alpha=1, color=sns.color_palette('BrBG')[3],label='MUSS')
  ax.axhspan(68.95+1.33, 72.98-4.28, alpha=1, color='#cddbbf')

  for index,i in enumerate(id_list):

    if index_ax !=1:
      x_list = [str(x) for x in range(7)]
      x_list.append('all')
    else:
      x_list = ['all words','all words\n(shuffled)']

    if index_ax == 0:
      ax.errorbar(x_list, np.mean(np.array(globals()['model_'+str(i)+'_result_combine'])[0,:,0:8],axis=0),
                  yerr=get_std(np.array(globals()['model_'+str(i)+'_result_combine'])[0,:,0:8]),#np.std(np.array(sari_list_list),axis=0),
                  # fmt='-',
                #  elinewidth = 0.8,
                  # capsize = 2, 
                  label=label_list[index],
                  color=color_list[index],
                  marker=marker_list[index],
                  fillstyle=fillstyle_list[index],
                  ms=ms_list[index],
                  linestyle=linestyle_list[index],
                  alpha=alpha_list[index],
                  
                  )

      ax.set_ylabel('BLEU ↑',fontsize=14)
      # ax.set_xlabel('# Prepended Named Entities\n (Entities from Complex Sentence)',fontsize=14)
    elif index_ax ==1:
      ax.errorbar(x_list, np.mean(np.array(globals()['model_'+str(i)+'_result_combine'])[0,:,8:],axis=0),
                yerr=get_std(np.array(globals()['model_'+str(i)+'_result_combine'])[0,:,8:]),#np.std(np.array(sari_list_list),axis=0),
                # fmt='-',
              #  elinewidth = 0.8,
                # capsize = 2, 
                label=label_list[index],
                color=color_list[index],
                marker=marker_list[index],
                fillstyle=fillstyle_list[index],
                ms=ms_list[index],
                linestyle=linestyle_list[index],
                alpha=alpha_list[index]
                )




# FKGL
for index_ax,ax in enumerate([ax3,ax4]):
  ax.axhspan(6.49+0.15, 6.49-0.15, alpha=1, color=sns.color_palette('BrBG')[2],label='Gold Reference')
  ax.axhspan(6.05+0.51, 6.05-0.51, alpha=1, color=sns.color_palette('BrBG')[3],label='MUSS')
  ax.axhspan(6.05+0.51, 6.49-0.15, alpha=1, color='#cddbbf')


  for index,i in enumerate(id_list):

    if index_ax !=1:
      x_list = [str(x) for x in range(7)]
      x_list.append('all')
    else:
      x_list = ['all words','all words\n(shuffled)']

    if index_ax == 0:
      ax.errorbar(x_list, np.mean(np.array(globals()['model_'+str(i)+'_result_combine'])[2,:,0:8],axis=0),
                  yerr=get_std(np.array(globals()['model_'+str(i)+'_result_combine'])[2,:,0:8]),#np.std(np.array(sari_list_list),axis=0),
                  # fmt='-',
                #  elinewidth = 0.8,
                  # capsize = 2, 
                  label=label_list[index],
                  color=color_list[index],
                  marker=marker_list[index],
                  fillstyle=fillstyle_list[index],
                  ms=ms_list[index],
                  linestyle=linestyle_list[index],
                  alpha=alpha_list[index],
                  
                  )

      ax.set_ylabel('FKGL ↓',fontsize=14)
      # ax.set_xlabel('# Prepended Named Entities\n (Entities from Complex Sentence)',fontsize=14)
    elif index_ax ==1:
      ax.errorbar(x_list, np.mean(np.array(globals()['model_'+str(i)+'_result_combine'])[2,:,8:],axis=0),
                yerr=get_std(np.array(globals()['model_'+str(i)+'_result_combine'])[2,:,8:]),#np.std(np.array(sari_list_list),axis=0),
                # fmt='-',
              #  elinewidth = 0.8,
                # capsize = 2, 
                label=label_list[index],
                color=color_list[index],
                marker=marker_list[index],
                fillstyle=fillstyle_list[index],
                ms=ms_list[index],
                linestyle=linestyle_list[index],
                alpha=alpha_list[index]
                )



# SARI
for index_ax,ax in enumerate([ax5,ax6]):

  ax.axhspan(44.87+0.36, 44.87-0.36, alpha=1, color=sns.color_palette('BrBG')[2],label='Gold Reference')
  ax.axhspan(44.15-0.56, 44.15+0.56, alpha=1, color=sns.color_palette('BrBG')[3],label='MUSS')
  ax.axhspan(44.15+0.56, 44.87-0.36, alpha=1, color='#cddbbf')

  
  for index,i in enumerate(id_list):

    if index_ax !=1:
      x_list = [str(x) for x in range(7)]
      x_list.append('all')
    else:
      x_list = ['all words','all words\n(shuffled)']

    if index_ax == 0:
      ax.errorbar(x_list, np.mean(np.array(globals()['model_'+str(i)+'_result_combine'])[1,:,0:8],axis=0),
                  yerr=get_std(np.array(globals()['model_'+str(i)+'_result_combine'])[1,:,0:8]),#np.std(np.array(sari_list_list),axis=0),
                  # fmt='-',
                #  elinewidth = 0.8,
                  # capsize = 2, 
                  label=label_list[index],
                  color=color_list[index],
                  marker=marker_list[index],
                  fillstyle=fillstyle_list[index],
                  ms=ms_list[index],
                  linestyle=linestyle_list[index],
                  alpha=alpha_list[index],
                  
                  )

      ax.set_ylabel('SARI ↑',fontsize=14)
      # ax.set_xlabel('# Prepended Named Entities\n (Entities from Complex Sentence)',fontsize=14)
    elif index_ax ==1:
      ax.errorbar(x_list, np.mean(np.array(globals()['model_'+str(i)+'_result_combine'])[1,:,8:],axis=0),
                yerr=get_std(np.array(globals()['model_'+str(i)+'_result_combine'])[1,:,8:]),#np.std(np.array(sari_list_list),axis=0),
                # fmt='-',
              #  elinewidth = 0.8,
                # capsize = 2, 
                label=label_list[index],
                color=color_list[index],
                marker=marker_list[index],
                fillstyle=fillstyle_list[index],
                ms=ms_list[index],
                linestyle=linestyle_list[index],
                alpha=alpha_list[index]
                )
    



# hard word retention rate
for index_ax,ax in enumerate([ax7,ax8]):
  
  ax.axhspan(np.mean(asset_result[3])+get_mean_confidence_interval(asset_result[3]), np.mean(asset_result[3])-get_mean_confidence_interval(asset_result[3]), alpha=1, color=sns.color_palette('BrBG')[2],label='Gold Reference')
  ax.axhspan(np.mean(muss_result[3])+get_mean_confidence_interval(muss_result[3]), np.mean(muss_result[3])-get_mean_confidence_interval(muss_result[3]), alpha=1, color=sns.color_palette('BrBG')[3],label='MUSS')
  ax.axhspan(np.mean(asset_result[3])+get_mean_confidence_interval(asset_result[3]), np.mean(asset_result[3])-get_mean_confidence_interval(asset_result[3]), alpha=1, color='#cddbbf')

  for index,i in enumerate(id_list):

    if index_ax !=1:
      x_list = [str(x) for x in range(7)]
      x_list.append('all')
    else:
      x_list = ['all words','all words\n(shuffled)']

    if index_ax == 0:
      ax.errorbar(x_list, np.mean(np.array(globals()['model_'+str(i)+'_result_combine'])[3,:,0:8],axis=0),
                  yerr=get_std(np.array(globals()['model_'+str(i)+'_result_combine'])[3,:,0:8]),#np.std(np.array(sari_list_list),axis=0),
                  # fmt='-',
                #  elinewidth = 0.8,
                  # capsize = 2, 
                  label=label_list[index],
                  color=color_list[index],
                  marker=marker_list[index],
                  fillstyle=fillstyle_list[index],
                  ms=ms_list[index],
                  linestyle=linestyle_list[index],
                  alpha=alpha_list[index],
                  
                  )

      ax.set_ylabel('Hard Words Retention Rate ↓',fontsize=14)
      ax.set_xlabel('\n# Prefixed Hard Word',fontsize=14)

    elif index_ax ==1:
      ax.errorbar(x_list, np.mean(np.array(globals()['model_'+str(i)+'_result_combine'])[3,:,8:],axis=0),
                yerr=get_std(np.array(globals()['model_'+str(i)+'_result_combine'])[3,:,8:]),#np.std(np.array(sari_list_list),axis=0),
                # fmt='-',
              #  elinewidth = 0.8,
                # capsize = 2, 
                label=label_list[index],
                color=color_list[index],
                marker=marker_list[index],
                fillstyle=fillstyle_list[index],
                ms=ms_list[index],
                linestyle=linestyle_list[index],
                alpha=alpha_list[index]
                )
      
      ax.set_xlabel('Prefix _ in Complex Sentnece',fontsize=14)
  

# ax.legend(loc='lower right',bbox_to_anchor=(1.5,-0.02,0,0),fontsize='large')
ax2.legend(loc='upper right',bbox_to_anchor=(1.02,1.5,0,0),fontsize='large')
# for label in ax.get_xticklabels():
#     label.set_ha("right")
#     label.set_rotation(45)


plt.tight_layout()

# plt.savefig('ABCD_result.pdf',bbox_inches='tight')
plt.show()



cd /content

"""# operation perserving and lexical simplification

## model 40 NER
"""

test_data_dir_list,test_data_list

i = 40
output_id = 'model_'+str(i)+'_'+model_dir_dict[i]['exp_dir'].split('/')[-2]

bleu_list_list, sari_list_list, fkgl_list_list,NE_retain_mean_list_list,NE_retain_std_list_list = [],[],[],[],[]
for generate_id_list in ['00','01','02','03','04']:#,'05','06']:
    
  dataset = 'asset'
  phase = 'test'

  bleu_list, sari_list, fkgl_list,NE_retain_mean_list,NE_retain_std_list = [],[],[],[],[]
  for output_file_name in test_data_list:
    bleu, sari, fkgl,NE_retain_mean,NE_retain_std = evaluate_my_model(output_id,generate_id_list,dataset,phase,output_file_name)
    bleu_list.append(bleu)
    sari_list.append(sari)
    fkgl_list.append(fkgl)
    NE_retain_mean_list.append(NE_retain_mean)
    NE_retain_std_list.append(NE_retain_std)
  
  bleu_list_list += [bleu_list]
  sari_list_list += [sari_list]
  fkgl_list_list += [fkgl_list]
  NE_retain_mean_list_list += [NE_retain_mean_list]
  NE_retain_std_list_list += [NE_retain_std_list]



model_40_result = [bleu_list_list, sari_list_list, fkgl_list_list,NE_retain_mean_list_list,NE_retain_std_list_list]

with open('/content/drive/MyDrive/muss/qualitative/model40_evaluation', 'wb') as fp:
    pickle.dump(model_40_result, fp)



"""## model 40 ABCD"""

test_data_dir_list,test_data_list

i = 40
output_id = 'model_'+str(i)+'_'+model_dir_dict[i]['exp_dir'].split('/')[-2]

bleu_list_list, sari_list_list, fkgl_list_list,NE_retain_mean_list_list,NE_retain_std_list_list = [],[],[],[],[]
for generate_id_list in ['00','01','02','03','04']:#,'05','06']:
    
  dataset = 'asset'
  phase = 'test'

  bleu_list, sari_list, fkgl_list,NE_retain_mean_list,NE_retain_std_list = [],[],[],[],[]
  for output_file_name in test_data_list:
    bleu, sari, fkgl,NE_retain_mean,NE_retain_std = evaluate_my_model(output_id,generate_id_list,dataset,phase,output_file_name,ABCD=True)
    bleu_list.append(bleu)
    sari_list.append(sari)
    fkgl_list.append(fkgl)
    NE_retain_mean_list.append(NE_retain_mean)
    NE_retain_std_list.append(NE_retain_std)
  
  bleu_list_list += [bleu_list]
  sari_list_list += [sari_list]
  fkgl_list_list += [fkgl_list]
  NE_retain_mean_list_list += [NE_retain_mean_list]
  NE_retain_std_list_list += [NE_retain_std_list]



model_40_result = [bleu_list_list, sari_list_list, fkgl_list_list,NE_retain_mean_list_list,NE_retain_std_list_list]

with open('/content/drive/MyDrive/muss/qualitative/model40_evaluation_ABCD', 'wb') as fp:
    pickle.dump(model_40_result, fp)



"""## model 40 combine

"""

test_data_dir_list,test_data_list

test_data_list[0:8]

id_list=[40]

for i in id_list:
  with open('/content/drive/MyDrive/muss/qualitative/model'+str(i)+'_evaluation', 'rb') as fp:
    var_name = 'model_'+str(i)+'_result'
    globals()[var_name] = pickle.load(fp)

  with open('/content/drive/MyDrive/muss/qualitative/model'+str(i)+'_evaluation_ABCD', 'rb') as fp:
    var_name_ABCD = 'model_'+str(i)+'_result_ABCD'
    globals()[var_name_ABCD] = pickle.load(fp)

  var_name_combined = 'model_'+str(i)+'_result_combine'
  globals()[var_name_combined] = np.array(globals()[var_name])[:,:,:7]
  globals()[var_name_combined] = np.concatenate((np.array(globals()[var_name_combined]), np.array(globals()[var_name])[:,:,-1:]), axis=2)
  globals()[var_name_combined] = np.concatenate((np.array(globals()[var_name_combined]), np.array(globals()[var_name])[:,:,7:]), axis=2)
  
  var_name_combined = 'model_'+str(i)+'_result_ABCD_combine'
  globals()[var_name_combined] = np.array(globals()[var_name_ABCD])[:,:,:7]
  globals()[var_name_combined] = np.concatenate((np.array(globals()[var_name_combined]), np.array(globals()[var_name_ABCD])[:,:,-1:]), axis=2)
  globals()[var_name_combined] = np.concatenate((np.array(globals()[var_name_combined]), np.array(globals()[var_name_ABCD])[:,:,7:]), axis=2)

  # globals()[var_name_combined] = np.concatenate((np.array(globals()[var_name_combined]), np.array(globals()[var_name_complex])[:,:,7:9]), axis=2)
  # globals()[var_name_combined] = np.concatenate((np.array(globals()[var_name_combined]), np.array(globals()[var_name])[:,:,9:]), axis=2)
  
  # break

for i in [21]:
  with open('/content/drive/MyDrive/muss/qualitative/model'+str(i)+'_evaluation', 'rb') as fp:
    var_name = 'model_'+str(i)+'_result'
    globals()[var_name] = pickle.load(fp)

  # with open('/content/drive/MyDrive/muss/qualitative/model'+str(i)+'_evaluation_ABCD', 'rb') as fp:
  #   var_name_ABCD = 'model_'+str(i)+'_result_ABCD'
  #   globals()[var_name_ABCD] = pickle.load(fp)

  var_name_combined = 'model_'+str(i)+'_result_combine'
  globals()[var_name_combined] = np.array(globals()[var_name])[:,:,0:8]
  # globals()[var_name_combined] = np.concatenate((np.array(globals()[var_name_combined]), np.array(globals()[var_name])[:,:,-1:]), axis=2)
  # globals()[var_name_combined] = np.concatenate((np.array(globals()[var_name_combined]), np.array(globals()[var_name])[:,:,7:]), axis=2)
  
  # var_name_combined = 'model_'+str(i)+'_result_ABCD_combine'
  # globals()[var_name_combined] = np.array(globals()[var_name_ABCD])[:,:,:7]
  # globals()[var_name_combined] = np.concatenate((np.array(globals()[var_name_combined]), np.array(globals()[var_name_ABCD])[:,:,-1:]), axis=2)
  # globals()[var_name_combined] = np.concatenate((np.array(globals()[var_name_combined]), np.array(globals()[var_name_ABCD])[:,:,7:]), axis=2)

  # globals()[var_name_combined] = np.concatenate((np.array(globals()[var_name_combined]), np.array(globals()[var_name_complex])[:,:,7:9]), axis=2)
  # globals()[var_name_combined] = np.concatenate((np.array(globals()[var_name_combined]), np.array(globals()[var_name])[:,:,9:]), axis=2)
  
  # break

with open('/content/drive/MyDrive/muss/qualitative/model38_evaluation_ABCD', 'rb') as fp:
    var_name = 'model_38_result_combine'
    globals()[var_name] = pickle.load(fp)

    globals()[var_name] = np.array(globals()[var_name])[:,:,0:8]

model_41_result_combine = np.concatenate((model_21_result_combine, model_38_result_combine), axis=2)
model_41_result_combine.shape

final_clrs = []
clrs = sns.color_palette('BrBG')#BrBG,,RdBu,RdGy,RdYlBu,Spectral,YlGnBu,,tab20c
final_clrs.extend(clrs)

clrs = sns.color_palette('tab20c')#BrBG,,RdBu,RdGy,RdYlBu,Spectral,YlGnBu,,tab20c
clrs
final_clrs.extend(clrs)

sns.reset_orig()  # get default matplotlib styles back

label_list=[
            'simple/simple/filtered pairs',
            'simple/simple',
            'filtered pairs'

]

color_list=[
            # final_clrs[0],
            final_clrs[5],
sns.color_palette('BrBG')[0],
sns.color_palette('BrBG')[1]

]

marker_list=[
             '8',
             's',
             's',

]

ms_list=[None,
         None,
         None
]

fillstyle_list=[None,
                None,
                None

                
                
                
                ]

linestyle_list=[None,
                'dashed',
                'dotted'
                
                
                ]

alpha_list=[
            1,
            1,
            1,

]

with open('/content/drive/MyDrive/muss/qualitative/asset_evaluation', 'rb') as fp:
    asset_result = pickle.load(fp)


with open('/content/drive/MyDrive/muss/qualitative/muss_evaluation', 'rb') as fp:
    muss_result = pickle.load(fp)

with open('/content/drive/MyDrive/muss/qualitative/asset_evaluation_ABCD', 'rb') as fp:
    asset_result_ABCD = pickle.load(fp)


with open('/content/drive/MyDrive/muss/qualitative/muss_evaluation_ABCD', 'rb') as fp:
    muss_result_ABCD = pickle.load(fp)

id_list=[40,41]

fig, ((ax1, ax2),(ax3, ax4),(ax5, ax6),(ax7, ax8),(ax9, ax10)) = plt.subplots(ncols=2,nrows=5, sharey='row',figsize=(10,14))

# BLEU
for index_ax,ax in enumerate([ax1,ax2]):
  ax.axhspan(68.95+1.33, 68.95-1.33, alpha=1, color=sns.color_palette('BrBG')[2],label='Gold Reference')
  ax.axhspan(72.98+4.28, 72.98-4.28, alpha=1, color=sns.color_palette('BrBG')[3],label='MUSS')
  ax.axhspan(68.95+1.33, 72.98-4.28, alpha=1, color='#cddbbf')

  for index,i in enumerate(id_list):

    if index_ax !=1:
      x_list = [str(x) for x in range(7)]
      x_list.append('all')
    # else:
    #   x_list = ['all words','all words\n(shuffled)']

    if index_ax == 0:
      ax.errorbar(x_list, np.mean(np.array(globals()['model_'+str(i)+'_result_combine'])[0,:,0:8],axis=0),
                  yerr=get_std(np.array(globals()['model_'+str(i)+'_result_combine'])[0,:,0:8]),#np.std(np.array(sari_list_list),axis=0),
                  # fmt='-',
                #  elinewidth = 0.8,
                  # capsize = 2, 
                  label=label_list[index],
                  color=color_list[index],
                  marker=marker_list[index],
                  fillstyle=fillstyle_list[index],
                  ms=ms_list[index],
                  linestyle=linestyle_list[index],
                  alpha=alpha_list[index],
                  
                  )

      ax.set_ylabel('BLEU ↑',fontsize=14)
      # ax.set_xlabel('# Prepended Named Entities\n (Entities from Complex Sentence)',fontsize=14)
    elif index_ax ==1:
      if index ==0:
        ax.errorbar(x_list, np.mean(np.array(globals()['model_'+str(i)+'_result_combine'])[0,:,8:],axis=0),
                  yerr=get_std(np.array(globals()['model_'+str(i)+'_result_combine'])[0,:,8:]),#np.std(np.array(sari_list_list),axis=0),
                  # fmt='-',
                #  elinewidth = 0.8,
                  # capsize = 2, 
                  label=label_list[index],
                  color=color_list[index],
                  marker=marker_list[index],
                  fillstyle=fillstyle_list[index],
                  ms=ms_list[index],
                  linestyle=linestyle_list[index],
                  alpha=alpha_list[index]
                  )
      if index ==1:
        index=2
        ax.errorbar(x_list, np.mean(np.array(globals()['model_'+str(i)+'_result_combine'])[0,:,8:],axis=0),
                  yerr=get_std(np.array(globals()['model_'+str(i)+'_result_combine'])[0,:,8:]),#np.std(np.array(sari_list_list),axis=0),
                  # fmt='-',
                #  elinewidth = 0.8,
                  # capsize = 2, 
                  label=label_list[index],
                  color=color_list[index],
                  marker=marker_list[index],
                  fillstyle=fillstyle_list[index],
                  ms=ms_list[index],
                  linestyle=linestyle_list[index],
                  alpha=alpha_list[index]
                  )




# FKGL
for index_ax,ax in enumerate([ax3,ax4]):
  ax.axhspan(6.49+0.15, 6.49-0.15, alpha=1, color=sns.color_palette('BrBG')[2],label='Gold Reference')
  ax.axhspan(6.05+0.51, 6.05-0.51, alpha=1, color=sns.color_palette('BrBG')[3],label='MUSS')
  ax.axhspan(6.05+0.51, 6.49-0.15, alpha=1, color='#cddbbf')


  # ax.axhspan(68.95+1.33, 68.95-1.33, alpha=1, color=sns.color_palette('BrBG')[2],label='Gold Reference')
  # ax.axhspan(72.98+4.28, 72.98-4.28, alpha=1, color=sns.color_palette('BrBG')[3],label='MUSS')
  # ax.axhspan(68.95+1.33, 72.98-4.28, alpha=1, color='#cddbbf')

  for index,i in enumerate(id_list):

    if index_ax !=1:
      x_list = [str(x) for x in range(7)]
      x_list.append('all')
    # else:
    #   x_list = ['all words','all words\n(shuffled)']

    if index_ax == 0:
      ax.errorbar(x_list, np.mean(np.array(globals()['model_'+str(i)+'_result_combine'])[2,:,0:8],axis=0),
                  yerr=get_std(np.array(globals()['model_'+str(i)+'_result_combine'])[2,:,0:8]),#np.std(np.array(sari_list_list),axis=0),
                  # fmt='-',
                #  elinewidth = 0.8,
                  # capsize = 2, 
                  label=label_list[index],
                  color=color_list[index],
                  marker=marker_list[index],
                  fillstyle=fillstyle_list[index],
                  ms=ms_list[index],
                  linestyle=linestyle_list[index],
                  alpha=alpha_list[index],
                  
                  )

      ax.set_ylabel('FKGL ↓',fontsize=14)
      # ax.set_xlabel('# Prepended Named Entities\n (Entities from Complex Sentence)',fontsize=14)
    elif index_ax ==1:
      if index ==1:
        index=2
      ax.errorbar(x_list, np.mean(np.array(globals()['model_'+str(i)+'_result_combine'])[2,:,8:],axis=0),
                yerr=get_std(np.array(globals()['model_'+str(i)+'_result_combine'])[2,:,8:]),#np.std(np.array(sari_list_list),axis=0),
                # fmt='-',
              #  elinewidth = 0.8,
                # capsize = 2, 
                label=label_list[index],
                color=color_list[index],
                marker=marker_list[index],
                fillstyle=fillstyle_list[index],
                ms=ms_list[index],
                linestyle=linestyle_list[index],
                alpha=alpha_list[index]
                )



# SARI
for index_ax,ax in enumerate([ax5,ax6]):

  ax.axhspan(44.87+0.36, 44.87-0.36, alpha=1, color=sns.color_palette('BrBG')[2],label='Gold Reference')
  ax.axhspan(44.15-0.56, 44.15+0.56, alpha=1, color=sns.color_palette('BrBG')[3],label='MUSS')
  ax.axhspan(44.15+0.56, 44.87-0.36, alpha=1, color='#cddbbf')

  
  for index,i in enumerate(id_list):

    if index_ax !=1:
      x_list = [str(x) for x in range(7)]
      x_list.append('all')
    # else:
    #   x_list = ['all words','all words\n(shuffled)']

    if index_ax == 0:
      ax.errorbar(x_list, np.mean(np.array(globals()['model_'+str(i)+'_result_combine'])[1,:,0:8],axis=0),
                  yerr=get_std(np.array(globals()['model_'+str(i)+'_result_combine'])[1,:,0:8]),#np.std(np.array(sari_list_list),axis=0),
                  # fmt='-',
                #  elinewidth = 0.8,
                  # capsize = 2, 
                  label=label_list[index],
                  color=color_list[index],
                  marker=marker_list[index],
                  fillstyle=fillstyle_list[index],
                  ms=ms_list[index],
                  linestyle=linestyle_list[index],
                  alpha=alpha_list[index],
                  
                  )

      ax.set_ylabel('SARI ↑',fontsize=14)
      # ax.set_xlabel('# Prepended Named Entities\n (Entities from Complex Sentence)',fontsize=14)
    elif index_ax ==1:
      if index ==1:
        index=2
      ax.errorbar(x_list, np.mean(np.array(globals()['model_'+str(i)+'_result_combine'])[1,:,8:],axis=0),
                yerr=get_std(np.array(globals()['model_'+str(i)+'_result_combine'])[1,:,8:]),#np.std(np.array(sari_list_list),axis=0),
                # fmt='-',
              #  elinewidth = 0.8,
                # capsize = 2, 
                label=label_list[index],
                color=color_list[index],
                marker=marker_list[index],
                fillstyle=fillstyle_list[index],
                ms=ms_list[index],
                linestyle=linestyle_list[index],
                alpha=alpha_list[index]
                )
    




# NE retention rate
for index_ax,ax in enumerate([ax7,ax8]):
  
  # ax.axhspan(np.mean(asset_result[3])+get_mean_confidence_interval(asset_result[3]), np.mean(asset_result[3])-get_mean_confidence_interval(asset_result[3]), alpha=1, color=sns.color_palette('BrBG')[2],label='Gold Reference')
  # ax.axhspan(np.mean(muss_result[3])+get_mean_confidence_interval(muss_result[3]), np.mean(muss_result[3])-get_mean_confidence_interval(muss_result[3]), alpha=1, color=sns.color_palette('BrBG')[3],label='MUSS')
  # ax.axhspan(np.mean(asset_result[3])+get_mean_confidence_interval(asset_result[3]), np.mean(asset_result[3])-get_mean_confidence_interval(asset_result[3]), alpha=1, color='#cddbbf')

  ax.axhspan(np.mean(asset_result[3])+get_mean_confidence_interval(asset_result[3]), np.mean(asset_result[3])-get_mean_confidence_interval(asset_result[3]), alpha=1, color=sns.color_palette('BrBG')[2],label='Gold Reference')
  ax.axhspan(np.mean(muss_result[3])+get_mean_confidence_interval(muss_result[3]), np.mean(muss_result[3])-get_mean_confidence_interval(muss_result[3]), alpha=1, color=sns.color_palette('BrBG')[3],label='MUSS')
  ax.axhspan(np.mean(muss_result[3])-get_mean_confidence_interval(muss_result[3]), np.mean(asset_result[3])+get_mean_confidence_interval(asset_result[3]), alpha=1, color='#cddbbf')


  for index,i in enumerate([40]):

    if index_ax !=1:
      x_list = [str(x) for x in range(7)]
      x_list.append('all')
    # else:
    #   x_list = ['all words','all words\n(shuffled)']

    if index_ax == 0:
      ax.errorbar(x_list, np.mean(np.array(globals()['model_'+str(i)+'_result_combine'])[3,:,0:8],axis=0),
                  yerr=get_std(np.array(globals()['model_'+str(i)+'_result_combine'])[3,:,0:8]),#np.std(np.array(sari_list_list),axis=0),
                  # fmt='-',
                #  elinewidth = 0.8,
                  # capsize = 2, 
                  label=label_list[index],
                  color=color_list[index],
                  marker=marker_list[index],
                  fillstyle=fillstyle_list[index],
                  ms=ms_list[index],
                  linestyle=linestyle_list[index],
                  alpha=alpha_list[index],
                  
                  )

      ax.set_ylabel('Named Entities\nRetention Rate ↑',fontsize=14)
      # ax.set_xlabel('\n# Prepended Hard Word',fontsize=14)

    elif index_ax ==1:
      if index ==1:
        index=2
      ax.errorbar(x_list, np.mean(np.array(globals()['model_'+str(i)+'_result_combine'])[3,:,8:],axis=0),
                yerr=get_std(np.array(globals()['model_'+str(i)+'_result_combine'])[3,:,8:]),#np.std(np.array(sari_list_list),axis=0),
                # fmt='-',
              #  elinewidth = 0.8,
                # capsize = 2, 
                label=label_list[index],
                color=color_list[index],
                marker=marker_list[index],
                fillstyle=fillstyle_list[index],
                ms=ms_list[index],
                linestyle=linestyle_list[index],
                alpha=alpha_list[index]
                )
  
  for index,i in enumerate([41]):

    # if index_ax !=1:
    #   x_list = [str(x) for x in range(7)]
    #   x_list.append('all')
    # else:
    #   x_list = ['all words','all words\n(shuffled)']

    if index_ax == 0:
      index=1
      ax.errorbar(x_list, np.mean(np.array(globals()['model_'+str(i)+'_result_combine'])[3,:,0:8],axis=0),
                  yerr=get_std(np.array(globals()['model_'+str(i)+'_result_combine'])[3,:,0:8]),#np.std(np.array(sari_list_list),axis=0),
                  # fmt='-',
                #  elinewidth = 0.8,
                  # capsize = 2, 
                  label=label_list[index],
                  color=color_list[index],
                  marker=marker_list[index],
                  fillstyle=fillstyle_list[index],
                  ms=ms_list[index],
                  linestyle=linestyle_list[index],
                  alpha=alpha_list[index],
                  
                  )

    #   ax.set_ylabel('Named Entities\nRetention Rate ↑',fontsize=14)
    #   # ax.set_xlabel('\n# Prepended Hard Word',fontsize=14)

    # elif index_ax ==1:
    #   ax.errorbar(x_list, np.mean(np.array(globals()['model_'+str(i)+'_result_combine'])[3,:,8:],axis=0),
    #             yerr=get_std(np.array(globals()['model_'+str(i)+'_result_combine'])[3,:,8:]),#np.std(np.array(sari_list_list),axis=0),
    #             # fmt='-',
    #           #  elinewidth = 0.8,
    #             # capsize = 2, 
    #             label=label_list[index],
    #             color=color_list[index],
    #             marker=marker_list[index],
    #             fillstyle=fillstyle_list[index],
    #             ms=ms_list[index],
    #             linestyle=linestyle_list[index],
    #             alpha=alpha_list[index]
    #             )
    #   # ax.set_xlabel('Prepend _ in Complex Sentnece',fontsize=14)
  


# ABCD retention rate
for index_ax,ax in enumerate([ax9,ax10]):
  
  ax.axhspan(np.mean(asset_result_ABCD[3])+get_mean_confidence_interval(asset_result_ABCD[3]), np.mean(asset_result_ABCD[3])-get_mean_confidence_interval(asset_result_ABCD[3]), alpha=1, color=sns.color_palette('BrBG')[2],label='Gold Reference')
  ax.axhspan(np.mean(muss_result_ABCD[3])+get_mean_confidence_interval(muss_result_ABCD[3]), np.mean(muss_result_ABCD[3])-get_mean_confidence_interval(muss_result_ABCD[3]), alpha=1, color=sns.color_palette('BrBG')[3],label='MUSS')
  ax.axhspan(np.mean(asset_result_ABCD[3])+get_mean_confidence_interval(asset_result_ABCD[3]), np.mean(asset_result_ABCD[3])-get_mean_confidence_interval(asset_result_ABCD[3]), alpha=1, color='#cddbbf')

  for index,i in enumerate([40]):

    if index_ax !=1:
      x_list = [str(x) for x in range(7)]
      x_list.append('all')
    # else:
    #   x_list = ['all words','all words\n(shuffled)']

    if index_ax == 0:
      ax.errorbar(x_list, np.mean(np.array(globals()['model_'+str(i)+'_result_ABCD_combine'])[3,:,0:8],axis=0),
                  yerr=get_std(np.array(globals()['model_'+str(i)+'_result_ABCD_combine'])[3,:,0:8]),#np.std(np.array(sari_list_list),axis=0),
                  # fmt='-',
                #  elinewidth = 0.8,
                  # capsize = 2, 
                  label=label_list[index],
                  color=color_list[index],
                  marker=marker_list[index],
                  fillstyle=fillstyle_list[index],
                  ms=ms_list[index],
                  linestyle=linestyle_list[index],
                  alpha=alpha_list[index],
                  
                  )

      ax.set_ylabel('Hard Word\nRetention Rate ↓',fontsize=14)
      ax.set_xlabel('# Prepended Named Entities',fontsize=14)

    elif index_ax ==1:
      if index ==1:
        index=2
      ax.errorbar(x_list, np.mean(np.array(globals()['model_'+str(i)+'_result_ABCD_combine'])[3,:,8:],axis=0),
                yerr=get_std(np.array(globals()['model_'+str(i)+'_result_ABCD_combine'])[3,:,8:]),#np.std(np.array(sari_list_list),axis=0),
                # fmt='-',
              #  elinewidth = 0.8,
                # capsize = 2, 
                label=label_list[index],
                color=color_list[index],
                marker=marker_list[index],
                fillstyle=fillstyle_list[index],
                ms=ms_list[index],
                linestyle=linestyle_list[index],
                alpha=alpha_list[index]
                )
      
      ax.set_xlabel('# Prepended Hard Word',fontsize=14)

  for index,i in enumerate([41]):

    # if index_ax !=1:
    #   x_list = [str(x) for x in range(7)]
    #   x_list.append('all')
    # else:
    #   x_list = ['all words','all words\n(shuffled)']

    if index_ax == 1:
      index = 2
      ax.errorbar(x_list, np.mean(np.array(globals()['model_'+str(i)+'_result_combine'])[3,:,8:],axis=0),
                  yerr=get_std(np.array(globals()['model_'+str(i)+'_result_combine'])[3,:,8:]),#np.std(np.array(sari_list_list),axis=0),
                  # fmt='-',
                #  elinewidth = 0.8,
                  # capsize = 2, 
                  label=label_list[index],
                  color=color_list[index],
                  marker=marker_list[index],
                  fillstyle=fillstyle_list[index],
                  ms=ms_list[index],
                  linestyle=linestyle_list[index],
                  alpha=alpha_list[index],
                  
                  )

      # ax.set_ylabel('Hard Word\nRetention Rate ↓',fontsize=14)
      ax.set_xlabel('# Prepended Hard Words',fontsize=14)

    # elif index_ax ==1:
    #   ax.errorbar(x_list, np.mean(np.array(globals()['model_'+str(i)+'_result_ABCD_combine'])[3,:,8:],axis=0),
    #             yerr=get_std(np.array(globals()['model_'+str(i)+'_result_ABCD_combine'])[3,:,8:]),#np.std(np.array(sari_list_list),axis=0),
    #             # fmt='-',
    #           #  elinewidth = 0.8,
    #             # capsize = 2, 
    #             label=label_list[index],
    #             color=color_list[index],
    #             marker=marker_list[index],
    #             fillstyle=fillstyle_list[index],
    #             ms=ms_list[index],
    #             linestyle=linestyle_list[index],
    #             alpha=alpha_list[index]
    #             )
      
    #   ax.set_xlabel('# Prepended Hard Word\n(each with All Named Entities)',fontsize=14)

from matplotlib.patches import Patch

handles, labels = ax2.get_legend_handles_labels()

handles, labels

handles_new, labels_new = ax3.get_legend_handles_labels()

handles_new, labels_new

handles.append(handles_new[-1])
labels.append(labels_new[-1])

fig, ((ax1, ax2),(ax3, ax4),(ax5, ax6),(ax7, ax8),(ax9, ax10)) = plt.subplots(ncols=2,nrows=5, sharey='row',figsize=(10,14))

# BLEU
for index_ax,ax in enumerate([ax1,ax2]):
  ax.axhspan(68.95+1.33, 68.95-1.33, alpha=1, color=sns.color_palette('BrBG')[2],label='Gold Reference')
  ax.axhspan(72.98+4.28, 72.98-4.28, alpha=1, color=sns.color_palette('BrBG')[3],label='MUSS')
  ax.axhspan(68.95+1.33, 72.98-4.28, alpha=1, color='#cddbbf')

  for index,i in enumerate(id_list):

    if index_ax !=1:
      x_list = [str(x) for x in range(7)]
      x_list.append('all')
    # else:
    #   x_list = ['all words','all words\n(shuffled)']

    if index_ax == 0:
      ax.errorbar(x_list, np.mean(np.array(globals()['model_'+str(i)+'_result_combine'])[0,:,0:8],axis=0),
                  yerr=get_std(np.array(globals()['model_'+str(i)+'_result_combine'])[0,:,0:8]),#np.std(np.array(sari_list_list),axis=0),
                  # fmt='-',
                #  elinewidth = 0.8,
                  # capsize = 2, 
                  label=label_list[index],
                  color=color_list[index],
                  marker=marker_list[index],
                  fillstyle=fillstyle_list[index],
                  ms=ms_list[index],
                  linestyle=linestyle_list[index],
                  alpha=alpha_list[index],
                  
                  )

      ax.set_ylabel('BLEU ↑',fontsize=14)
      # ax.set_xlabel('# Prepended Named Entities\n (Entities from Complex Sentence)',fontsize=14)
    elif index_ax ==1:
      if index ==0:
        ax.errorbar(x_list, np.mean(np.array(globals()['model_'+str(i)+'_result_combine'])[0,:,8:],axis=0),
                  yerr=get_std(np.array(globals()['model_'+str(i)+'_result_combine'])[0,:,8:]),#np.std(np.array(sari_list_list),axis=0),
                  # fmt='-',
                #  elinewidth = 0.8,
                  # capsize = 2, 
                  label=label_list[index],
                  color=color_list[index],
                  marker=marker_list[index],
                  fillstyle=fillstyle_list[index],
                  ms=ms_list[index],
                  linestyle=linestyle_list[index],
                  alpha=alpha_list[index]
                  )
      if index ==1:
        index=2
        ax.errorbar(x_list, np.mean(np.array(globals()['model_'+str(i)+'_result_combine'])[0,:,8:],axis=0),
                  yerr=get_std(np.array(globals()['model_'+str(i)+'_result_combine'])[0,:,8:]),#np.std(np.array(sari_list_list),axis=0),
                  # fmt='-',
                #  elinewidth = 0.8,
                  # capsize = 2, 
                  label=label_list[index],
                  color=color_list[index],
                  marker=marker_list[index],
                  fillstyle=fillstyle_list[index],
                  ms=ms_list[index],
                  linestyle=linestyle_list[index],
                  alpha=alpha_list[index]
                  )




# FKGL
for index_ax,ax in enumerate([ax3,ax4]):
  ax.axhspan(6.49+0.15, 6.49-0.15, alpha=1, color=sns.color_palette('BrBG')[2],label='Gold Reference')
  ax.axhspan(6.05+0.51, 6.05-0.51, alpha=1, color=sns.color_palette('BrBG')[3],label='MUSS')
  ax.axhspan(6.05+0.51, 6.49-0.15, alpha=1, color='#cddbbf')


  # ax.axhspan(68.95+1.33, 68.95-1.33, alpha=1, color=sns.color_palette('BrBG')[2],label='Gold Reference')
  # ax.axhspan(72.98+4.28, 72.98-4.28, alpha=1, color=sns.color_palette('BrBG')[3],label='MUSS')
  # ax.axhspan(68.95+1.33, 72.98-4.28, alpha=1, color='#cddbbf')

  for index,i in enumerate(id_list):

    if index_ax !=1:
      x_list = [str(x) for x in range(7)]
      x_list.append('all')
    # else:
    #   x_list = ['all words','all words\n(shuffled)']

    if index_ax == 0:
      ax.errorbar(x_list, np.mean(np.array(globals()['model_'+str(i)+'_result_combine'])[2,:,0:8],axis=0),
                  yerr=get_std(np.array(globals()['model_'+str(i)+'_result_combine'])[2,:,0:8]),#np.std(np.array(sari_list_list),axis=0),
                  # fmt='-',
                #  elinewidth = 0.8,
                  # capsize = 2, 
                  label=label_list[index],
                  color=color_list[index],
                  marker=marker_list[index],
                  fillstyle=fillstyle_list[index],
                  ms=ms_list[index],
                  linestyle=linestyle_list[index],
                  alpha=alpha_list[index],
                  
                  )

      ax.set_ylabel('FKGL ↓',fontsize=14)
      # ax.set_xlabel('# Prepended Named Entities\n (Entities from Complex Sentence)',fontsize=14)
    elif index_ax ==1:
      if index ==1:
        index=2
      ax.errorbar(x_list, np.mean(np.array(globals()['model_'+str(i)+'_result_combine'])[2,:,8:],axis=0),
                yerr=get_std(np.array(globals()['model_'+str(i)+'_result_combine'])[2,:,8:]),#np.std(np.array(sari_list_list),axis=0),
                # fmt='-',
              #  elinewidth = 0.8,
                # capsize = 2, 
                label=label_list[index],
                color=color_list[index],
                marker=marker_list[index],
                fillstyle=fillstyle_list[index],
                ms=ms_list[index],
                linestyle=linestyle_list[index],
                alpha=alpha_list[index]
                )



# SARI
for index_ax,ax in enumerate([ax5,ax6]):

  ax.axhspan(44.87+0.36, 44.87-0.36, alpha=1, color=sns.color_palette('BrBG')[2],label='Gold Reference')
  ax.axhspan(44.15-0.56, 44.15+0.56, alpha=1, color=sns.color_palette('BrBG')[3],label='MUSS')
  ax.axhspan(44.15+0.56, 44.87-0.36, alpha=1, color='#cddbbf')

  
  for index,i in enumerate(id_list):

    if index_ax !=1:
      x_list = [str(x) for x in range(7)]
      x_list.append('all')
    # else:
    #   x_list = ['all words','all words\n(shuffled)']

    if index_ax == 0:
      ax.errorbar(x_list, np.mean(np.array(globals()['model_'+str(i)+'_result_combine'])[1,:,0:8],axis=0),
                  yerr=get_std(np.array(globals()['model_'+str(i)+'_result_combine'])[1,:,0:8]),#np.std(np.array(sari_list_list),axis=0),
                  # fmt='-',
                #  elinewidth = 0.8,
                  # capsize = 2, 
                  label=label_list[index],
                  color=color_list[index],
                  marker=marker_list[index],
                  fillstyle=fillstyle_list[index],
                  ms=ms_list[index],
                  linestyle=linestyle_list[index],
                  alpha=alpha_list[index],
                  
                  )

      ax.set_ylabel('SARI ↑',fontsize=14)
      # ax.set_xlabel('# Prepended Named Entities\n (Entities from Complex Sentence)',fontsize=14)
    elif index_ax ==1:
      if index ==1:
        index=2
      ax.errorbar(x_list, np.mean(np.array(globals()['model_'+str(i)+'_result_combine'])[1,:,8:],axis=0),
                yerr=get_std(np.array(globals()['model_'+str(i)+'_result_combine'])[1,:,8:]),#np.std(np.array(sari_list_list),axis=0),
                # fmt='-',
              #  elinewidth = 0.8,
                # capsize = 2, 
                label=label_list[index],
                color=color_list[index],
                marker=marker_list[index],
                fillstyle=fillstyle_list[index],
                ms=ms_list[index],
                linestyle=linestyle_list[index],
                alpha=alpha_list[index]
                )
    




# NE retention rate
for index_ax,ax in enumerate([ax7,ax8]):
  
  # ax.axhspan(np.mean(asset_result[3])+get_mean_confidence_interval(asset_result[3]), np.mean(asset_result[3])-get_mean_confidence_interval(asset_result[3]), alpha=1, color=sns.color_palette('BrBG')[2],label='Gold Reference')
  # ax.axhspan(np.mean(muss_result[3])+get_mean_confidence_interval(muss_result[3]), np.mean(muss_result[3])-get_mean_confidence_interval(muss_result[3]), alpha=1, color=sns.color_palette('BrBG')[3],label='MUSS')
  # ax.axhspan(np.mean(asset_result[3])+get_mean_confidence_interval(asset_result[3]), np.mean(asset_result[3])-get_mean_confidence_interval(asset_result[3]), alpha=1, color='#cddbbf')

  ax.axhspan(np.mean(asset_result[3])+get_mean_confidence_interval(asset_result[3]), np.mean(asset_result[3])-get_mean_confidence_interval(asset_result[3]), alpha=1, color=sns.color_palette('BrBG')[2],label='Gold Reference')
  ax.axhspan(np.mean(muss_result[3])+get_mean_confidence_interval(muss_result[3]), np.mean(muss_result[3])-get_mean_confidence_interval(muss_result[3]), alpha=1, color=sns.color_palette('BrBG')[3],label='MUSS')
  ax.axhspan(np.mean(muss_result[3])-get_mean_confidence_interval(muss_result[3]), np.mean(asset_result[3])+get_mean_confidence_interval(asset_result[3]), alpha=1, color='#cddbbf')


  for index,i in enumerate([40]):

    if index_ax !=1:
      x_list = [str(x) for x in range(7)]
      x_list.append('all')
    # else:
    #   x_list = ['all words','all words\n(shuffled)']

    if index_ax == 0:
      ax.errorbar(x_list, np.mean(np.array(globals()['model_'+str(i)+'_result_combine'])[3,:,0:8],axis=0),
                  yerr=get_std(np.array(globals()['model_'+str(i)+'_result_combine'])[3,:,0:8]),#np.std(np.array(sari_list_list),axis=0),
                  # fmt='-',
                #  elinewidth = 0.8,
                  # capsize = 2, 
                  label=label_list[index],
                  color=color_list[index],
                  marker=marker_list[index],
                  fillstyle=fillstyle_list[index],
                  ms=ms_list[index],
                  linestyle=linestyle_list[index],
                  alpha=alpha_list[index],
                  
                  )

      ax.set_ylabel('Named Entities\nRetention Rate ↑',fontsize=14)
      # ax.set_xlabel('\n# Prepended Hard Word',fontsize=14)

    elif index_ax ==1:
      if index ==1:
        index=2
      ax.errorbar(x_list, np.mean(np.array(globals()['model_'+str(i)+'_result_combine'])[3,:,8:],axis=0),
                yerr=get_std(np.array(globals()['model_'+str(i)+'_result_combine'])[3,:,8:]),#np.std(np.array(sari_list_list),axis=0),
                # fmt='-',
              #  elinewidth = 0.8,
                # capsize = 2, 
                label=label_list[index],
                color=color_list[index],
                marker=marker_list[index],
                fillstyle=fillstyle_list[index],
                ms=ms_list[index],
                linestyle=linestyle_list[index],
                alpha=alpha_list[index]
                )
  
  for index,i in enumerate([41]):

    # if index_ax !=1:
    #   x_list = [str(x) for x in range(7)]
    #   x_list.append('all')
    # else:
    #   x_list = ['all words','all words\n(shuffled)']

    if index_ax == 0:
      index=1
      ax.errorbar(x_list, np.mean(np.array(globals()['model_'+str(i)+'_result_combine'])[3,:,0:8],axis=0),
                  yerr=get_std(np.array(globals()['model_'+str(i)+'_result_combine'])[3,:,0:8]),#np.std(np.array(sari_list_list),axis=0),
                  # fmt='-',
                #  elinewidth = 0.8,
                  # capsize = 2, 
                  label=label_list[index],
                  color=color_list[index],
                  marker=marker_list[index],
                  fillstyle=fillstyle_list[index],
                  ms=ms_list[index],
                  linestyle=linestyle_list[index],
                  alpha=alpha_list[index],
                  
                  )

    #   ax.set_ylabel('Named Entities\nRetention Rate ↑',fontsize=14)
    #   # ax.set_xlabel('\n# Prepended Hard Word',fontsize=14)

    # elif index_ax ==1:
    #   ax.errorbar(x_list, np.mean(np.array(globals()['model_'+str(i)+'_result_combine'])[3,:,8:],axis=0),
    #             yerr=get_std(np.array(globals()['model_'+str(i)+'_result_combine'])[3,:,8:]),#np.std(np.array(sari_list_list),axis=0),
    #             # fmt='-',
    #           #  elinewidth = 0.8,
    #             # capsize = 2, 
    #             label=label_list[index],
    #             color=color_list[index],
    #             marker=marker_list[index],
    #             fillstyle=fillstyle_list[index],
    #             ms=ms_list[index],
    #             linestyle=linestyle_list[index],
    #             alpha=alpha_list[index]
    #             )
    #   # ax.set_xlabel('Prepend _ in Complex Sentnece',fontsize=14)
  


# ABCD retention rate
for index_ax,ax in enumerate([ax9,ax10]):
  
  ax.axhspan(np.mean(asset_result_ABCD[3])+get_mean_confidence_interval(asset_result_ABCD[3]), np.mean(asset_result_ABCD[3])-get_mean_confidence_interval(asset_result_ABCD[3]), alpha=1, color=sns.color_palette('BrBG')[2],label='Gold Reference')
  ax.axhspan(np.mean(muss_result_ABCD[3])+get_mean_confidence_interval(muss_result_ABCD[3]), np.mean(muss_result_ABCD[3])-get_mean_confidence_interval(muss_result_ABCD[3]), alpha=1, color=sns.color_palette('BrBG')[3],label='MUSS')
  ax.axhspan(np.mean(asset_result_ABCD[3])+get_mean_confidence_interval(asset_result_ABCD[3]), np.mean(asset_result_ABCD[3])-get_mean_confidence_interval(asset_result_ABCD[3]), alpha=1, color='#cddbbf')

  for index,i in enumerate([40]):

    if index_ax !=1:
      x_list = [str(x) for x in range(7)]
      x_list.append('all')
    # else:
    #   x_list = ['all words','all words\n(shuffled)']

    if index_ax == 0:
      ax.errorbar(x_list, np.mean(np.array(globals()['model_'+str(i)+'_result_ABCD_combine'])[3,:,0:8],axis=0),
                  yerr=get_std(np.array(globals()['model_'+str(i)+'_result_ABCD_combine'])[3,:,0:8]),#np.std(np.array(sari_list_list),axis=0),
                  # fmt='-',
                #  elinewidth = 0.8,
                  # capsize = 2, 
                  label=label_list[index],
                  color=color_list[index],
                  marker=marker_list[index],
                  fillstyle=fillstyle_list[index],
                  ms=ms_list[index],
                  linestyle=linestyle_list[index],
                  alpha=alpha_list[index],
                  
                  )

      ax.set_ylabel('Hard Word\nRetention Rate ↓',fontsize=14)
      ax.set_xlabel('# Prefixed Named Entities',fontsize=14)

    elif index_ax ==1:
      if index ==1:
        index=2
      ax.errorbar(x_list, np.mean(np.array(globals()['model_'+str(i)+'_result_ABCD_combine'])[3,:,8:],axis=0),
                yerr=get_std(np.array(globals()['model_'+str(i)+'_result_ABCD_combine'])[3,:,8:]),#np.std(np.array(sari_list_list),axis=0),
                # fmt='-',
              #  elinewidth = 0.8,
                # capsize = 2, 
                label=label_list[index],
                color=color_list[index],
                marker=marker_list[index],
                fillstyle=fillstyle_list[index],
                ms=ms_list[index],
                linestyle=linestyle_list[index],
                alpha=alpha_list[index]
                )
      
      ax.set_xlabel('# Prefixed Hard Word',fontsize=14)

  for index,i in enumerate([41]):

    # if index_ax !=1:
    #   x_list = [str(x) for x in range(7)]
    #   x_list.append('all')
    # else:
    #   x_list = ['all words','all words\n(shuffled)']

    if index_ax == 1:
      index = 2
      ax.errorbar(x_list, np.mean(np.array(globals()['model_'+str(i)+'_result_combine'])[3,:,8:],axis=0),
                  yerr=get_std(np.array(globals()['model_'+str(i)+'_result_combine'])[3,:,8:]),#np.std(np.array(sari_list_list),axis=0),
                  # fmt='-',
                #  elinewidth = 0.8,
                  # capsize = 2, 
                  label=label_list[index],
                  color=color_list[index],
                  marker=marker_list[index],
                  fillstyle=fillstyle_list[index],
                  ms=ms_list[index],
                  linestyle=linestyle_list[index],
                  alpha=alpha_list[index],
                  
                  )

      # ax.set_ylabel('Hard Word\nRetention Rate ↓',fontsize=14)
      # ax.set_xlabel('# Prefixed Hard Words',fontsize=14)

    # elif index_ax ==1:
    #   ax.errorbar(x_list, np.mean(np.array(globals()['model_'+str(i)+'_result_ABCD_combine'])[3,:,8:],axis=0),
    #             yerr=get_std(np.array(globals()['model_'+str(i)+'_result_ABCD_combine'])[3,:,8:]),#np.std(np.array(sari_list_list),axis=0),
    #             # fmt='-',
    #           #  elinewidth = 0.8,
    #             # capsize = 2, 
    #             label=label_list[index],
    #             color=color_list[index],
    #             marker=marker_list[index],
    #             fillstyle=fillstyle_list[index],
    #             ms=ms_list[index],
    #             linestyle=linestyle_list[index],
    #             alpha=alpha_list[index]
    #             )
      
    #   ax.set_xlabel('# Prepended Hard Word\n(each with All Named Entities)',fontsize=14)
  


ax2.legend(handles=handles, labels=labels,loc='upper right',bbox_to_anchor=(1.02,1.7,0,0),fontsize='large')


plt.tight_layout()
# plt.savefig('two_ope_result.pdf',bbox_inches='tight')
plt.show()

cd /content

"""# SARI operation"""

def get_mean_CI_sari_operation(i,generate_id_list,test_data_list,):
  

  output_id = 'model_'+str(i)+'_'+model_dir_dict[i]['exp_dir'].split('/')[-2] # model_18_local_1629593348299

  sari_add_list, sari_keep_list, sari_del_list = [],[],[]

  dataset = 'asset'
  phase = 'test'

  for generate_id in generate_id_list:
    
    tmp_sari_add,tmp_sari_keep,tmp_sari_del=[],[],[]
    for output_file_name in test_data_list:

      sari_add, sari_keep, sari_del = evaluate_my_model_operation(output_id,generate_id,dataset,phase,output_file_name)

      tmp_sari_add.append(sari_add)
      tmp_sari_keep.append(sari_keep)
      tmp_sari_del.append(sari_del)

    sari_add_list += [tmp_sari_add]
    sari_keep_list += [tmp_sari_keep]
    sari_del_list += [tmp_sari_del]

    
  print('model',i)
  print('sari_add',round(np.mean(sari_add_list), 2),'±',round(get_mean_confidence_interval(sari_add_list), 2))
  print('sari_keep_list',round(np.mean(sari_keep_list), 2),'±',round(get_mean_confidence_interval(sari_keep_list), 2))
  print('sari_del_list',round(np.mean(sari_del_list), 2),'±',round(get_mean_confidence_interval(sari_del_list), 2))

  return  sari_add_list,sari_keep_list,sari_del_list

# second, evalaute 
sari_add_list,sari_keep_list,sari_del_list = get_mean_CI_sari_operation(i,generate_id_list,test_data_list,)

# first, define the model id and the corsponding generate folder id
# test data can be 'asset.simple.test' and 'asset.complex.test'
i = 18
generate_id_list = ['02','03','04','05','06']
test_data_list = ['asset.simple.test']

# third, save the result
with open('/content/drive/MyDrive/muss/qualitative/SARI_ope_18_complex', 'wb') as fp:
    pickle.dump([sari_add_list,sari_keep_list,sari_del_list], fp)

# Following: repeat for all models

i = 18
generate_id_list = ['02','03','04','05','06']
test_data_list = ['asset.complex.test']

i = 19
generate_id_list = ['00','01','02','03','04']
test_data_list = ['asset.complex.test']

i = 20
generate_id_list = ['00','01','02','03','05']
test_data_list = ['asset.complex.test']

with open('/content/drive/MyDrive/muss/qualitative/SARI_ope_20_simple', 'wb') as fp:
    pickle.dump([sari_add_list,sari_keep_list,sari_del_list], fp)

with open('/content/drive/MyDrive/muss/qualitative/SARI_ope_20_complex', 'wb') as fp:
    pickle.dump([sari_add_list,sari_keep_list,sari_del_list], fp)

i = 21
generate_id_list = ['00','01','02','03','04']
test_data_list = ['asset.complex.test']

with open('/content/drive/MyDrive/muss/qualitative/SARI_ope_21_simple', 'wb') as fp:
    pickle.dump([sari_add_list,sari_keep_list,sari_del_list], fp)

with open('/content/drive/MyDrive/muss/qualitative/SARI_ope_21_complex', 'wb') as fp:
    pickle.dump([sari_add_list,sari_keep_list,sari_del_list], fp)

i = 21
generate_id_list = ['00','01','02','03','04']
test_data_list = test_data_list[0:8]
test_data_list

np.mean(np.array(sari_add_list),axis=0)

with open('/content/drive/MyDrive/muss/qualitative/SARI_ope_21_simple_trend', 'wb') as fp:
    pickle.dump([sari_add_list,sari_keep_list,sari_del_list], fp)

i = 22
generate_id_list = ['00','01','02','03','04']
test_data_list = ['asset.complex.test']

with open('/content/drive/MyDrive/muss/qualitative/SARI_ope_22_simple', 'wb') as fp:
    pickle.dump([sari_add_list,sari_keep_list,sari_del_list], fp)

with open('/content/drive/MyDrive/muss/qualitative/SARI_ope_22_complex', 'wb') as fp:
    pickle.dump([sari_add_list,sari_keep_list,sari_del_list], fp)

i = 23
generate_id_list = ['00','01','02','03','04']
test_data_list = ['asset.complex.test']

with open('/content/drive/MyDrive/muss/qualitative/SARI_ope_23_simple', 'wb') as fp:
    pickle.dump([sari_add_list,sari_keep_list,sari_del_list], fp)

with open('/content/drive/MyDrive/muss/qualitative/SARI_ope_23_complex', 'wb') as fp:
    pickle.dump([sari_add_list,sari_keep_list,sari_del_list], fp)

i = 39
generate_id_list = ['00','01','02','03','04']
test_data_list = ['asset.complex.test']

with open('/content/drive/MyDrive/muss/qualitative/SARI_ope_39_simple', 'wb') as fp:
    pickle.dump([sari_add_list,sari_keep_list,sari_del_list], fp)

with open('/content/drive/MyDrive/muss/qualitative/SARI_ope_39_complex', 'wb') as fp:
    pickle.dump([sari_add_list,sari_keep_list,sari_del_list], fp)

"""## operation preserving


"""

!pip install matplotlib==3.4



with open('/content/drive/MyDrive/muss/qualitative/SARI_ope_asset', 'rb') as fp:
    asset_ope = pickle.load(fp)

with open('/content/drive/MyDrive/muss/qualitative/SARI_ope_muss', 'rb') as fp:
    muss_ope = pickle.load(fp)

with open('/content/drive/MyDrive/muss/qualitative/SARI_ope_21_simple_trend', 'rb') as fp:
  [sari_add_list,sari_keep_list,sari_del_list] = pickle.load(fp)

df = pd.DataFrame(np.array([  [round(x,1) for x in np.mean(np.array(sari_add_list),axis=0)] ,
[round(x,1) for x in np.mean(np.array(sari_keep_list),axis=0)] ,
[round(x,1) for x in np.mean(np.array(sari_del_list),axis=0)] ,
]).T, columns =['add', 'keep', 'del'], dtype = float)

df

x_list = [str(x) for x in range(7)]
x_list.append('all')
df['x_list']=x_list
df

df_ref = pd.DataFrame( columns =['add', 'keep', 'del'], dtype = float)

df_ref.loc[0] =np.array([round(np.mean(muss_ope[0]),1),round(np.mean(muss_ope[1]),1),round(np.mean(muss_ope[2]),1)], dtype = float)

df_ref

df_ref.loc[1] =np.array([round(np.mean(asset_ope[0]),1),round(np.mean(asset_ope[1]),1),round(np.mean(asset_ope[2]),1)], dtype = float)

df_ref

df_ref['x_list']=['muss','asset']
df_ref

final_clrs = []
clrs = sns.color_palette('BrBG')#BrBG,,RdBu,RdGy,RdYlBu,Spectral,YlGnBu,,tab20c
final_clrs = clrs
final_clrs

f, (ax, ax2) = plt.subplots(1, 2, gridspec_kw={'width_ratios': [4, 1]}, figsize=(10, 6),sharey=True)

df.plot(ax=ax,x='x_list',kind='bar', stacked=True, rot=0, xlabel='Class', ylabel='Count', color=final_clrs)
for index,c in enumerate(ax.containers):
    if index !=0:
      ax.bar_label(c, label_type='center')
    else:
      ax.bar_label(c, label_type='center',color='white')


df_ref.plot(ax=ax2,x='x_list',kind='bar', stacked=True,  rot=0, color=final_clrs,legend=False)
for index,c in enumerate(ax2.containers):
    if index !=0:
      ax2.bar_label(c, label_type='center')
    else:
      ax2.bar_label(c, label_type='center',color='white')


# ax.legend(loc='upper right',bbox_to_anchor=(1.01,2,0,0),fontsize='large')
ax.set_ylabel('F1 score',fontsize=14)
ax.set_xlabel('# Prefixed Named Entities\nfrom Simple sentence',fontsize=14)

ax2.set_xlabel('\nBaselines',fontsize=14)

plt.tight_layout()

# plt.savefig('trend_sari_ope.pdf',bbox_inches='tight')
plt.show()

id_list = [x for x in range(18,24)]
id_list.append(39)
id_list

for i in id_list:
  

  with open('/content/drive/MyDrive/muss/qualitative/SARI_ope_'+str(i)+'_simple', 'rb') as fp:
    var_name = 'model_'+str(i)+'_simple'
    globals()[var_name] = pickle.load(fp)

  with open('/content/drive/MyDrive/muss/qualitative/SARI_ope_'+str(i)+'_complex', 'rb') as fp:
    var_name = 'model_'+str(i)+'_complex'
    globals()[var_name] = pickle.load(fp)

df_all_NE = pd.DataFrame( columns =['add', 'keep', 'del'], dtype = float)

# complex/complex use all complex NE
df_all_NE.loc[0] =np.array([round(np.mean(model_18_complex[0]),1),round(np.mean(model_18_complex[1]),1),round(np.mean(model_18_complex[2]),1)], dtype = float)#,

df_all_NE

# simple/simple use all ccomplex NE
df_all_NE.loc[1] =np.array([round(np.mean(model_21_complex[0]),1),round(np.mean(model_21_complex[1]),1),round(np.mean(model_21_complex[2]),1)], dtype = float)#,

df_all_NE

# simple/simple use all simple NE
df_all_NE.loc[2] =np.array([round(np.mean(model_21_simple[0]),1),round(np.mean(model_21_simple[1]),1),round(np.mean(model_21_simple[2]),1)], dtype = float)#,

df_all_NE

# both/both use all simple NE
df_all_NE.loc[3] =np.array([round(np.mean(model_39_simple[0]),1),round(np.mean(model_39_simple[1]),1),round(np.mean(model_39_simple[2]),1)], dtype = float)#,

df_all_NE

df_all_NE.loc[4] =np.array([round(np.mean(muss_ope[0]),1),round(np.mean(muss_ope[1]),1),round(np.mean(muss_ope[2]),1)], dtype = float)#,

df_all_NE

df_all_NE.loc[5] =np.array([round(np.mean(asset_ope[0]),1),round(np.mean(asset_ope[1]),1),round(np.mean(asset_ope[2]),1)], dtype = float)#,

df_all_NE

df_all_NE['x_list']=['complex/complex','simple/simple','simple/simple','both/both','muss','asset']

df_all_NE[0:2]

f, (ax, ax2,ax3) = plt.subplots(1, 3, gridspec_kw={'width_ratios': [2,2, 2]}, figsize=(10, 6),sharey=True)

df_all_NE[0:2].plot(ax=ax,x='x_list',kind='bar', stacked=True, rot=0, xlabel='Class', ylabel='Count', color=final_clrs).legend(loc='upper left')
for index,c in enumerate(ax.containers):
    if index !=0:
      ax.bar_label(c, label_type='center')
    else:
      ax.bar_label(c, label_type='center',color='white')

ax.set_xlabel('Experiment \n(All Named Entities from\nComplex sentence)',fontsize=12)

df_all_NE[2:4].plot(ax=ax2,x='x_list',kind='bar', stacked=True,  rot=0, color=final_clrs,legend=False)
for index,c in enumerate(ax2.containers):
    if index !=0:
      ax2.bar_label(c, label_type='center')
    else:
      ax2.bar_label(c, label_type='center',color='white')

ax2.set_xlabel('Experiment \n(All Named Entities from\nSimple sentence)',fontsize=12)

df_all_NE[4:].plot(ax=ax3,x='x_list',kind='bar', stacked=True,  rot=0, color=final_clrs,legend=False)
for index,c in enumerate(ax3.containers):
    if index !=0:
      ax3.bar_label(c, label_type='center')
    else:
      ax3.bar_label(c, label_type='center',color='white')

# ax.legend(loc='upper right',bbox_to_anchor=(1.01,2,0,0),fontsize='large')
ax.set_ylabel('F1 score',fontsize=14)


ax3.set_xlabel('\nBaselines',fontsize=12)

plt.tight_layout()

plt.savefig('best_sari_ope.pdf',bbox_inches='tight')
plt.show()

"""## operation lexical simplification"""

!pip install matplotlib==3.4

with open('/content/drive/MyDrive/muss/qualitative/SARI_ope_asset', 'rb') as fp:
    asset_ope = pickle.load(fp)

with open('/content/drive/MyDrive/muss/qualitative/SARI_ope_muss', 'rb') as fp:
    muss_ope = pickle.load(fp)

with open('/content/drive/MyDrive/muss/qualitative/SARI_ope_38_ABCD_new', 'rb') as fp:
  [sari_add_list,sari_keep_list,sari_del_list] = pickle.load(fp)

df = pd.DataFrame(np.array([  [round(x,1) for x in np.mean(np.array(sari_add_list),axis=0)] ,
[round(x,1) for x in np.mean(np.array(sari_keep_list),axis=0)] ,
[round(x,1) for x in np.mean(np.array(sari_del_list),axis=0)] ,
]).T, columns =['add', 'keep', 'del'], dtype = float)

df

x_list = [str(x) for x in range(7)]
x_list.extend(['all','all\nwords','all\nwords\nshuffled'])
df['x_list']=x_list
df

df_ref = pd.DataFrame( columns =['add', 'keep', 'del'], dtype = float)

df_ref.loc[0] =np.array([round(np.mean(muss_ope[0]),1),round(np.mean(muss_ope[1]),1),round(np.mean(muss_ope[2]),1)], dtype = float)#,

df_ref

df_ref.loc[1] =np.array([round(np.mean(asset_ope[0]),1),round(np.mean(asset_ope[1]),1),round(np.mean(asset_ope[2]),1)], dtype = float)#,

df_ref

df_ref.loc[2] =np.array([0.5,62.0,34.7], dtype = float)#,

df_ref

df_ref['x_list']=['muss','asset','all hard\nwords\nremoved']
df_ref

final_clrs = []
clrs = sns.color_palette('BrBG')#BrBG,,RdBu,RdGy,RdYlBu,Spectral,YlGnBu,,tab20c
final_clrs = clrs
final_clrs

f, (ax, ax2,ax3) = plt.subplots(1, 3, gridspec_kw={'width_ratios': [7,2, 3]}, figsize=(10, 6),sharey=True)

df[0:8].plot(ax=ax,x='x_list',kind='bar', stacked=True, rot=0, xlabel='Class', ylabel='Count', color=final_clrs,legend=False)
for index,c in enumerate(ax.containers):
    if index !=0:
      ax.bar_label(c, label_type='center')
    else:
      ax.bar_label(c, label_type='center',color='white')

df[8:].plot(ax=ax2,x='x_list',kind='bar', stacked=True, rot=0, xlabel='Class', ylabel='Count', color=final_clrs).legend(loc='upper left')
for index,c in enumerate(ax2.containers):
    if index !=0:
      ax2.bar_label(c, label_type='center')
    else:
      ax2.bar_label(c, label_type='center',color='white')

df_ref.plot(ax=ax3,x='x_list',kind='bar', stacked=True,  rot=0, color=final_clrs,legend=False)
for index,c in enumerate(ax3.containers):
    if index !=0:
      ax3.bar_label(c, label_type='center')
    else:
      ax3.bar_label(c, label_type='center',color='white')


# ax.legend(loc='upper right',bbox_to_anchor=(1.01,2,0,0),fontsize='large')
ax.set_ylabel('F1 score',fontsize=14)
ax.set_xlabel('\n\n# Prefixed Hard Words',fontsize=14)

ax2.set_xlabel('Prefix _ in\nComplex Sentence',fontsize=14)
ax3.set_xlabel('Baselines',fontsize=14)

plt.tight_layout()

# plt.savefig('ls_ope.pdf',bbox_inches='tight')
plt.show()

cd /content

"""## operation preserving + lexical simplification"""

test_data_dir_list

test_data_list

i = 40
generate_id_list = ['00','01','02','03','04']

with open('/content/drive/MyDrive/muss/qualitative/SARI_ope_model40', 'rb') as fp:
    [sari_add_list,sari_keep_list,sari_del_list] = pickle.load(fp)

with open('/content/drive/MyDrive/muss/qualitative/SARI_ope_asset', 'rb') as fp:
    asset_ope = pickle.load(fp)

with open('/content/drive/MyDrive/muss/qualitative/SARI_ope_muss', 'rb') as fp:
    muss_ope = pickle.load(fp)

df = pd.DataFrame(np.array([  [round(x,1) for x in np.mean(np.array(sari_add_list),axis=0)] ,
[round(x,1) for x in np.mean(np.array(sari_keep_list),axis=0)] ,
[round(x,1) for x in np.mean(np.array(sari_del_list),axis=0)] ,
]).T, columns =['add', 'keep', 'del'], dtype = float)

df

line = pd.DataFrame({"add": 11.5, "keep": 60.6,'del':66.2}, index=[7])
line

df = pd.concat([df.iloc[:7], line, df.iloc[7:]]).reset_index(drop=True)
df

x_list = [str(x) for x in range(7)]
x_list.extend(['all'])
x_list.extend([str(x) for x in range(7)])
x_list.extend(['all'])
df['x_list']=x_list
df

df_ref = pd.DataFrame( columns =['add', 'keep', 'del'], dtype = float)

df_ref.loc[0] =np.array([round(np.mean(muss_ope[0]),1),round(np.mean(muss_ope[1]),1),round(np.mean(muss_ope[2]),1)], dtype = float)#,

df_ref

df_ref.loc[1] =np.array([round(np.mean(asset_ope[0]),1),round(np.mean(asset_ope[1]),1),round(np.mean(asset_ope[2]),1)], dtype = float)#,

df_ref

df_ref['x_list']=['muss','asset']
df_ref

final_clrs = []
clrs = sns.color_palette('BrBG')#BrBG,,RdBu,RdGy,RdYlBu,Spectral,YlGnBu,,tab20c
final_clrs = clrs
final_clrs

f, (ax, ax2,ax3) = plt.subplots(1, 3, gridspec_kw={'width_ratios': [8,8,2]}, figsize=(13, 6),sharey=True)

df[0:8].plot(ax=ax,x='x_list',kind='bar', stacked=True, rot=0, xlabel='Class', ylabel='Count', color=final_clrs,legend=False)
for index,c in enumerate(ax.containers):
    if index !=0:
      ax.bar_label(c, label_type='center')
    else:
      ax.bar_label(c, label_type='center',color='white')

df[8:].plot(ax=ax2,x='x_list',kind='bar', stacked=True, rot=0, xlabel='Class', ylabel='Count', color=final_clrs,legend=False)
for index,c in enumerate(ax2.containers):
    if index !=0:
      ax2.bar_label(c, label_type='center')
    else:
      ax2.bar_label(c, label_type='center',color='white')

df_ref.plot(ax=ax3,x='x_list',kind='bar', stacked=True,  rot=0, color=final_clrs,legend=False)
for index,c in enumerate(ax3.containers):
    if index !=0:
      ax3.bar_label(c, label_type='center')
    else:
      ax3.bar_label(c, label_type='center',color='white')


ax3.legend(loc='upper right',bbox_to_anchor=(1.06,1.23,0,0),fontsize='large')
ax.set_ylabel('F1 score',fontsize=14)
ax.set_xlabel('# Prefixed Named Entities',fontsize=14)

ax2.set_xlabel('# Prefixed Hard Words',fontsize=14)
ax3.set_xlabel('Baselines',fontsize=14)

plt.tight_layout()

# plt.savefig('pre_ls_ope.pdf',bbox_inches='tight')
plt.show()

cd /content

